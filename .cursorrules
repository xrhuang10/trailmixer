# Introduction

Extract insights from your video content with our AI that identifies objects, actions, speech, and text - enabling you to build powerful applications through simple APIs.

[Sign up](https://playground.twelvelabs.io/) for a free account, retrieve your API key, and make your first search request in minutes.


  ```Python Python maxLines=8
  from twelvelabs import TwelveLabs

  client = TwelveLabs(api_key="")

  index = client.index.create(name="", models=[{"name": "marengo2.7","options": ["visual", "audio"],}])
  print(f"Created index: id={index.id} name={index.name}")
  task = client.task.create(index_id=index.id, url="")
  print(f"Created task: id={task.id}")

  task.wait_for_done(sleep_interval=5, callback=lambda t: print(f"  Status={t.status}"))
  if task.status != "ready":
      raise RuntimeError(f"Indexing failed with status {task.status}")
  print(f"Upload complete. The unique identifier of your video is {task.video_id}.")

  search_results = client.search.query(index_id=index.id, query_text="", options=["visual", "audio"])
  for clip in search_results.data:
      print(f" video_id={clip.video_id} score={clip.score} start={clip.start} end={clip.end} confidence={clip.confidence}")
  ```

  ```JavaScript Node.js maxLines=8
  import { TwelveLabs } from "twelvelabs-js";

  const client = new TwelveLabs({ apiKey: "" });

  const index = await client.index.create({name: "", models: [{ name: "marengo2.7", options: ["visual", "audio"]}]});
  console.log(`Created index: id=${index.id} name=${index.name}`);

  const task = await client.task.create({indexId: index.id, url: ""});
  console.log(`Created task: id=${task.id}`);
  await task.waitForDone(5000, (task) => {
    console.log(`  Status=${task.status}`);
  });
  if (task.status !== "ready") {
    throw new Error(`Indexing failed with status ${task.status}`);
  }
  console.log(`Upload complete. The unique identifier of your video is ${task.videoId}`);

  const searchResults = await client.search.query({indexId: index.id, queryText: "", options: ["visual", "audio"]});
  for (const clip of searchResults.data) {
    console.log(`video_id=${clip.videoId} score=${clip.score} start=${clip.start} end=${clip.end} confidence=${clip.confidence}`);
  }
  ```


# Most popular


  
    Follow concise, step-by-step instructions to integrate the TwelveLabs Video Understanding Platform into your application quickly.
  

  
    Comprehensive documentation for each endpoint.
  

  
    Discover how to utilize the official client libraries to streamline your integration process.
  

  
    Test the capabilities of the platform before writing any code.
  


# TwelveLabs models


  Embedding model, proficient at performing cross-modal searches – across text, audio, image, and video. Use this model for search and embedding tasks.



  Generative model that can answer questions, generate creative outputs, and provide detailed analysis of any video. Use this model to analyze videos and generate text based on their content.


# Guides


  Search for specific content in your videos using text or image queries.



  Analyze videos to create different types of text based on their content.



  Create embeddings from your video data for semantic search, hybrid search, recommender systems, anomaly detection, and other applications.



# Quickstart

{/* 
The 1.3 version of the API includes significant improvements and introduces breaking changes. If you use v1.2, refer to the [Migration guide](/v1.3/docs/resources/migration-guide) page for a detailed list of changes, migration instructions, and code examples.
 */}

The quickstart guides in this section provide concise, step-by-step instructions to help you quickly integrate the TwelveLabs Video Understanding Platform into your application. Each guide focuses on a specific task and uses the [official SDKs](/v1.3/docs/resources/twelve-labs-sd-ks) for a streamlined experience.


  {/* Learn how to perform searches. */}



  {/* Learn how to generate text from your video files. */}



  {/* Learn how to generate different types of embeddings. */}


For more in-depth information on each task, see the guides below:

* [Search](/v1.3/docs/guides/search)
* [Analyze videos](/v1.3/docs/guides/analyze-videos)
* [Create embeddings](/v1.3/docs/guides/create-embeddings)

***

# Interactive notebooks

In addition to the quickstart guides, these interactive notebooks below provide complete, executable code for each task. Hosted on Google Colab, they allow you to experiment with the code directly, making them ideal for hands-on learning and customization.

| Capability | Description                           | Try it                                                                                                                                                                                                                            |
| ---------- | ------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Search     | Find specific moments in your videos. | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Search.ipynb)     |
| Analyze    | Analyze videos to generate text.      | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Analyze.ipynb)    |
| Embed      | Create multimodal embeddings.         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Embeddings.ipynb) |


# Search

This quickstart guide provides a simplified introduction to searching video content using the TwelveLabs Video Understanding Platform. It includes:

* A basic working example
* Minimal implementation details
* Core parameters for common use cases

For a comprehensive guide, see the [Search](/v1.3/docs/guides/search) page.

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The videos you wish to use must meet the following requirements:
  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.

  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.

  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  * **Duration**: Must be between 4 seconds and 2 hours (7,200s).

  * **File size**:  Must not exceed 2 GB.\
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

# Starter code

Copy and paste the code below to make a search request, replacing the placeholders surrounded by `<>` with your values.


  ```Python Python maxLines=8
  from twelvelabs import TwelveLabs

  client = TwelveLabs(api_key="")

  index = client.index.create(name="", models=[{"name": "marengo2.7","options": ["visual", "audio"],}])
  print(f"Created index: id={index.id} name={index.name}")
  task = client.task.create(index_id=index.id, url="")
  print(f"Created task: id={task.id}")

  task.wait_for_done(sleep_interval=5, callback=lambda t: print(f"  Status={t.status}"))
  if task.status != "ready":
      raise RuntimeError(f"Indexing failed with status {task.status}")
  print(f"Upload complete. The unique identifier of your video is {task.video_id}.")

  search_results = client.search.query(index_id=index.id, query_text="", options=["visual", "audio"])
  for clip in search_results.data:
      print(f" video_id={clip.video_id} score={clip.score} start={clip.start} end={clip.end} confidence={clip.confidence}")
  ```

  ```JavaScript Node.js maxLines=8
  import { TwelveLabs } from "twelvelabs-js";

  const client = new TwelveLabs({ apiKey: "" });

  const index = await client.index.create({name: "", models: [{ name: "marengo2.7", options: ["visual", "audio"]}]});
  console.log(`Created index: id=${index.id} name=${index.name}`);

  const task = await client.task.create({indexId: index.id, url: ""});
  console.log(`Created task: id=${task.id}`);
  await task.waitForDone(5000, (task) => {
    console.log(`  Status=${task.status}`);
  });
  if (task.status !== "ready") {
    throw new Error(`Indexing failed with status ${task.status}`);
  }
  console.log(`Upload complete. The unique identifier of your video is ${task.videoId}`);

  const searchResults = await client.search.query({indexId: index.id, queryText: "", options: ["visual", "audio"]});
  for (const clip of searchResults.data) {
    console.log(`video_id=${clip.videoId} score=${clip.score} start=${clip.start} end=${clip.end} confidence=${clip.confidence}`);
  }
  ```


# Step-by-step guide


  
    Create a client instance to interact with the TwelveLabs Video Understanding Platform.
  

  
    Indexes help you organize and search through related videos efficiently. To create an index, you must provide its name and configure the [video understanding models](/v1.3/docs/concepts/models) you wish to enable. This example uses [Marengo](/v1.3/docs/concepts/models/marengo) as the video understanding model and specifies that the platform should analyze the visual and audio modalities. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.
  

  
    To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.
  

  
    The platform requires some time to index videos. Check the status of the video indexing task until it's completed.
  

  
    Search the videos within the specified index using natural language to find video segments matching specific keywords or phrases.
  

  
    This example prints the search results to the standard output.
  



# Analyze videos

This quickstart guide provides a simplified introduction to analyzing videos to generate text using the TwelveLabs Video Understanding Platform. It includes:

* A basic working example
* Minimal implementation details
* Core parameters for common use cases

For a comprehensive guide, see the [Analyze videos](/v1.3/docs/guides/analyze-videos) section.

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The videos you wish to use must meet the following requirements:
  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.

  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.

  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  * **Duration**: Must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration will be 2 hours (7,200 seconds).

  * **File size**:  Must not exceed 2 GB.\
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

# Starter code

You can copy and paste the code below to analyze videos and generate text based on their content. Replace the placeholders surrounded by `<>` with your values.


  
    
      ```Python Python maxLines=8
      from twelvelabs import TwelveLabs

      client = TwelveLabs(api_key="")

      index = client.index.create(name="", models=[{"name": "pegasus1.2", "options": ["visual", "audio"]}])
      print(f"Created index: id={index.id} name={index.name}")

      task = client.task.create(index_id=index.id, url="")
      print(f"Created task: id={task.id}")
      task.wait_for_done(sleep_interval=5, callback=lambda t: print(f"  Status={t.status}"))
      if task.status != "ready":
          raise RuntimeError(f"Indexing failed with status {task.status}")
      print(f"Upload complete. The unique identifier of your video is {task.video_id}.")

      gist = client.gist(video_id=task.video_id,types=["title", "topic", "hashtag"])
      print(f"Title={gist.title}\nTopics={gist.topics}\nHashtags={gist.hashtags}")
      ```

      ```JavaScript Node.js maxLines=8
      import { TwelveLabs } from "twelvelabs-js";

      const client = new TwelveLabs({ apiKey: "" });

      let index = await client.index.create({name: "", models: [{name: "pegasus1.2", options: ["visual", "audio"]}]});
      console.log(`Created index: id=${index.id} name=${index.name}`);

      const task = await client.task.create({indexId: index.id, url: ""});
      console.log(`Created task: id=${task.id}`);
      await task.waitForDone(5000, (task) => {
        console.log(`  Status=${task.status}`);
      });
      if (task.status !== "ready") {
        throw new Error(`Indexing failed with status ${task.status}`);
      }
      console.log(`Upload complete. The unique identifier of your video is ${task.videoId}`);

      const gist = await client.gist(task.videoId, ["title", "topic", "hashtag"]);
      console.log(`Title: ${gist.title}\nTopics=${gist.topics}\nHashtags=${gist.hashtags}`);
      ```
    
  

  
    
      ```Python Python maxLines=8
      from twelvelabs import TwelveLabs

      client = TwelveLabs(api_key="")

      index = client.index.create(name="", models=[{"name": "pegasus1.2", "options": ["visual", "audio"]}])
      print(f"Created index: id={index.id} name={index.name}")

      task = client.task.create(index_id=index.id, url="")
      print(f"Created task: id={task.id}")
      task.wait_for_done(sleep_interval=5, callback=lambda t: print(f"  Status={t.status}"))
      if task.status != "ready":
          raise RuntimeError(f"Indexing failed with status {task.status}")
      print(f"Upload complete. The unique identifier of your video is {task.video_id}.")

      res = client.summarize(video_id=task.video_id, type="summary")
      print(f"Summary= {res.summary}")

      res = client.summarize(video_id=task.video_id, type="chapter")
      for chapter in res.chapters:
          print(
              f"""Chapter {chapter.chapter_number},
      start={chapter.start},
      end={chapter.end}
      Title: {chapter.chapter_title}
      Summary: {chapter.chapter_summary}
      """
      )

      res = client.summarize(video_id=task.video_id, type="highlight")
      for highlight in res.highlights:
          print(f"Highlight: {highlight.highlight}, start: {highlight.start}, end: {highlight.end}")
      ```

      ```JavaScript Node.js maxLines=8
      import { TwelveLabs } from "twelvelabs-js";

      const client = new TwelveLabs({ apiKey: "" });

      let index = await client.index.create({name: "", models: [{name: "pegasus1.2", options: ["visual", "audio"]}]});
      console.log(`Created index: id=${index.id} name=${index.name}`);

      const task = await client.task.create({indexId: index.id, url: ""});
      console.log(`Created task: id=${task.id}`);
      await task.waitForDone(5000, (task) => {
        console.log(`  Status=${task.status}`);
      });
      if (task.status !== "ready") {
        throw new Error(`Indexing failed with status ${task.status}`);
      }
      console.log(`Upload complete. The unique identifier of your video is ${task.videoId}`);

      const summary = await client.summarize(task.videoId, "summary");
      console.log(`Summary: ${summary.summary}`);

      const chapters = await client.summarize(task.videoId, "chapter");
      for (const chapter of chapters.chapters) {
        console.log(
          `Chapter ${chapter.chapterNumber}\nstart=${chapter.start}\nend=${chapter.end}\nTitle=${chapter.chapterTitle}\nSummary=${chapter.chapterSummary}`,
        );
      }

      const highlights = await client.summarize(task.videoId, "highlight");
      for (const highlight of highlights.highlights) {
        console.log(`Highlight: ${highlight.highlight}, start: ${highlight.start}, end: ${highlight.end}`);
      }
      ```
    
  

  
    
      ```Python Python maxLines=8
      from twelvelabs import TwelveLabs

      client = TwelveLabs(api_key="")

      index = client.index.create(name="", models=[{"name": "pegasus1.2", "options": ["visual", "audio"]}])
      print(f"Created index: id={index.id} name={index.name}")

      task = client.task.create(index_id=index.id, url="")
      print(f"Created task: id={task.id}")
      task.wait_for_done(sleep_interval=5, callback=lambda t: print(f"  Status={t.status}"))
      if task.status != "ready":
          raise RuntimeError(f"Indexing failed with status {task.status}")
      print(f"Upload complete. The unique identifier of your video is {task.video_id}.")

      text_stream = client.analyze_stream(video_id=task.video_id, prompt="")
      for text in text_stream:
          print(text)
      print(f"Aggregated text: {text_stream.aggregated_text}")
      ```

      ```JavaScript Node.js maxLines=8
      import { TwelveLabs } from "twelvelabs-js";

      const client = new TwelveLabs({ apiKey: "" });

      let index = await client.index.create({name: "", models: [{name: "pegasus1.2", options: ["visual", "audio"]}]});
      console.log(`Created index: id=${index.id} name=${index.name}`);

      const task = await client.task.create({indexId: index.id, url: ""});
      console.log(`Created task: id=${task.id}`);
      await task.waitForDone(5000, (task) => {
        console.log(`  Status=${task.status}`);
      });
      if (task.status !== "ready") {
        throw new Error(`Indexing failed with status ${task.status}`);
      }
      console.log(`Upload complete. The unique identifier of your video is ${task.videoId}`);

      const textStream = await client.analyzeStream({videoId: task.videoId, prompt: ""});

      for await (const text of textStream) {
        console.log(text);
      }
      console.log(`Aggregated text: ${textStream.aggregatedText}`);
      ```
    
  


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.
      

      
        Indexes help you organize and search through related videos efficiently. To create an index, you must provide its name and configure the [video understanding models](/v1.3/docs/concepts/models) you wish to enable. This example uses [Pegasus](/v1.3/docs/concepts/models/pegasus) as the video understanding model and specifies that the platform should analyze the visual and audio modalities. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.
      

      
        Generate one or more of the following types of text: titles, topics, and hashtags.
      

      
        This example prints the generated text to the standard output.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.
      

      
        Indexes help you organize and search through related videos efficiently. To create an index, you must provide its name and configure the [video understanding models](/v1.3/docs/concepts/models) you wish to enable. This example uses [Pegasus](/v1.3/docs/concepts/models/pegasus) as the video understanding model and specifies that the platform should analyze the visual and audio modalities. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.
      

      
        Generate one of the following types of text: summaries, chapters, or highlights.
      

      
        This example prints the generated text to the standard output.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.
      

      
        Indexes help you organize and search through related videos efficiently. To create an index, you must provide its name and configure the [video understanding models](/v1.3/docs/concepts/models) you wish to enable. This example uses [Pegasus](/v1.3/docs/concepts/models/pegasus) as the video understanding model and specifies that the platform should analyze the visual and audio modalities. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.
      

      
        Analyze your videos to generate open-ended text based on their content.
      

      
        This example prints the generated text to the standard output.
      
    
  



# Create embeddings

This quickstart guide provides a simplified introduction to generating text from video using the TwelveLabs Video Understanding Platform. It includes:

* A basic working example
* Minimal implementation details
* Core parameters for common use cases

For a comprehensive guide, see the [Create embeddings](/v1.3/docs/guides/create-embeddings) section.

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The videos you wish to use must meet the following requirements:
  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.

  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.

  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  * **Duration**: Must be between 4 seconds and 2 hours (7,200s).

  * **File size**:  Must not exceed 2 GB.\
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

* The audio files you wish to use use must meet the following requirements:
  * **Format**: WAV (uncompressed), MP3 (lossy), and FLAC (lossless)
  * **File size**: Must not exceed 10MB.

* The images you wish to use use must meet the following requirements:
  * **Format**: JPEG and PNG.
  * **Dimension**:  Must be at least 128 x 128 pixels.
  * **Size**: Must not exceed 5MB.

# Starter code

You can copy and paste the code below to create embeddings. Replace the placeholders surrounded by `<>` with your values.


  
    
      ```Python Python maxLines=8
      from twelvelabs import TwelveLabs
      from typing import List
      from twelvelabs.models.embed import EmbeddingsTask, SegmentEmbedding

      client = TwelveLabs(api_key="")

      task = client.embed.task.create(model_name="Marengo-retrieval-2.7", video_url="" });

      let task = await client.embed.task.create("Marengo-retrieval-2.7", {url: ""});
      console.log(`Created task: id=${task.id} modelName=${task.modelName} status=${task.status}`);

      const status = await task.waitForDone(5000, (task) => { console.log(`  Status=${task.status}`);});
      console.log(`Embedding done: ${status}`);

      task = await task.retrieve({ embeddingOption: ["visual-text", "audio"] });

      const printSegments = (segments, maxElements = 5) => {
        segments.forEach((segment) => {
          console.log(`embeddingScope=${segment.embeddingScope} embeddingOption=${segment.embeddingOption} startOffsetSec=${segment.startOffsetSec} endOffsetSec=${segment.endOffsetSec}`);
          console.log("embeddings: ", segment.embeddingsFloat.slice(0, maxElements));
        });
      };
      if (task.videoEmbedding) {
        if (task.videoEmbedding.segments) {
          printSegments(task.videoEmbedding.segments);
        }
      }
      ```
    
  

  
    
      ```Python Python maxLines=8
      from twelvelabs import TwelveLabs
      from typing import List
      from twelvelabs.models.embed import SegmentEmbedding

      client = TwelveLabs(api_key="")
                  
      res = client.embed.create(model_name="Marengo-retrieval-2.7",text="")

      print(f"Created text embedding: model_name={res.model_name}")
      def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
          for segment in segments:
              print(f"  embeddings: {segment.embeddings_float[:max_elements]}")

      if res.text_embedding is not None and res.text_embedding.segments is not None:
              print_segments(res.text_embedding.segments)
      ```

      ```JavaScript Node.js maxLines=8
      import { TwelveLabs} from "twelvelabs-js";

      const client = new TwelveLabs({ apiKey: "" });

      let res = await client.embed.create({modelName: "Marengo-retrieval-2.7", text: ""});

      console.log(`Created text embedding: modelName=${res.modelName}`);
      const printSegments = (segments, maxElements = 5) => {
      segments.forEach((segment) => {
              console.log("  embeddings: ",segment.embeddingsFloat.slice(0, maxElements));
      });
      };
      if (res.textEmbedding?.segments) {
              printSegments(res.textEmbedding.segments);
      }
      ```
    
  

  
    
      ```Python Python maxLines=8
      from twelvelabs import TwelveLabs
      from typing import List
      from twelvelabs.models.embed import SegmentEmbedding

      client = TwelveLabs(api_key="")

      res = client.embed.create(model_name="Marengo-retrieval-2.7", image_url="")

      print(f"Created image embedding: model_name={res.model_name}")
      def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
          for segment in segments:
              print(f"  embeddings: {segment.embeddings_float[:max_elements]}")
      if res.image_embedding is not None and res.image_embedding.segments is not None:
          print_segments(res.image_embedding.segments)
      ```

      ```JavaScript Node.js maxLines=8
      import { TwelveLabs } from "twelvelabs-js";

      const client = new TwelveLabs({ apiKey: "" });

      const res = await client.embed.create({modelName: "Marengo-retrieval-2.7", imageUrl: ""});

      console.log(`Created image embedding: modelName=${res.modelName}`);
      const printSegments = (segments, maxElements = 5) => {
          segments.forEach((segment) => {
            console.log("  embeddings: ",segment.embeddingsFloat.slice(0, maxElements));
          });
        };
      if (res.imageEmbedding?.segments) {
        printSegments(res.imageEmbedding.segments);
      }
      ```
    
  

  
    
      ```Python Python maxLines=8
      from twelvelabs import TwelveLabs
      from typing import List
      from twelvelabs.models.embed import SegmentEmbedding

      client = TwelveLabs(api_key="")

      res = client.embed.create(model_name="Marengo-retrieval-2.7", audio_url="")

      print(f"Created audio embedding: model_name={res.model_name}")
      def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
          for segment in segments:
              print(f"  start_offset_sec={segment.start_offset_sec}")
              print(f"  embeddings: {segment.embeddings_float[:max_elements]}")
      if res.audio_embedding is not None and res.audio_embedding.segments is not None:
          print_segments(res.audio_embedding.segments)
      ```

      ```JavaScript Node.js maxLines=8
      import { TwelveLabs } from "twelvelabs-js";

      const client = new TwelveLabs({ apiKey:"" });

      const res = await client.embed.create({modelName: "Marengo-retrieval-2.7", audioUrl: ""});

      console.log(`Created audio embedding: modelName=${res.modelName}`);
      const printSegments = (segments, maxElements = 5) => {
          segments.forEach((segment) => {
            console.log(`  start_offset_sec=${segment.startOffsetSec}`);
            console.log("  embeddings: ",segment.embeddingsFloat.slice(0, maxElements));
          });
        };
      if (res.audioEmbedding?.segments) {
        printSegments(res.audioEmbedding.segments);
      }
      ```
    
  


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish processing them.
      

      
        The platform requires some time to process videos. Check the status of the video embedding task until it's completed.
      

      
        Once the platform has finished processing your video, you can retrieve the embeddings.
      

      
        This example iterates over the results and prints the key properties and a portion of the embedding vectors for each segment to the standard output.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.
      

      

      
        This example prints a portion of the embedding vectors to the standard output.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.
      

      

      
        This example prints a portion of the embedding vectors to the standard output.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.
      

      

      
        This example prints a portion of the embedding vectors to the standard output.
      
    
  



# Manage your plan

TwelveLabs offers two plans: Free and Developer. For a detailed comparison and to understand the resources available with each plan, see the Pricing page. After signing up, you are automatically assigned the Free plan. If your needs exceed the Free plan, consider upgrading to the Developer plan.

# Upgrade your plan

To switch to the Developer plan, complete the following steps:


  
    Go to the Playground page and log in using your credentials.
  

  
    Select the **Upgrade** button:
    ![](file:08bbb48e-af7a-406d-a1b1-15d5dfb80f9a)
  

  
    Provide all the required information.
  

  
    When you've finished, select the **Upgrade** button.
  


# Downgrade your plan

To switch to the Free plan, complete the following steps:


  
    Go to the Billing page and log in using your credentials.
  

  
    Select **Cancel Enrollment**:
    ![](file:a1c0e09e-d286-4fba-9991-67adcaa0158e)
  

  
    Follow the provided instructions.
  

  
    When you've finished, select the **Proceed** button.
  


Note the following about downgrading your plan:

* Your indexes expire 90 days from the date of their creation.
* The platform maintains the total duration of your indexed videos without resetting it.


# Rate limits

Some endpoints are rate-limited to ensure fair usage and maintain optimal performance for all users.

# Rate-limited endpoints

The table below shows the number of requests allowed per rate limit window, which is one day (24 hours). Note that rate limits differ based on the [subscription plan](https://www.twelvelabs.io/pricing) and endpoint.

| Endpoint                                                                                                                                                                                                                                  | Free plan | Developer plan |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------- | :------------- |
| [Any-to-video search](/v1.3/api-reference/any-to-video-search/make-search-request)                                                                                                                                                        | 50        | 3000           |
| [Generate topics, titles, or hashtags](/v1.3/api-reference/analyze-videos/gist)                                                                                                                                                           | 50        | 3000           |
| [Generate summaries, chapters, or highlights](/v1.3/api-reference/analyze-videos/summarize)                                                                                                                                               | 50        | 3000           |
| [Perform open-ended analysis](/v1.3/docs/guides/analyze-videos/open-ended-analysis)                                                                                                                                                       | 50        | 3000           |
| [Create embeddings for text, image, and audio](/v1.3/api-reference/text-image-audio-embeddings/create-text-image-audio-embeddings)
[Create a video embedding task](/v1.3/api-reference/video-embeddings/create-video-embedding-task) | 100       | 1000           |
| [Create video indexing tasks](/v1.3/api-reference/tasks/create)                                                                                                                                                                           | 50        | 1000           |


  When using an organization account, rate limits apply in aggregate to all users in the organization.


# Exceeding rate limits

If you exceed the rate limit for a specific endpoint, the API returns an `HTTP 429 - Too many requests` error response.

# Upgrade your plan

After signing up, you are automatically assigned the Free plan. If your needs exceed the Free plan, consider [upgrading to the Developer plan](/v1.3/docs/get-started/manage-your-plan#upgrade-your-plan).

# Headers

The headers in each response provide information about your rate limits:

* `X-Ratelimit-Limit`: The maximum number of requests you can make per rate limit window for the specific endpoint. This value depends on your plan and the endpoint, as shown in the table above.
* `X-Ratelimit-Remaining`: The number of requests remaining in the current rate limit window for the specific endpoint. This value decreases with each request you make to that endpoint and resets at the start of the next rate limit window.
* `X-Ratelimit-Used`: The number of requests you have made in the current rate limit window for the specific endpoint. This value increases with each request you make to that endpoint and resets to zero at the start of the next rate limit window.
* `X-Ratelimit-Reset`: The time at which the current rate limit window resets, expressed in UTC epoch seconds. After this time, the values of the `X-Ratelimit-Remaining` and `X-Ratelimit-Used` parameters will be reset to their initial values for the next rate limit window.


# Release notes

The sections below list all new features, enhancements, and changes to the platform, in chronological order.

# Version 1.3

## July 18, 2025

### The pre-release version 1.0.0 of the Python and Node.js SDKs is now available

TwelveLabs is proud to introduce redesigned SDKs that incorporate modern development practices to enhance the developer experience.


  Version 1.0.0 is a pre-release version. Version 0.4.x remains the current stable version.


#### Key Improvements

* **Enhanced type safety and development experience**: Complete typing support, full type hints, auto-completion, and Pydantic v1/v2 compatibility for superior IDE integration, safer code, and modern tooling.
* **Auto-generated architecture**: Built directly from the OpenAPI specifications, ensuring API consistency and automatic updates with new features.
* **Improved pagination**: Automatic page fetching, allowing easier handling of large datasets.
* **Native async support**: The Python SDK now fully supports `async`/`await` with a dedicated asynchronous client.
* **Improved documentation**: Detailed docstrings/JSDoc comments and thorough SDK Reference sections that outline all parameters and response fields.

#### Actions required

The pre-release version includes breaking changes from the stable 0.4.x release. Before you use version 1.0.0 in your application:

* Update your code to work with the new version
* Test all changes thoroughly
* Verify that everything functions as expected before you deploy to production.


  If you encounter any issues with this pre-release version, please report them on the Issues page of the respective GitHub repository:

  * [twelvelabs-python](https://github.com/twelvelabs-io/twelvelabs-python/issues).
  * [twelvelabs-js](https://github.com/twelvelabs-io/twelvelabs-js/issues)


#### Version support

TwelveLabs will stop maintaining versions up to 0.4.x after the stable 1.0.0 version is officially released.

#### Documentation

The documentation has been updated with current examples, detailed guides, and comprehensive SDK Reference sections.

View the [updated documentation ](https://beta.docs.twelvelabs.io).

## July 16, 2025

### Marengo 2.7 and Pegasus 1.2 are now available in Amazon Bedrock

Amazon Bedrock now supports the Marengo 2.7 and Pegasus 1.2 video understanding models.

**Regional availability**

* **Marengo 2.7**: Available in US East (N. Virginia), Europe (Ireland), and Asia Pacific (Seoul)
* **Pegasus 1.2**: Available in US West (Oregon) and Europe (Ireland) through cross-region inference

For details on accessing these models, see the [TwelveLabs in Amazon Bedrock](https://aws.amazon.com/bedrock/twelvelabs/) page.

For details on pricing, see the [Amazon Bedrock pricing](https://aws.amazon.com/bedrock/pricing/) page.

## July 11, 2025

### Enhanced user metadata capabilities

The platform now provides advanced capabilities for managing user metadata, offering improved flexibility and precision:

* **Upload videos with metadata**: When you upload a video, you can now include user-defined metadata in the request body. This metadata will be attached to the video, helping you categorize it effectively.

* **Update or delete metadata fields**: After a video has been uploaded to the platform, you can update specific metadata fields, such as the file name, or delete them by setting the value to `null`.

* **Controls metadata in search results**: When you make search requests, you can specify whether user-defined metadata appears in the search results.


  To use these new capabilities, you must invoke the API directly. These changes will be supported in a future version of the official SDKs.


For more details on these enhancemenents, refer to the updated documentation:

* [Create a video indexing task](/v1.3/api-reference/tasks/create)
* [Partial update video information](/v1.3/api-reference/videos/partial-update-video-information)
* [Make any-to-video search requests](/v1.3/api-reference/any-to-video-search/make-search-request)
* [Retrieve a specific page of search results](/v1.3/api-reference/any-to-video-search/retrieve-page).

## June 24, 2025

### Organizations now support self-service creation and are available to all users

The Organizations feature has been improved to offer better accessibility and more control. This update includes several key changes:

* **Self-service creation**: You can now create and manage your organization directly from the [Playground](http://playground.twelvelabs.io).
* **Available to all users**: The Organizations feature is no longer restricted to paid plans.
* **Enhanced administrative features**: Administrators now have the ability to upgrade or downgrade their organization's plan.

For detailed instructions on setting up and managing your organization, see the updated [Organizations](/docs/advanced/organizations) section.
If you previously had an organization set up through our sales team, your existing organization remains unchanged.

## June 13, 2025

### Marengo 2.6 and API v1.2 have been deprecated

The Marengo video understanding model version 2.6 has been deprecated. To ensure your applications remain compatible, please update them to use Marengo version 2.7.

Additionally, the API version 1.2 has also been deprecated. You must update your applications to utilize API version 1.3.

For detailed instructions on both updates, refer to the [Migration guide](/v1.3/docs/resources/migration-guide) page.

## June 4, 2025

### The Generate API has been renamed to the Analyze API

The Generate API has been renamed to the Analyze API to more accurately reflect its purpose of analyzing videos to generate text. This update includes changes to specific API endpoints and SDK methods, outlined below. You can continue using the Generate API until July 30, 2025. After this date, the Generate API will be deprecated, and you must transition to the Analyze API.

**Changes to the API**:

* The `/generate` endpoint is now the `/analyze` endpoint.
* The `/gist` endpoint remains unchanged.
* The `/summarize` endpoint remains unchanged.

**Changes to SDKs**:

The `generate` prefix has been removed from method names, and the methods below have been renamed as follows:

* `generate.gist` is now `gist`
* `generate.summarize` is now `summarize`
* `generate.text` is now `analyze`
* `generate.text_stream` is now `analyze_stream ` (Python)
* `generate.textStream` is now `analyzeStream` (Node.js)

To maintain compatibility, update your API calls and SDK methods to the new names before July 30, 2025. For additional details, refer to the following resources:

* [API Reference](/api-reference/analyze-videos)
* [Python SDK Reference](/sdk-reference/python/analyze-videos)
* [Node.js SDK Reference](/sdk-reference/node-js/analyze-videos)

## May 16, 2025

### Increased prompt length

The maximum prompt length for the [`/analyze`](/api-reference/analyze-videos/analyze) and [`/summarize`](/api-reference/analyze-videos/summarize) endpoints is now 2,000 tokens.

## May 9, 2025

### Support for retrieving transcriptions

The platform now allows you to retrieve transcriptions for your videos. This update adds new functionality without breaking existing code and affects the following endpoints:

* `/indexes/:index-id/videos/:video-id`: Set the new `transcription` query parameter to `true` to retrieve transcriptions. The platform will include a field named `transcription` in the response. It's an array of objects, each consisting of the time range and spoken words for that segment. For more details, see the [Retrieve video information](/api-reference/videos/retrieve) page.
* `/search`: The response now includes a field named `transcription` for each match found. It's a string that contains the transcription of the spoken words in the video. For more details, see the [Make any-to-any search requests](/api-reference/any-to-video-search/make-search-request) page.

## April 15, 2025

### New embedding retrieval options

The Embed API now provides more control over the types of embeddings you retrieve. This is a breaking change that requires updating your code and affects the following endpoints:

* `/indexes/:index-id/videos/:video-id`: The `embed` parameter has been deprecated and replaced with the new `embedding_option` parameter, allowing you to retrieve specific types of embeddings. For details, see the [Retrieve video information](/api-reference/videos/retrieve) page.
* `/embed/tasks/:task_id`: You can now use the optional `embedding_option` parameter to specify which types of embeddings to retrieve. For details, see the [Retrieve video embeddings](/api-reference/video-embeddings/retrieve-video-embeddings) page.

In the responses from both endpoints, each segment now includes a new field named `embedding_option` located at `video_embedding.segments[].embedding_option`, which identifies the type of embedding as either "visual-text" or "audio."


  If you use the TwelveLabs SDKs, ensure you have updated to the latest version.


## March 18, 2025

### User-provided transcriptions have been deprecated

The [`POST`](/api-reference/tasks/create) method of the `/tasks` endpoint no longer processes transcription data submitted through the following parameters:

* `provide_transcription`
* `transcription_file`
* `transcription_url`

Note that the platform will not return an error if you include these parameters in a request.

## March 3, 2025

### Introducing the Organizations feature

TwelveLabs is excited to announce the launch of the Organizations feature, available exclusively for Enterprise customers. This feature enables the sharing of indexes, videos, and S3 integrations across the organization.

Enterprise customers interested in setting up an organization can do so by contacting our sales team at [sales@twelvelabs.io](mailto:sales@twelvelabs.io). Once the organization is configured, administrators will be able to invite team members and start collaborating.

For more details about this feature, see the [Organizations](/docs/advanced/organizations) page.

## February 11, 2025

### Pegasus 1.2 has been released

TwelveLabs announces the official release of the Pegasus 1.2 video understanding model. For details on the new features and improvements in this version, refer to this blog post: [Introducing Pegasus 1.2: An Industry-Grade Video Language Model for Scalable Applications](https://www.twelvelabs.io/blog/introducing-pegasus-1-2).

Note that you can no longer use Pegasus 1.1 to create new indexes, and this version will be discontinued on February 25, 2025. All existing Pegasus 1.1 indexes will automatically be upgraded to Pegasus 1.2 on a rolling basis. No manual intervention is required for this migration process, and all indexes will utilize Pegasus 1.2 upon completion.

## January 13, 2025

### Pegasus 1.2 public preview

TwelveLabs announces the public preview release of Pegasus 1.2, our latest video understanding model.

**Key improvements**:
The new model offers significant improvements over Pegasus 1.1:

* Extended video processing capacity from 30 minutes to 1 hour per video.
* Enhanced performance across video-language tasks compared to Pegasus 1.1 and other models of the same size.
* More granular visual comprehension of objects, on-screen text, and numerical content.
* More accurate temporal grounding and timestamp identification. For example, you can ask questions about the timestamps of certain events.

During the preview phase, the model is available only for new and sample indexes. Existing Pegasus 1.1 indexes remain fully supported. All current indexes will be automatically migrated to Pegasus 1.2 at no cost during the official release (date to be announced).

Note that the model may produce occasional errors or hallucinations. For support or feedback, contact [support@twelvelabs.io](mailto:support@twelvelabs.io).

## December 2, 2024

TwelveLabs is proud to introduce the following new features and improvements:

* **Marengo 2.7**: This new version of the Marengo video understanding engine improves accuracy and performance in the following areas:
  * Multimodal processing that combines visual, audio, and text elements.
  * Fine-grained image-to-video search: detect brand logos, text, and small objects (as small as 10% of the video frame).
  * Improvement in motion search capability.
  * Counting capabilities.
  * More nuanced audio comprehension: music, lyrics, sound, and silence.
    For more details on the new features and improvements in this version, refer to this blog post: [Introducing Marengo 2.7: Pioneering Multi-Vector Embeddings for Advanced Video Understanding](https://www.twelvelabs.io/blog/introducing-marengo-2-7).
* Simplified modalities:
  * `visual`:  includes objects, actions, text OCR, logos.
  * `audio`: includes speech, music, and ambient sounds.
  * `conversation` has been deprecated.
  * `text_in_video` and `logo` are now part of `visual`.
* Streamlined endpoint structure: Several endpoints and parameters have been deprecated, removed, or renamed.


  - The 1.3 version of the API version only supports Marengo 2.7.
  - Marengo 2.7 generates embeddings that are not backward compatible. You must reindex all your videos and regenerate all your embeddings with Marengo 2.7.
  - The audio search feature in Marengo 2.7 works best with full-sentence queries. Short queries may yield suboptimal results. This limitation is temporary and will be addressed in a future release.


See the [Migration guide](/docs/resources/migration-guide) page for a detailed list of the changes and instructions on how you can update your code.

# Version 1.2

If you have used the 1.1.2 version of the API, please refer to the following section for important information regarding the changes.

## November 12, 2024

### Improvements

* **Cloud-to-cloud Integrations API**: The API has been updated to provide a more intuitive experience. The `/tasks/transfers` endpoint will be deprecated. Use the following endpoints instead:
  * [Import videos](/api-reference/tasks/cloud-to-cloud-integrations/create)
  * [Retrieve import status](/api-reference/tasks/cloud-to-cloud-integrations/get-status)
  * [Retrieve import logs](/api-reference/tasks/cloud-to-cloud-integrations/get-logs)
  
    Cloud-to-cloud integrations now require a paid plan. If you're on the Free plan, you can find information on upgrading your plan in the  [Upgrade your plan](/docs/get-started/manage-your-plan#upgrade-your-plan) section.
  

## November 5, 2024

### Improvements

* **Embed API**: The structure of the responses has been streamlined across all endpoints to provide a more consistent and intuitive experience:
  * **Standardized object naming**:
    * The `video_embeddings` field has been renamed to `video_embedding`.
    * The `video_embedding` object now encapsulates the embeddings, related metadata, and additional information.
  * **Enhanced response structure**:
    * The embedding vectors are now nested under an array named `segments`.
    * The `metadata` objects have been moved under their respective parent embedding objects.
    * The `is_success` boolean has been removed.
  * **Affected endpoints**:
    * [All video embedding endpoints](/api-reference/video-embeddings) the endpoint for creating video embedding tasks.
    * [Create embeddings for text, image, and audio](/api-reference/text-image-audio-embeddings).
    * [Retrieve video information](/api-reference/videos/retrieve).
* **Embed API**: You can now retrieve vector embeddings for any indexed video by setting `embed=true` in your [`GET`](/api-reference/videos/retrieve) `/indexes/{index-id}/videos/{video-id}` requests.

## October 24, 2024

### New features

* **Embed API**:
  * You can create image and audio embeddings in addition to its existing video and text capabilities. See the [Create embeddings](/docs/guides/create-embeddings) page for details.
  * You can now retrieve a list of the video embedding tasks in your account by invoking the [`GET`](/api-reference/video-embeddings/list-video-embedding-tasks)  method of the `/embed/tasks` endpoint.

## July 7, 2024

### New features

* **Pegasus 1.1**: The 1.1 version of the Pegasus video understanding engine has been released, introducing the following enhancements:

  * Improved model accuracy for video description and question-answering.
  * Fine-grained visual understanding and instruction following.
  * Streaming support when generating open-ended texts. For details, refer to the [Streaming responses](/docs/guides/analyze-videos/open-ended-analysis#streaming-responses) section.
  * Increased maximum prompt length to 375 tokens.
  * Extended maximum video duration to 30 minutes.

  
    Effective July 8, 2024, Pegasus 1.0 is no longer supported. All existing indexes created with Pegasus 1.0 will be automatically upgraded to Pegasus 1.1. No manual intervention is required for this migration process, and all indexes will utilize Pegasus 1.1 upon completion.
  

## June 18, 2024

### New features

* **Image-to-Video Search API**: TwelveLabs is proud to introduce the Image-to-Video Search API. This new API allows you to find semantically related video segments by providing an image as a query. The platform identifies similar content within videos. To get started with the Image-to-Video Search API, refer to the [Search](/docs/guides/search)page.

## May 15, 2024

### New features

* **Embed API**: TwelveLabs is proud to introduce the Embed API. You can use this new API to create multimodal embeddings that are contextual vector representations for your videos and texts. You can utilize multimodal embeddings in various downstream tasks, including but not limited to training custom multimodal models for applications such as clustering, classification, search, recommendation, and anomaly detection. See the [Create embeddings](/docs/guides/create-embeddings) page for details.

## March 12, 2024

### New features

* TwelveLabs is proud to introduce the new versions of its video understanding models:
  * **Marengo 2.6**: A new state-of-the-art (SOTA) multimodal foundation model capable of performing any-to-any search tasks, including Text-To-Video, Text-To-Image, Text-To-Audio, Audio-To-Video, Image-To-Video, and more. Note that the platform currently supports text-to-video search and classification features. Other modalities will be supported in a future release. This model represents a significant leap in video understanding technology, enabling more intuitive and comprehensive search capabilities across various media types. For an overview of the new features and improvements in this version, refer to this blog post: [Introducing Marengo 2.6: A New State-of-the-Art Video Foundation Model for Any-to-Any Search](https://www.twelvelabs.io/blog/introducing-marengo-2-6).
  * **Pegasus 1.0 beta**: This version of the model provides fine-grained video descriptions, summaries, and question-answering capabilities. For an overview of the new features and improvements in this version, refer to this blog post: [Pegasus-1 Open Beta: Setting New Standards in Video-Language Modeling](https://www.twelvelabs.io/blog/upgrading-pegasus-1).
* The platform now supports search queries in multiple languages. For a complete list of supported languages, refer to the [Supported languages](/docs/supported-languages) page.

### Updates

* You can now enable the Pegasus and Marengo video understanding engines on the same index.

## February 15, 2024

### New features

* You can now tune the temperature to control the randomness of the text output generated by the [`/summarize`](/api-reference/analyze-videos/summarize) and [`/generate`](/api-reference/analyze-videos/analyze) endpoints. See the [Tune the temperature](/docs/guides/analyze-videos/tune-the-temperature) page for details.

## October 30, 2023

### New features

Version 1.2 of the TwelveLabs Video Understanding Platform introduces the following new features:

* The alpha version of the Pegasus video understanding engine has been released. You can now use it to [analyze videos](/docs/guides/analyze-videos).
* You can now upload videos from external providers. Currently, only YouTube is supported as an external provider, but we will add support for additional providers in the future.

### Updates

This section lists the differences between version 1.1.2 and version 1.2 of the TwelveLabs Video Understanding API.

* When you make an API call, make sure that you specify the `1.2` version in the URL.
  The URL should look similar to the following one: `https://api.twelvelabs.io./v1.2/{resource}/{path_parameters}?{query_parameters}`. For more details, see the [Call an endpoint](/reference/api-reference#call-an-endpoint) section.
* To enable the utilization of multiple engines for an index, the following changes have been made:
  * **POST** `/indexes`: The `engine_id` and `indexing_options` parameters of the request have been deprecated. Instead, you can now define the engine configuration as a list of objects. See the [Create an index](/reference/create-index) page for details.
  * **GET** `/indexes/{index_id}`:  The `engine_id` field in the response has been superseded by an array of objects named `engines`. See the [Retrieve an index](/api-reference/indexes/retrieve) page for details.
  * **GET** `/indexes`:
    * The `engine_id` field in the response has been superseded by an array of objects named `engines`. See the [List indexes](/api-reference/indexes/list) page for details.
    * The `engine_family` query parameter has been introduced, allowing you to filter by engine family.
    * The `index_options` query parameter has been marked for deprecation. You can still use it in this version of the API, but it will be deprecated in a future release. Instead, use `engine_options` or `engine_family`.
  * **GET** `/engines`: The `allowed_index_option` field in the response has been renamed to `allowed_engine_options`.
  * **GET**`/engines/{engine-id}` The `allowed_index_option` field in the response has been renamed to `allowed_engine_options`. See the `Retrieve an engine page` for details.
* The `/search` and `/search/{page-token}` endpoints no longer return the `conversation_option`, `search_options`, and `query` fields.

# Version 1.1.2

If you have used the 1.1.1 version of the API, please refer to the following section for important information regarding the changes.

## Improvements

To further improve the usability of the `/classify` endpoint, the following changes have been made:

* The endpoint now allows you to classify a set of videos. The `video_id` parameter has been deprecated and now you must pass an array of strings named `video_ids` instead. Each element of the array represents the unique identifier of a video you want to classify.

* The `threshold` field in the request is now an object, and you can use it to filter based on the following criteria:

  * The confidence level that a video matches the specified class
  * The confidence level that a clip matches the specified class.
  * The duration ratio, which is the sum of the lengths of the matching video clips inside a video divided by the total length of the video.

  For details, see the `Filtering > Content classification` page.

* The endpoint now supports pagination.

* The duration-weighted score has been deprecated. When setting the `show_detailed_score` parameter to `true`, the platform now returns the maximum, average, and normalized scores.

# Version 1.1.1

If you have used the 1.1 version of the API, please refer to the following sections for important information regarding the changes.

## New features

Version 1.1.1 of the TwelveLabs Video Understanding Platform introduces the following new features:

* Version 2.5 of the Marengo video understanding engine has been released. For details, see the [Video understanding engines](/docs/concepts/models) page.
* The `/indexes`, `/search`, `/combined-search`, and `/classify` endpoints now support the ability to integrate with the [Playground](https://playground.twelvelabs.io), a sandbox environment that allows you to try out the features of the TwelveLabs Video Understanding Platform through an intuitive web page.
* The platform now supports the ability to store the video you're uploading. For details, see the [Create a video indexing task](/reference/create-video-indexing-task) page.

## Improvements

To further improve flexibility, usability, and clarity, the following changes have been made:

* **Combined queries**:
  * You can now define global values for the `search_options` and `conversation_option` parameters for the entire request instead of per-query basis. For details, see the **Use combined queries** page.
  * The `/beta/search` endpoint has been renamed to `/combined-search`.
* **Logo detection**: The `logo` add-on has been deprecated. To enable logo detection for an index, you must now use the `logo` indexing option.
* **Conversation option**:  The `transcription` conversation option has been renamed to `exact_match`.
* **Classifying videos**:
  * The `labels` parameter has been renamed to `classes`.
  * The `threshold` field you can use to narrow down a response obtained from the platform is now of type `int`. For details, see the **API Reference > Classify a video** page.

# Version 1.1

The introduction of new features and improvements in the 1.1 version of the TwelveLabs Video Understanding Platform has required changes to some endpoints. If you have used the 1.0 version of the API, please refer to the following sections for important information regarding the changes.

## New features

Version 1.1 of the TwelveLabs Video Understanding Platform introduces the following new features:

* **Classification of content:** You can now define a list of labels that you want to classify your videos into, and the new classification API endpoint will return the duration for which the specified labels have appeared in your videos and the confidence that each of the matches represents the label you've specified.
* **Combined Queries:** The `1.1` version of the API introduces a new format of search queries named combined queries. A combined query includes any number of subqueries linked with any number of logical operators. Combined queries are executed in one API request.
  Combined queries support the following additional features:
  * **Negating a condition**: In addition to the existing `AND` operator, the platform now allows you to use the `NOT` operator to negate a condition. For example, this allows you to write a query that retrieves all the video clips in which someone is cooking but neither spaghetti nor lasagna is mentioned in the conversation.
  * **The THEN operator**:  The platform now supports the `THEN` operator that allows you to specify that the platform must return only the results for which the order of the matching video clips is the same as the order of your queries.
  * **Time-proximity search**: The TwelveLabs Video Understanding API now allows you to use the `proximity` parameter to extend the lower and upper boundaries of each subquery. For example, this allows you to write a query that finds all car accidents that happened within a specific interval of time before someone wins a race.
    For details, see the **Use combined queries** page.
* **Logo detection**: The platform can now detect brand logos.

## Updates

This section lists the differences between version 1 and version 1.1 of the TwelveLabs Video Understanding API.

* When you make an API call, make sure that you specify the `1.1` version in the URL.
  The URL should look similar to the following one: `https://api.twelvelabs.io./v1.1/{resource}/{path_parameters}?{query_parameters}`. For more details, see the [Call an endpoint](/reference/api-reference#call-an-endpoint) section.
* The following methods now return a `200 OK` status code when the response is empty:
  * `[GET] /indexes`
  * `[GET] /tasks`
  * `[GET] /indexes/{index_id}/videos`
* The `/tasks` endpoint is now a separate endpoint and is no longer part of the `/indexes` endpoint. The table below shows the changes made to each method of the `/tasks` endpoint:

  | 1.0                               | 1.1                       |
  | --------------------------------- | ------------------------- |
  | GET `/indexes/tasks`              | GET `/tasks`              |
  | POST `/indexes/tasks`             | POST `/tasks`             |
  | GET `/indexes/tasks/{task_id}`    | GET `/tasks/{task_id}`    |
  | DELETE `/indexes/tasks/{task_id}` | DELETE `/tasks/{task_id}` |
  | POST `/indexes/tasks/transfers`   | POST `/tasks/transfers`   |
  | GET `/indexes/tasks/status`       | GET `/tasks/status`       |
* The `/indexes/tasks/{task_id}/video_id` endpoint has been deprecated. You can now retrieve the unique identifier of a video by invoking the GET method of the `/tasks/{task_id}` endpoint. The response will contain a field named `video_id`.
* When an error occurs, the platform now follows the recommendations of the [RFC 9110](https://www.rfc-editor.org/rfc/rfc9110.html) standard. Instead of numeric codes, the platform now returns string values containing human-readable descriptions of the errors. The format of the error messages is as follows:

  * `code`: A string representing the error code.
  * `message`: A human-readable string describing the error, intended to be suitable for display in a user interface.
  * *(Optional)* `docs_url`: The URL of the relevant documentation page.
    For example, if you tried to list all the videos in an index and the unique identifier of the index you specified didn't exist, the `1.0` version of the API returned an error similar to the following one:

  ```json
  {
    "error_code": 201,
    "message": "ID 234234 does not exist"
  }
  ```

  Now, when using the `1.1` version of the API, the error should look similar to the following one:

  ```json
  {
    "code": "parameter_not_provided",
    "message": "The index_id parameter is required but was not provided."
  }
  ```

  For a list of error messages, see the [API Reference > Error codes](/reference/error-codes) page.
* The `next_page_id` and `prev_page_id` fields of the `page_info` object have been renamed to `next_page_token` and `prev_page_id.`
* The `type` field has been removed from all the responses.
* When performing searches specifying multiple search options, the platform returns an object containing the confidence level that a specific video clip matched your search terms for each type of search. In version `v1.0`, this field was a dictionary named `module_confidence`.  In version `v1.1`, this field is now named `module` and is of type `array`.
* The POST method of the `/search/{page-token}` endpoint has been deprecated. To retrieve the subsequent pages, you must now call the GET method of the `/search/{page-token}` endpoint, passing it the unique identifier of the page you want to retrieve.


# Search

The TwelveLabs Video Understanding Platform analyzes videos by integrating images, audio, speech, and text, offering a deeper understanding than single-modal methods. It captures complex relationships between these elements, detects subtle details, and supports natural language queries and images for intuitive and precise use.

Key features:

* **Improved accuracy**: Multimodal integration enhances accuracy.
* **Easy interaction**: Natural language queries simplify searches.
* **Advanced search**: Enables image-based queries for precise results.
* **Fewer errors:** Multi-faceted analysis reduces misinterpretation.
* **Time savings**: Quickly finds relevant clips without manual review.

Use cases:

* **Spoken word search**: Find video segments where specific words or phrases are spoken.
* **Visual element search**: Locate video segments that match descriptions of visual elements or scenes.
* **Action or event search**: Identify video segments that depict specific actions or events.
* **Image similarity search**: Find video segments that visually resemble a provided image.

To understand how your usage is measured and billed, see the [Pricing](https://www.twelvelabs.io/pricing) page.


  You can only perform searches at the individual index level, meaning you can only search within one index per request and cannot search at the video level or across multiple indexes simultaneously.


{/* This guide provides a complete example. For a simplified introduction with just the essentials, see the [Search](/v1.3/docs/get-started/quickstart/search) quickstart guide. */}

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The videos you wish to use must meet the following requirements:
  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.

  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.

  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  * **Duration**: Must be between 4 seconds and 2 hours (7,200s).

  * **File size**:  Must not exceed 2 GB.\
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

* If you wish to use images as queries, ensure that your images meet the following requirements:
  * **Format**: JPEG and PNG.
  * **Dimension**: Must be at least 64 x 64 pixels.
  * **Size**: Must not exceed 5MB.

# Complete example

This complete example shows how to create an index, upload a video, and perform search requests using text and image queries. Ensure you replace the placeholders surrounded by `<>` with your values.


  ```python Python maxLines=8
  from twelvelabs import TwelveLabs
  from twelvelabs.models.task import Task

  # 1. Initialize the client
  client = TwelveLabs(api_key="")

  # 2. Create an index
  models = [ {"name": "marengo2.7", "options": ["visual", "audio"]}]
  index = client.index.create(name="", models=models)
  print(f"Index created: id={index.id}, name={index.name}")

  # 3. Upload a video
  task = client.task.create(index_id=index.id, url="")
  print(f"Task id={task.id}, Video id={task.video_id}")

  # 4. Monitor the indexing process
  def on_task_update(task: Task):
      print(f"  Status={task.status}")
  task.wait_for_done(sleep_interval=5, callback=on_task_update)
  if task.status != "ready":
      raise RuntimeError(f"Indexing failed with status {task.status}")
  print(f"The unique identifier of your video is {task.video_id}.")

  # 5. Perform a search request
  search_results = client.search.query(
      index_id=index.id,
      query_text="",
      options=["visual", "audio"],
      # operator="or"
  )

  # 6. Process the search results
  def print_page(page):
      for clip in page:
          print(
              f" video_id={clip.video_id} score={clip.score} start={clip.start} end={clip.end} confidence={clip.confidence}"
          )
  print_page(search_results.data)
  while True:
      try:
          print_page(next(search_results))
      except StopIteration:
          break
  ```

  ```javascript Node.js maxLines=8
  import { TwelveLabs } from "twelvelabs-js";

  // 1. Initialize the client
  const client = new TwelveLabs({ apiKey: "" });

  // 2. Create an index
  const models = [{ name: "marengo2.7", options: ["visual", "audio"] }];
  const index = await client.index.create({
    name: "",
    models: models,
  });
  console.log(`Index created: id=${index.id} name=${index.name}`);

  // 3. Upload a video
  const task = await client.task.create({
    indexId: index.id,
    url: "",
  });
  console.log(`Task id=${task.id} Video id=${task.videoId}`);

  // 4. Monitor the indexing process
  await task.waitForDone(5000, (task) => {
    console.log(`  Status=${task.status}`);
  });

  if (task.status !== "ready") {
    throw new Error(`Indexing failed with status ${task.status}`);
  }
  console.log(`The unique identifier of your video is ${task.videoId}`);

  // 5. Perform a search request
  let searchResults = await client.search.query({
    indexId: index.id,
    queryText: "",
    options: ["visual", "audio"],
    // operator: "or",
  });

  // 6. Process the search results
  printPage(searchResults.data);
  while (true) {
    const page = await searchResults.next();
    if (page === null) break;
    else printPage(page);
  }
  function printPage(searchData) {
    searchData.forEach((clip) => {
      console.log(
        `video_id= ${clip.videoId} score=${clip.score} start=${clip.start} end=${clip.end} confidence=${clip.confidence}`
      );
    });
  }
  ```


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/python/the-twelve-labs-class#the-initializer) of the `TwelveLabs` class.

        **Parameters**:

        * `api_key`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        Indexes help you organize and search through related videos efficiently. This example creates a new index, but you can also use an existing index by specifying its unique identifier. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.

        **Function call**: You call the [`index.create`](/v1.3/sdk-reference/python/manage-indexes#create-an-index) function.

        **Parameters**:

        * `name`: The name of the index.
        * `models`: An object specifying your model configuration. This example enables the Marengo video understanding model and the `visual` and `audio` model options.


        **Return value**: An object containing, among other information, a field named `id` representing the unique identifier of the newly created index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.

        **Function call**: You call the [`task.create`](/v1.3/sdk-reference/python/upload-videos#create-a-video-indexing-task) function.

        **Parameters**:

        * `index_id`: The unique identifier of your index.
        * `url` or `file`: The publicly accessible URL or the path of your video file.


        **Return value**: An object that contains the unique identifier of your video and the status of the associated video indexing task. You can use this object to track the status of your video upload and indexing process.

        
          You can also upload multiple videos in a single API call. For details, see the [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations) page.
        
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.

        **Function call**: You call the [`task.wait_for_done`](/v1.3/sdk-reference/python/upload-videos#wait-for-a-video-indexing-task-to-complete) function.

        **Parameters**:

        * `sleep_interval`: The time interval, in seconds, between successive status checks. In this example, the method checks the status every five seconds.
        * `callback`:  A callback function that the SDK executes each time it checks the status.


        **Return value**: An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        Perform a search within your index using a text or image query.

        
          
            **Function call**: You call the [`search.query`](/v1.3/sdk-reference/python/search#make-a-search-request) method.

            **Parameters**:

            * `index_id`: The unique identifier of the index.
            * `query_text`: Your search query. Note that the platform supports full natural language-based search.
            * `options`: The modalities the platform uses when performing a search. This example searches using visual cues.
            * *(Optional)* `operator`: The logical operator, either `or` or `and`, specifies how your search combines multiple sources of information; it defaults to `or`. Use this parameter when the `options` parameter lists more than one source of information. For example, when you set this parameter to `and`, the search returns video segments matching all specified sources of information. 


            **Return value**: An object which contains the following fields:

            * `data`: A list of objects representing the search results, each of which contains the following fields:
              * `video_id`: The unique identifier of the video that matched your search terms.
              * `start`: The start time of the matching video clip, expressed in seconds.
              * `end`: The end time of the matching video clip, expressed in seconds.
              * `score`: A quantitative value determined by the platform representing the level of confidence that the results match your search terms.
            * `page_info`: An object that provides information about pagination. The platform returns the results one page at a time, with a default limit of 10 results per page.
            * `pool`: An object that contains the total number of videos within the index, the total duration of the videos, and the unique identifier of the index that you’ve searched.

            For details about all fields, see the [API Reference > Make any-to-video search requests page](/v1.3/api-reference/any-to-video-search/make-search-request).
          

          
            **Function call**: You call the [`search.query`](/v1.3/sdk-reference/python/search#make-a-search-request) method.

            **Parameters**:

            * `index_id`: The unique identifier of the index.
            * `query_media_type`: The type of query. It must be set to "image".
            * `query_media_file` or `query_media_url`: The path or the publicly accesible URL of your image file.

            * `options`: The modalities the platform uses when performing a search. This example searches using visual cues.


            **Return value**: An object which contains the following fields:

            * `data`: A list of objects representing the search results, each of which contains the following fields:
              * `video_id`: The unique identifier of the video that matched your search terms.
              * `start`: The start time of the matching video clip, expressed in seconds.
              * `end`: The end time of the matching video clip, expressed in seconds.
              * `score`: A quantitative value determined by the platform representing the level of confidence that the results match your search terms.
            * `page_info`: An object that provides information about pagination. The platform returns the results one page at a time, with a default limit of 10 results per page.
            * `pool`: An object that contains the total number of videos within the index, the total duration of the videos, and the unique identifier of the index that you’ve searched.

            For details about all fields, see the [API Reference > Make any-to-video search requests page](/v1.3/api-reference/any-to-video-search/make-search-request).
          
        
      

      
        Display the search results, including handling pagination to retrieve all pages.

        **Function call**: Iterate over the results by calling the `next` function.

        **Parameters**: The search results retrieved in the previous step.


        **Return value**: The next page or raises a `StopIteration` exception if the iterator has reached the last page.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/node-js/the-twelve-labs-class#the-constructor) of the `TwelveLabs` class.

        **Parameters**:

        * `apiKey`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        Indexes help you organize and search through related videos efficiently. This example creates a new index, but you can also use an existing index by specifying its unique identifier. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.

        **Function call**: You call the [`index.create`](/v1.3/sdk-reference/node-js/manage-indexes#create-an-index) function.

        **Parameters**:

        * `name`: The name of the index.
        * `models`: An object specifying your model configuration. This example enables the Marengo video understanding model and the `visual` and `audio` model options.


        **Return value**: An object containing, among other information, a field named `id` representing the unique identifier of the newly created index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.

        **Function call**: You call the [`task.create`](/v1.3/sdk-reference/node-js/upload-videos#create-a-video-indexing-task) function.

        **Parameters**:

        * `indexId`: The unique identifier of your index.
        * `url` or `file`: The publicly accessible URL or the path of your video file.


        **Return value**: An object that contains the unique identifier of your video and the status of the associated video indexing task. You can use this object to track the status of your video upload and indexing process.

        
          You can also upload multiple videos in a single API call. For details, see the [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations) page.
        
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.

        **Function call**: You call the [`task.waitForDone`](/v1.3/sdk-reference/node-js/upload-videos#wait-for-a-video-indexing-task-to-complete) function.

        **Parameters**:

        * `sleepInterval`: The time interval, in milliseconds, between successive status checks. In this example, the method checks the status every five seconds.
        * `callback`:  A callback function that the SDK executes each time it checks the status.


        **Return value**: An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        Perform a search within your index using a text or image query.

        
          
            **Function call**: You call the [`search.query`](/v1.3/sdk-reference/node-js/search#make-a-search-request) method.

            **Parameters**:

            * `indexId`: The unique identifier of the index.
            * `queryText`: Uour search query. Note that the platform supports full natural language-based search.
            * `options`: An array of strings specifying the modalities the platform uses when performing a search. This example searches using visual cues.
            * *(Optional)* `operator`: The logical operator, either `or` or `and`, specifies how your search combines multiple sources of information; it defaults to `or`. Use this parameter when the `options` parameter lists more than one source of information. For example, when you set this parameter to `and`, the search returns video segments matching all specified sources of information. 


            **Return value**: An object which contains the following fields:

            * `data`: A list of objects representing the search results, each of which contains the following fields:
              * `videoId`: The unique identifier of the video that matched your search terms.
              * `start`: The start time of the matching video clip, expressed in seconds.
              * `end`: The end time of the matching video clip, expressed in seconds.
              * `score`: A quantitative value determined by the platform representing the level of confidence that the results match your search terms.
            * `pageInfo`: An object that provides information about pagination. The platform returns the results one page at a time, with a default limit of 10 results per page.
            * `pool`: An object that contains the total number of videos within the index, the total duration of the videos, and the unique identifier of the index that you’ve searched.

            For details about all fields, see the [API Reference > Make any-to-video search requests page](/v1.3/api-reference/any-to-video-search/make-search-request).
          

          
            **Function call**: You call the [`search.query`](/v1.3/sdk-reference/node-js/search#make-a-search-request) method.

            **Parameters**:

            * `index_id`: The unique identifier of the index.
            * `queryMediaType`: The type of query. It must be set to "image".
            * `queryMediaFile` or `queryMediaUrl`: The path or the publicly accesible URL of your image file.
            * `options`: An array of strings specifying the modalities the platform uses when performing a search. This example searches using visual cues.


            **Return value**: An object which contains the following fields:

            * `data`: A list of objects representing the search results, each of which contains the following fields:
              * `videoId`: The unique identifier of the video that matched your search terms.
              * `start`: The start time of the matching video clip, expressed in seconds.
              * `end`: The end time of the matching video clip, expressed in seconds.
              * `score`: A quantitative value determined by the platform representing the level of confidence that the results match your search terms.
            * `pageInfo`: An object that provides information about pagination. The platform returns the results one page at a time, with a default limit of 10 results per page.
            * `pool`: An object that contains the total number of videos within the index, the total duration of the videos, and the unique identifier of the index that you’ve searched.

            For details about all fields, see the [API Reference > Make any-to-video search requests page](/v1.3/api-reference/any-to-video-search/make-search-request).
          
        
      

      
        Display the search results, including handling pagination to retrieve all pages.

        **Function call**: Iterate over the results by calling the `next` function.

        **Parameters**: The function doesn't take any parameters.


        **Return value**: The next page or `null` if the iterator has reached the last page.
      
    
  



# Search with text and image queries

This guide provides an overview of using text and image queries to search for specific content within videos. You'll learn about the capabilities of each type of query, along with best practices for achieving accurate search results.

# Text queries

Text queries allow you to search for video segments using natural language descriptions. The platform interprets your query to find matching content based on visual elements, actions, sounds, and on-screen text.

Note the following about using text queries:

* The platform supports full natural language-based search. The following examples are valid queries: "birds flying near a castle," "sun shining on the water," "chickens on the road," "an officer holding a child's hand," and "crowd cheering in the stadium."
* To search for specific text shown in videos, use queries that target on-screen text rather than objects or concepts. Note that the platform may return both textual and visual matches. For example,  searching for the word "smartphone" might return both segments where "smartphone" appears as on-screen text and segments where smartphones are visible as objects.
* To detect logos, specify the text within the logo. If the logo doesn't contain text, you can search using image queries.

# Image Queries

Image queries enable you to search for video segments using images. The platform performs semantic searches to find content contextually similar to your query image.

Note the following about using images as queries:

* The platform supports only semantic searches. When performing a semantic search, the platform determines the meaning of the image you provide and finds the video segments containing contextually similar elements. For example, if you use an image of a tree as the query, the search results might contain videos featuring different trees, focusing on the overall characteristics rather than specific details.
  ![](file:ba820582-9e96-4c96-951f-bee18bbcb926)

* The objects you want to search for must be sufficiently large and detailed. For example, using an image of a car in a parking lot is more likely to yield precise results than an image of a small branded pen on a table in a large room.
  ![](file:a0b91371-46e0-413c-9628-817a13f1b78b)

* The platform does not support searching for specific words or phrases spoken or displayed as text within videos. For example, if you provide an image of a cat as your query and want to find all the video segments where the word "cat"  is mentioned or appears on the screen, image queries cannot retrieve those results. Use text queries instead.
  ![](file:122b947e-9bf1-4ce1-9d66-03c66d3b0c45)

# Choose between text and image queries

Use text queries when:

* Searching for spoken words or phrases.
* Finding on-screen text, such as signs or captions.
* Describing scenes, actions, or concepts in natural language.
* Detecting logos with text.

Use image queries when:

* Finding visual content similar to an image you provide.
* Searching for objects, scenes, or visual patterns.
* Detecting logos without text.


# Query engineering

Query engineering is the process of crafting and refining search queries to enhance the relevancy and accuracy of your search results. The key to successful query engineering is understanding the platform's capabilities and then formulating your queries to maximize these capabilities effectively.

# Steps in query engineering

While there isn't a singular approach to querying our multimodal foundation model, the steps presented in this section will enhance your querying engineering proficiency and help you develop an understanding through experimentation.

1. **Clearly define the objective**: Determine what you want to retrieve. It can be a specific action, event, object, or something more complex, like a series of activities or people with various attributes.
2. **Create the initial query**: Craft your initial query, specifying the sources of information you want to search within.
3. **Refine and improve**: If the platform isn't providing the expected search results, make your prompt more explicit. For example, instead of using broad terms like `man` or `guitar`, use more descriptive phrases like `man playing guitar in the rain`.  Experiment with different adjectives, sequences, or sources of information. This helps in narrowing down the search.

# Tips for writing better queries

Effective query engineering can significantly enhance the quality of results returned by the platform. The tips in this section will help you write better queries.

## Be as specific and descriptive as possible

When crafting a query, specificity is often paramount. While single keywords might yield many results, they often don't capture the context. Thus, using descriptive phrases significantly enhances the relevance of your search results.

Examples:

* Instead of `dancer`, use `ballet dancer performing on a stage with dim lighting`. The additional context gives the platform insight into the type of dance and ambiance.
* Instead of `dog`, use `dog playing fetch in a park`. The revised query clearly specifies an activity and setting.

## Use synonyms

Every word comes with a set of synonyms that can offer slight variances in meaning. If the platform doesn't return the expected results when using a specific word, try its synonyms.

Examples:

* Instead of `automobile`, consider `car` or `vehicle` to narrow down your search results.
* Instead of `house`,  experiment with `bungalow` or `villa` to target a particular style.

## Consider subjectivity

Videos often contain content that can be open to interpretation, given the inherently subjective nature of certain concepts. When searching for such information, it's important to understand that your query may not capture the full breadth of relevant content. Thus, iterative querying (adjusting, refining, and trying different phrasings or perspectives) becomes essential.

Examples:

* Using `beautiful scenery` as your search query might return both a serene mountain or a beach sunset. To refine this query, specify either `snow-capped mountain scenery` or `tropical beach at sunset`.
* Using `dogs doing something funny` as your search query might return both a dog chasing its tail or a puppy getting startled by a harmless object. Similar to the above, refine this query by specifying what you find humorous.

## Use natural language

Instead of relying on traditional keyword-based search terms, help the platform deliver more accurate and contextually relevant results by expressing your queries in a more conversational or natural manner.

Examples:

* Instead of `guitar, solo, concert`,  use `footage where a guitarist plays a solo during a concert`.
* Instead of `cat, dog, chase`, use `videos where a cat is chasing a dog`.

## Rephrase domain-specific terms

Occasionally, the platform might not accurately recognize domain-specific terms. A more descriptive query that uses everyday language can enhance the accuracy of your search results in such instances.

Examples:

* In a football context, instead of `hurdle`, use `a football player jumping over another player who has their feet on the ground`.
* In mountaineering, instead of `alpenglow`, use `a reddish glow near the horizon`.

See the [Find football hurdles](#find-football-hurdles) section below for a practical example of how rephrasing domain-specific terms can enhance the precision of your search results.

# Practical examples of query engineering

The examples in this section are from the Playground. However, the principles demonstrated are similar when invoking the API programmatically.

## Use descriptive phrases

This example demonstrates how using descriptive phrases significantly enhances the accuracy of your search results.

**Objective**: Find a video showing the perfect adrenaline rush activity for your next vacation.

**Initial query**: Use `adrenaline rush` as your initial query. As shown in the screenshot below, the search results might be too broad:

![](file:f7e1b40e-1172-43f7-a561-fd7004915893)

**Step-by-step refinement:**

1. Refine your query by specifying the location - `adrenaline rush activities on the beach`.

   ![](file:3e1cebbb-9d81-4721-9f84-1fd87cdbdb5c)
2. In the screenshot above, the first search result is from a video showing people skydiving from a helicopter, and you want to find more videos similar to this. To do so, you can refine your query even further by specifying both the activity and the location -`People skydiving from a helicopter and landing on the beach`. Now, the search results are narrowed down:\
   ![](file:91d5e367-21c2-41ba-92fe-ddbdfe80f1a4)

## Use everyday language

This example demonstrates how using everyday language instead of jargon can improve the precision of the search results.

**Objective**: Find videos showing football hurdles.

**Initial query**: Use `football hurdles` as your initial query. As shown in the screenshot below, the search results are too broad, and only a few of them show the exact moments when players hurdle over their opponents:

![](file:b159c215-578c-48c2-b273-d06cd3020de5)

**Step-by-step refinement:**

1. Use everyday language to refine your query  - `A football player jumping over another player who has their feet on the ground`. Now, the search results are narrowed down:\
   ![](file:c1b42b61-1902-4d81-b530-a5fa02e10c55)

## Search for specific shot types

This example shows how to search for specific shot types and camera movements.
**Objective**: Find cinematic aerial shots of cities transitioning from day to night.\
**Initial query:** Use "city time-lapse" as your initial query. The results might include various types of city footage:

![](file:4412749b-7621-4147-a848-aed83562d1c6)

**Step-by-step refinement:**

1. Specify the type of shot - "aerial shot of a city skyline." This narrows down the results to drone footage:
   ![](file:b44e864c-7598-40f2-acad-49cbdcc71a08)

2. Combine the shot type with a technique - "aerial shot combined with time-lapse showing city lights turning on at dusk." The results now show the specific type of transition you're looking for:
   ![](file:00c366b7-4671-4f27-87bc-8adf6e21fdd4)


# Pagination

The SDKs provide two distinct methods, each suited to different use cases and requirements. Selecting the most efficient method is important for optimizing the performance of your application:

* **Iterative pagination**: Choose this method mainly when your application must retrieve a large number of items.
* **Direct pagination**: Choose this method mainly when the total number of items is manageable, or you must fetch a single page of results.

# Iterative pagination


  
    To retrieve the first page of results, invoke the [`search.query`](/v1.3/sdk-reference/python/search) method:

    ```Python Python maxLines=8
    search_results = client.search.query(
        index_id="", 
        options=["visual"],
        page_limit=5
    )
    ```

    When the response is grouped by video, you can use the `page_limit` parameter to specify the number of videos the platform will return on each page. The following example code sets the page limit to three and specifies that the result must be grouped by video:

    ```Python Python maxLines=8
    search_results = client.search.query(
        index_id="", 
        query_text="", 
        options=["visual"],
        page_limit=3,
        group_by="video"
    )
    ```
  

  
    To retrieve the first page of results, invoke the [`search.query`](/v1.3/sdk-reference/node-js/search#make-a-search-request) method:

    ```JavaScript Node.js maxLines=8
    let searchResults = await client.search.query({
        indexId: "",
        queryText: "",
        options: ["visual"],
    });
    ```

    To retrieve subsequent pages of results, use the async iterator protocol:

    ```JavaScript Node.js maxLines=8
    let searchResults = await client.search.query({
        indexId: "",
        queryText: "",
        options: ["visual"],
    });
    printPage(searchResults.data);
    while (true) {
        const page = await searchResults.next();
        if (page === null) break;
        else printPage(page);
    }
    // Utility function to print a specific page
    function printPage(searchData) {
    (searchData as SearchData[]).forEach((clip) => {
        console.log(
        `video_id= ${clip.videoId} score=${clip.score} start=${clip.start} end=${clip.end} confidence=${clip.confidence}`,
        );
    });
    }
    ```

    Use the `pageLimit` parameter to specify a limit of items on each page. The example below sets the limit to 5:

    ```JavaScript Node.js maxLines=8
    let searchResults = await client.search.query({
        indexId: "",
        queryText: "",
        options: ["visual"],
        pageLimit: 5,
    });
    ```

    When the response is grouped by video, you can use the `pageLimit` parameter to specify the number of videos the platform will return on each page. The following example code sets the page limit to three and specifies that the result must be grouped by video:

    ```JavaScript Node.js
    let searchResults = await client.search.query({
        indexId: "",
        queryText: "",
        options: ["visual"],
        pageLimit: 3,
        groupBy: "video",
    });
    ```
  


# Direct pagination


  
    To retrieve the first page of results, invoke the [`search.query`](/v1.3/sdk-reference/python/search#make-a-search-request) method:

    ```Python Python maxLines=8
    search_results = client.search.query(
        index_id="", 
        query_text="", 
        options=[""]
    )
    ```

    The response will contain an object named `page_info` containing the following properties:

    * `limit_per_page`: The maximum number of results per page.
    * `total_results`: The total number of results.
    * `page_expires_at`: The time when the current page expires.
    * `next_page_token`: A token you can use to retrieve the results on the next page. The platform doesn't return this field when you've reached the end of the dataset
    * `prev_page_token`:  A token you can use to retrieve the results on the previous page. The platform doesn't return this field when you've retrieved the first page.


    To retrieve a specific page of results, invoke the [`search.by_page_token`](/v1.3/sdk-reference/python/search#retrieve-a-specific-page-of-search-results) method, passing the `next_page_token` property of the `page_info` object as the value of the `page_token` parameter. The response will be similar to the one returned when retrieving the first page of results. When the platform does not return the `next_page_token` field, you've reached the end of the dataset.

    ```Python Python
    search_results = client.search.by_page_token(page_token="")
    ```

    Use the `page_limit` parameter to specify a limit of items on each page. The example below sets the limit to 5:

    ```Python Python maxLines=8
    search_results = client.search.query(
        index_id=""],
        page_limit=5
    )
    ```

    
      When the response is grouped by video, you can use the `page_limit` parameter to specify the number of videos the platform will return on each page.
    
  

  
    To retrieve the first page of results, invoke the [`search.query`](/v1.3/sdk-reference/node-js/search#make-a-search-request) method:

    ```JavaScript Node.js maxLines=8
    let searchResults = await client.search.query({
        indexId: "",
        queryText: "",
        options: [""],
    });
    ```

    The response will contain an object named `pageInfo` containing the following properties:

    * `limitPerPage`: The maximum number of results per page.
    * `totalResults`: The total number of results.
    * `pageExpiresAt`: The time when the current page expires.
    * `nextPageToken`: A token you can use to retrieve the results on the next page. The platform doesn't return this field when you've reached the end of the dataset
    * `prevPageToken`:  A token you can use to retrieve the results on the previous page. The platform doesn't return this field when you've retrieved the first page.


    To retrieve a specific page of results, invoke the [`byPageToken`](/v1.3/sdk-reference/node-js/search#retrieve-a-specific-page-of-search-results) method of the `search` object, passing the `nextPageToken` property of the `pageInfo` object as the value of the `pagePoken` parameter. The response will be similar to the one returned when retrieving the first page of results. When the platform does not return the `nextPageToken` field, you've reached the end of the dataset.

    ```JavaScript Node.js
    let searchResults = await client.search.byPageToken('')
    ```

    Use the `pageLimit` parameter to specify a limit of items on each page. The example below sets the limit to 5:

    ```JavaScript Node.js maxLines=8
    let searchResults = await client.search.query({
        indexId: "",
        queryText: "",
        options: [""],
        pageLimit: 5
    });
    ```

    
      When the response is grouped by video, you can use the `page_limit` parameter to specify the number of videos the platform will return on each page.
    
  



# Sorting


  
    You can sort search results using the `sort_option` and `group_by` parameters. The platform returns a `confidence` field for each matching video clip, indicating how well the clip matches your query.

    When you omit the `group_by` parameter, the platform returns a flat list of clips sorted by the `confidence` field in descending order.

    When you set `group_by` to "video", the platform groups results by video and sorts the clips within each video by the confidence field in descending order. You can further sort the list of videos using the `sort_option` parameter:

    * `clip_count`: Sorts videos by the number of matching clips in descending order.
    * `score`: Sorts videos by the highest `confidence` value among their clips in descending order.

    To group the response by video and sort the list of videos on the number of matches:

    ```Python Python maxLines=8
    search_results = client.search.query(
        index_id=", 
        query_text="", 
        options=["visual"],
        group_by="video",
        sort_option="clip_count"
    )
    ```

    To group the response by video and sort the list of videos on the maximum value of the `confidence` field:

    ```Python Python maxLines=8
    search_results = client.search.query(
        index_id="", 
        query_text="

  
    You can sort search results using the `sortOption` and `groupBy` parameters. The platform returns a `confidence` field for each matching video clip, indicating how well the clip matches your query.

    When you omit the `groupBy` parameter, the platform returns a flat list of clips sorted by the `confidence` field in descending order.

    When you set `group_by` to "video", the platform groups results by video and sorts the clips within each video by the confidence field in descending order. You can further sort the list of videos using the `sortOption` parameter:

    * `clipCount`: Sorts videos by the number of matching clips in descending order.
    * `score`: Sorts videos by the highest `confidence` value among their clips in descending order.

    To group the response by video and sort the list of videos on the number of matches:

    ```JavaScript Node.js maxLines=8
    let searchResults = await client.search.query({
        indexId: "",
        queryText: "",
        options: ["visual"],
        groupBy: "video",
        sortOption: "clip_count",
    });

    ```

    To group the response by video and sort the list of videos on the maximum value of the `confidence` field:

    ```JavaScript Node.js maxLines=8
    let searchResults = await client.search.query({
        indexId: "",
        queryText: "",
        options: ["visual"],
        groupBy: "video",
        sortOption: "score",
    });
    ```
  



# Grouping

Grouping and ungrouping allow you to organize search results in different ways. The platform organizes matching clips under their respective videos when you group them by video. This helps when building a user interface, because it allows your users to better understand and navigate the search results. When you ungroup, the platform presents all matching clips in a flat list, which is useful for a simple view of all results.


  
    When you set `group_by` to "video", the response contains an array of objects, each object corresponding to a video that matches your query and includes the following fields:

    * `id`: The unique identifier of the video.
    * `clips`: An array that groups the information about all the matching video clips in that video.

    ```Python Python maxLines=8
    search_results = client.search.query(
    index_id=", 
    query_text=", 
    options=["visual"],
    group_by="video"
    )

    def print_page(page):
        for video in page:
            print(f"Video id: {video.id}")
            for clip in video.clips:
                print(clip)
                print(
                    f"\tscore={clip.score} start={clip.start} end={clip.end} confidence={clip.confidence}"
                )

    print_page(search_results.data)

    while True:
        try:
            print_page(next(search_results))
        except StopIteration:
            break
    ```
  

  
    When you set `groupBy` to "video", the response contains an array of objects, each object corresponding to a video that matches your query and includes the following fields:

    * `id`: The unique identifier of the video.
    * `clips`: An array that groups the information about all the matching video clips in that video.

    ```JavaScript Node.js maxLines=8
    let searchResults = await client.search.query({
      indexId: ""
      queryText: "",
      options: ["visual"],
      groupBy: "video",
    });
    printPage(searchResults);
    while (true) {
      const page = await searchResults.next();
      if (page === null) break;
      else printPage(page);
    }
    // Utility function to print a specific page
    function printPage(searchData) {
    (searchData.data as GroupByVideoSearchData[]).forEach((video) => {
        console.log(`videoId=${video.id}`);
        video.clips?.forEach((clip) => {
        console.log(
            `     score=${clip.score} start=${clip.start} end=${clip.end} confidence=${clip.confidence}`,
        );
        });
    });
    }
    ```
  



# Filtering

When you perform a search, the platform returns all the relevant matches. Filtering narrows the scope of your query. The platform allows you to filter your search results based on metadata or the level of confidence that the search results match your query.

# Filtering search results based on metadata

To filter your search results based on metadata, use the `filter` parameter.

The `filter` parameter is of type `Object` and can contain both system-generated and user-provided metadata fields.fields. For details on system-generated metadata, see the [Video object](/v1.3/api-reference/videos/the-video-object) page. For details on providing custom metadata, see the [Update video information](/v1.3/api-reference/videos/update) page.

To indicate the relationship between a field and its value, you can use the exact match or comparison operators.

## Exact match operator

The exact match operator matches only the results that equal the value you specify. The syntax is as follows: `: `.

## Comparison operators

Use the comparison operators (`lte` and `gte`) to match based on the arithmetic comparison. The syntax is as follows: `:{"gte": , "lte":  Make any-to-video search request](/v1.3/api-reference/any-to-video-search/make-search-request) page.

# Examples

## Filter on a specific video ID

The following example code uses the `id` field of the `filter` parameter to filter on a specific video ID:


  ```python Python maxLines=8
  from twelvelabs import TwelveLabs

  client = TwelveLabs(api_key="")

  search_results = client.search.query(
    index_id="",
    query_text= "",
    options=["visual"],
    filter={"id":[""]}
  )
  ```

  ```javascript Node.js maxLines=8
  let searchResults = await client.search.query({
    indexId: "",
    queryText: "",
    options: ["visual"],
    filter: { id: [""] },
  });
  ```


## Filter on multiple video IDs

The following example code uses the `id` field of the `filter` query parameter to filter on multiple video IDs:


  ```python Python maxLines=8
  search_results = client.search.query(
    index_id="",
    query_text= "",
    options=["visual"],
    filter={"id":["", ""]}
  )
  ```

  ```javascript Node.js maxLines=8
  import { TwelveLabs, SearchData } from 'twelvelabs-js';

  const client = new TwelveLabs({ apiKey: ''});

  let searchResults = await client.search.query({
    indexId: "",
    queryText: "",
    options: ["visual"],
    filter: { id: ["", ""] },
  });
  ```


## Filter on size, width, and height

The example code below uses the `size`, `width`, and `height` fields of the `filter`  parameter to return only the matches found in videos that meet all the following criteria:

* Size is greater than or equal to `50000000` bytes and less and equal to `53000000` bytes.
* Width is greater than or equal to `850`.
* Height is greater than or equal to `400` and less and equal to `500.`


  ```python Python maxLines=8
  search_results = client.search.query(
    index_id="",
    query_text= "",
    options=["visual"],
    filter={
      "size": {
        "gte": 50000000, "lte": 53000000
      },
      "width":
        {
          "gte": 850
        },
      "height":
        {
          "gte": 400, "lte": 500
        }
    }
  )
  ```

  ```javascript Node.js maxLines=8
  let searchResults = await client.search.query({
    indexId: "",
    queryText: "",
    options: ["conversation"],
    filter: {
      size: {
        gte: 50000000,
        lte: 53000000,
      },
      width: {
        gte: 850,
      },
      height: {
        gte: 400,
        lte: 500,
      },
    },
  });
  ```


## Filter on custom metadata

The example code below filters on a custom field named `views` of type `integer`. The platform will return only the results found in the videos for which the value of the `views` field equals `120000`. For details about specifying custom metadata, see the [Provide custom metadata](/v1.3/docs/advanced/metadata#provide-custom-metadata) section.


  ```python Python maxLines=8
  search_results = client.search.query(
    index_id="",
    query_text= "",
    options=["conversation"],
    filter = {
      "views": 120000
    }
  )
  ```

  ```javascript Node.js maxLines=8
  let searchResults = await client.search.query({
    indexId: "",
    queryText: "",
    options: ["conversation"],
    filter: {
      views: 120000,
    },
  });
  ```


## Filtering on the level of confidence

The following example code specifies that the minimum level of confidence shouldn't be lower than `medium`:


  ```python Python maxLines=8
  search_results = client.search.query(
    index_id="",
    query_text= "",
    options=["visual"],
    threshold='medium'
  )
  ```

  ```javascript Node.js maxLines=8
  let searchResults = await client.search.query({
    indexId: ',
    queryText: '',
    options: ['visual'],
    threshold: 'medium',
  });
  ```



# Analyze videos

The Analyze API suite uses a multimodal approach to analyze videos and generate text, processing visuals, sounds, spoken words, and texts to provide a comprehensive understanding. This method captures nuances that unimodal interpretations might miss, allowing for accurate and context-rich text generation based on video content.


  This API was formerly known as the Generate API. The name has been updated to Analyze API to more accurately reflect its purpose of analyzing videos to generate text. You may continue using the [`/generate`](/v1.3/api-reference/analyze-videos/open-ended) endpoint until July 30, 2025. After this date, you must use the [`/analyze`](/v1.3/api-reference/analyze-videos/analyze) endpoint.


Key features:

* **Multimodal analysis**: Processes visuals, sounds, spoken words, and texts for a holistic understanding of video content.
* **Customizable prompts**: Allows tailored outputs through instructive, descriptive, or question-based prompts.
* **Flexible text generation**: Supports various tasks, including summarization, chaptering, and open-ended text generation.

Use cases:

* **Content structuring**: Organize and structure content for e-learning platforms to improve usability.
* **SEO optimization**: Optimize content to rank higher in search engine results.
* **Highlight creation**: Create short, engaging video clips for media and broadcasting.
* **Incident reporting**: Record and report incidents for security and law enforcement purposes.

To understand how your usage is measured and billed, see the [Pricing](https://www.twelvelabs.io/pricing) page.

Depending on your use case, follow the steps in one of the guides below:


  * **Description**: Generates quick text summaries of video content using predefined formats.

  * **Customization**: Uses fixed formats with no prompts.

  * **Best use:** Ideal for fast, simple text outputs without customization.




  * **Description**: Creates summarized text, chapters, or highlights based on predefined formats, with optional custom prompts.

  * **Customization**: Supports predefined formats and optional prompts for tailored outputs.

  * **Best use:** Balances efficiency of predefined formats with some customization.




  * **Description**: Analyzes videos and produces fully customizable text based on your prompts.

  * **Customization**: Requires clear user prompts for maximum flexibility.

  * **Best use**: Suits advanced users needing specific outputs beyond predefined formats.




  * Your prompts can be instructive or descriptive, or you can also phrase them as questions.
  * The platform generates text according to the [model options](/v1.3/docs/concepts/modalities#model-options) enabled for your index, which determine the types of information the video understanding model processes.

    **Example**:

    * If both the `visual` and `audio` model options are enabled, the platform generates text based on both visual and audio information.
    * If only the `visual` option is enabled, the platform generates text based only on visual information.
    * The maximum length of a prompt is 2,000 tokens.



# Titles, topics, and hashtags

This guide shows how you can use the Analyze API to generate the following types of text:

* **Titles** are brief phrases that capture the main idea of a video, making it easy to understand and categorize.
* **Topics** identify the central themes of the video for effective categorization.
* **Hashtags** are keywords that summarize a video, enhancing its searchability on social media.

You can generate one or more types of text in a single API call.

{/* This guide provides a complete example. For a simplified introduction with just the essentials, see the [Analyze videos](/v1.3/docs/get-started/quickstart/analyze-videos) quickstart guide. */}

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The videos you wish to use must meet the following requirements:
  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.

  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.

  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  * **Duration**: Must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration will be 2 hours (7,200 seconds).

  * **File size**:  Must not exceed 2 GB.\
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

# Complete example

This complete example shows how to create an index, upload a video, and generate a title, topics and hashtags. Ensure you replace the placeholders surrounded by `<>` with your values.


  ```python Python maxLines=8
  from twelvelabs import TwelveLabs
  from twelvelabs.models.task import Task

  # 1. Initialize the client
  client = TwelveLabs(api_key="")

  # 2. Create an index
  models = [
  {
          "name": "pegasus1.2",
          "options": ["visual", "audio"]
      }
  ]
  index = client.index.create(name="", models=models)
  print(f"Index created: id={index.id}, name={index.name}")

  # 3. Upload a video
  task = client.task.create(index_id=index.id, url="")
  print(f"Task id={task.id}, Video id={task.video_id}")

  # 4. Monitor the indexing process
  def on_task_update(task: Task):
      print(f"  Status={task.status}")
  task.wait_for_done(sleep_interval=5, callback=on_task_update)
  if task.status != "ready":
      raise RuntimeError(f"Indexing failed with status {task.status}")
  print(f"The unique identifier of your video is {task.video_id}.")

  # 5. Generate title, topics, and hashtags
  gist = client.gist(video_id=task.video_id, types=["title", "topic", "hashtag"])

  # 6. Process the results
  print(f"Title={gist.title}\nTopics={gist.topics}\nHashtags={gist.hashtags}")
  ```

  ```JavaScript Node.js maxLines=8
  import { TwelveLabs } from "twelvelabs-js";

  // 1. Initialize the client
  const client = new TwelveLabs({ apiKey: "" });

  // 2. Create an index
  const models = [
    {
      name: "pegasus1.2",
      options: ["visual", "audio"],
    },
  ];
  const index = await client.index.create({
    name: "",
    models: models,
  });
  console.log(
    `A new index has been created: id=${index.id} name=${
      index.name
    } models=${JSON.stringify(index.models)}`
  );

  // 3. Upload a video
  const task = await client.task.create({
    indexId: index.id,
    url: "",
  });
  console.log(`Task id=${task.id} Video id=${task.videoId}`);

  // 4. Monitor the indexing process
  await task.waitForDone(5000, (task) => {
    console.log(`  Status=${task.status}`);
  });
  if (task.status !== "ready") {
    throw new Error(`Indexing failed with status ${task.status}`);
  }
  console.log(`The unique identifier of your video is ${task.videoId}`);

  // 5. Generate title, topics, and hashtags
  const gist = await client.gist(task.videoId, [
    "title",
    "topic",
    "hashtag",
  ]);

  // 6. Process the results
  console.log(
    `Title: ${gist.title}\nTopics=${gist.topics}\nHashtags=${gist.hashtags}`
  );
  ```


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/python/the-twelve-labs-class#the-initializer) of the `TwelveLabs` class.

        **Parameters**:

        * `api_key`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        Indexes help you organize and search through related videos efficiently. This example creates a new index, but you can also use an existing index by specifying its unique identifier. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.

        **Function call**: You call the [`index.create`](/v1.3/sdk-reference/python/manage-indexes#create-an-index) function.

        **Parameters**:

        * `name`: The name of the index.
        * `models`: An object specifying your model configuration. This example enables the Pegasus video understanding model and the `visual` and `audio` model options.


        **Return value**: An object containing, among other information, a field named `id` representing the unique identifier of the newly created index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.

        **Function call**: You call the [`task.create`](/v1.3/sdk-reference/python/upload-videos#create-a-video-indexing-task) function.

        **Parameters**:

        * `index_id`: The unique identifier of your index.
        * `url` or `file`: The publicly accessible URL or the path of your video file.


        **Return value**: An object that contains the unique identifier of your video and the status of the associated video indexing task. You can use this object to track the status of your video upload and indexing process.

        
          You can also upload multiple videos in a single API call. For details, see the [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations) page.
        
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.

        **Function call**: You call the [`task.wait_for_done`](/v1.3/sdk-reference/python/upload-videos#wait-for-a-video-indexing-task-to-complete) function.

        **Parameters**:

        * `sleep_interval`: The time interval, in seconds, between successive status checks. In this example, the method checks the status every five seconds.
        * `callback`:  A callback function that the SDK executes each time it checks the status.


        **Return value**: An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        **Function call**: You call the [`gist`](/v1.3/sdk-reference/python/analyze-videos#titles-topics-and-hashtags) method.

        **Parameters**:

        * `video_id`: The unique identifier of the video for which you want to generate text.
        * `types`: An array of strings representing the types of text you want to generate. It can take one or more of the following values: `["title", "topic", "hashtag"]`.

          **Return value**: An object containing, among other information, one or more of the following fields of type string: `title`, `topics`, `hashtags`.
      

      
        This example prints the generated text to the standard output.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/node-js/the-twelve-labs-class#the-constructor) of the `TwelveLabs` class.

        **Parameters**:

        * `apiKey`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        Indexes help you organize and search through related videos efficiently. This example creates a new index, but you can also use an existing index by specifying its unique identifier. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.

        **Function call**: You call the [`index.create`](/v1.3/sdk-reference/node-js/manage-indexes#create-an-index) function.

        **Parameters**:

        * `name`: The name of the index.
        * `models`: An object specifying your model configuration. This example enables the Pegasus video understanding model and the `visual` and `audio` model options.


        **Return value**: An object containing, among other information, a field named `id` representing the unique identifier of the newly created index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.

        **Function call**: You call the [`task.create`](/v1.3/sdk-reference/node-js/upload-videos#create-a-video-indexing-task) function.

        **Parameters**:

        * `indexId`: The unique identifier of your index.
        * `url` or `file`: The publicly accessible URL or the path of your video file.


        **Return value**: An object that contains the unique identifier of your video and the status of the associated video indexing task. You can use this object to track the status of your video upload and indexing process.

        
          You can also upload multiple videos in a single API call. For details, see the [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations) page.
        
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.

        **Function call**: You call the [`task.waitForDone`](/v1.3/sdk-reference/node-js/upload-videos#wait-for-a-video-indexing-task-to-complete) function.

        **Parameters**:

        * `sleepInterval`: The time interval, in milliseconds, between successive status checks. In this example, the method checks the status every five seconds.
        * `callback`:  A callback function that the SDK executes each time it checks the status.


        **Return value**: An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        **Function call**: You call the [`gist`](/v1.3/sdk-reference/node-js/analyze-videos#titles-topics-and-hashtags) method.

        **Parameters**:


        * `videoId`: The unique identifier of the video for which you want to generate text.
        * `types`: An array of strings representing the types of text you want to generate. This example uses `["title", "topic", "hashtag"]`.

          **Return value**: An object containing, among other information, one or more of the following fields of type string: `title`, `topics`, `hashtags`.
      

      
        This example prints the generated text to the standard output.
      
    
  



# Summaries, chapters, and highlights

This guide shows how you can use the Analyze API to generate summaries, chapters, and highlights from videos using pre-defined formats and optional prompts for customization.

* **Summaries** are concise overviews capturing the key points, adaptable into formats like paragraphs, emails, or bullet points based on your prompt.
* **Chapters** offer a chronological breakdown of the video, with timestamps, headlines, and summaries for each section.
* **Highlights** list the most significant events chronologically, including timestamps and brief descriptions.

Below are some examples of how to guide the platform in generating content tailored to your needs.

| Content type                | Prompt example                                                                                         |
| :-------------------------- | :----------------------------------------------------------------------------------------------------- |
| Specify the target audience | Generate a summary suitable for a high school audience studying environmental science.                 |
| Adjust the tone             | Generate a light-hearted and humorous chapter breakdown of this documentary.                           |
| Indicate length constraints | Provide a summary fit for a Twitter post under 280 characters.                                         |
| Customize text format       | Generate a summary in no more than 5 bullet points.                                                    |
| Specify the purpose         | Summarize this video from a marketer's perspective, focusing on brand mentions and product placements. |

{/* This guide provides a complete example. For a simplified introduction with just the essentials, see the [Analyze videos](/v1.3/docs/get-started/quickstart/analyze-videos) quickstart guide. */}

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The videos you wish to use must meet the following requirements:
  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.

  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.

  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  * **Duration**: Must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration will be 2 hours (7,200 seconds).

  * **File size**:  Must not exceed 2 GB.\
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

# Complete example

This complete example shows how to create an index, upload a video, and analyze videos to generate summaries, chapters, and highlights. Ensure you replace the placeholders surrounded by `<>` with your values.


  ```Python Python maxLines=8

  from twelvelabs import TwelveLabs
  from twelvelabs.models.task import Task

  # 1. Initialize the client
  client = TwelveLabs(api_key="")

  # 2. Create an index
  models = [
      {
          "name": "pegasus1.2",
          "options": ["visual", "audio"]
      }
  ]
  index = client.index.create(name="", models=models)
  print(f"Index created: id={index.id}, name={index.name}")

  # 3. Upload a video
  task = client.task.create(index_id=index.id, url="")
  print(f"Task id={task.id}, Video id={task.video_id}")

  # 4. Monitor the indexing process
  def on_task_update(task: Task):
      print(f"  Status={task.status}")
  task.wait_for_done(sleep_interval=5, callback=on_task_update)
  if task.status != "ready":
      raise RuntimeError(f"Indexing failed with status {task.status}")
  print(f"The unique identifier of your video is {task.video_id}.")

  # 5. Generate summaries, chapters, and highlights
  res_summary = client.summarize(
      video_id=task.video_id,
      type="summary",
      # prompt="",
      # temperature=0.2
  )
  res_chapters = client.summarize(
      video_id=task.video_id,
      type="chapter",
      # prompt="",
      # temperature=0.2
  )
  res_highlights = client.summarize(
      video_id=task.video_id,
      type="highlight",
      # prompt="",
      # temperature=0.2
  )

  #6. Process the results
  print(f"Summary= {res_summary.summary}")
  for chapter in res_chapters.chapters:
      print(
          f"""Chapter {chapter.chapter_number},
  start={chapter.start},
  end={chapter.end}
  Title: {chapter.chapter_title}
  Summary: {chapter.chapter_summary}
  """
      )
  for highlight in res_highlights.highlights:
      print(
          f"Highlight: {highlight.highlight}, start: {highlight.start}, end: {highlight.end}")
  ```

  ```JavaScript Node.js maxLines=8
  import { TwelveLabs } from "twelvelabs-js";

  // 1. Initialize the client
  const client = new TwelveLabs({ apiKey: "" });

  // 2. Create an index
  const models = [
      {
      name: "pegasus1.2",
      options: ["visual", "audio"],
      },
  ];
  const index = await client.index.create({
      name: "",
      models: models,
  });
  console.log(
      `A new index has been created: id=${index.id} name=${index.name} models=${JSON.stringify(index.models)}`,
  );

  // 3. Upload a video
  const task = await client.task.create({
      indexId: index.id,
      url: "",
  });
  console.log(`Task id=${task.id} Video id=${task.videoId}`);

  // 4. Monitor the indexing process
  await task.waitForDone(5000, (task) => {
      console.log(`  Status=${task.status}`);
  });

  if (task.status !== "ready") {
      throw new Error(`Indexing failed with status ${task.status}`);
  }
  console.log(`The unique identifier of your video is ${task.videoId}`);

  // 5. Generate summaries, chapters, and highlights
  const summary = await client.summarize(
    task.videoId,
    "summary",
    // "",
    // 0.2
  );
  const chapters = await client.summarize(
    task.videoId,
    "chapter",
    // "",
    // 0.2
  );
  const highlights = await client.summarize(
    task.videoId,
    "highlight",
    // "",
    // 0.2
  );

  // 6. Process the results
  console.log(`Summary: ${summary.summary}`);
  for (const chapter of chapters.chapters) {
      console.log(
      `Chapter ${chapter.chapterNumber}\nstart=${chapter.start}\nend=${chapter.end}\nTitle=${chapter.chapterTitle}\nSummary=${chapter.chapterSummary}`,
      );
  }
  for (const highlight of highlights.highlights) {
      console.log(
      `Highlight: ${highlight.highlight}, start: ${highlight.start}, end: ${highlight.end}`,
      );
  }
  ```


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/python/the-twelve-labs-class#the-initializer) of the `TwelveLabs` class.

        **Parameters**:

        * `api_key`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        Indexes help you organize and search through related videos efficiently. This example creates a new index, but you can also use an existing index by specifying its unique identifier. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.

        **Function call**: You call the [`index.create`](/v1.3/sdk-reference/python/manage-indexes#create-an-index) function.

        **Parameters**:

        * `name`: The name of the index.
        * `models`: An object specifying your model configuration. This example enables the Pegasus video understanding model and the `visual` and `audio` model options.


        **Return value**: An object containing, among other information, a field named `id` representing the unique identifier of the newly created index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.

        **Function call**: You call the [`task.create`](/v1.3/sdk-reference/python/upload-videos#create-a-video-indexing-task) function.

        **Parameters**:

        * `index_id`: The unique identifier of your index.
        * `url` or `file`: The publicly accessible URL or the path of your video file.


        **Return value**: An object that contains the unique identifier of your video and the status of the associated video indexing task. You can use this object to track the status of your video upload and indexing process.

        
          You can also upload multiple videos in a single API call. For details, see the [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations) page.
        
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.

        **Function call**: You call the [`task.wait_for_done`](/v1.3/sdk-reference/python/upload-videos#wait-for-a-video-indexing-task-to-complete) function.

        **Parameters**:

        * `sleep_interval`: The time interval, in seconds, between successive status checks. In this example, the method checks the status every five seconds.
        * `callback`:  A callback function that the SDK executes each time it checks the status.


        **Return value**: An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        **Function call**: You call the [`summarize`](/v1.3/sdk-reference/python/analyze-videos#summaries-chapters-and-highlights) method.

        **Parameters**:

        * `video_id`: The unique identifier of the video for which you want to generate text.
        * `type`: The type of text you want to generate. It can take one of the following values: "summary", "chapter", or "highlight".
        * *(Optional)* `prompt`: A string you can use to provide context for the summarization task. The maximum length of a prompt is 2,000 tokens. Example: "Generate chapters using casual and conversational language to match the vlogging style of the video."
        * *(Optional)*  `temperature`: A number that controls the randomness of the text. A higher value generates more creative text, while a lower value produces more deterministic text.


        **Return value**: A string for summaries and a list of objects for chapters and highlights.
      

      
        For summaries, you can directly print the result. You must iterate over the list and print each item individually for chapters, and highlights.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/node-js/the-twelve-labs-class#the-constructor) of the `TwelveLabs` class.

        **Parameters**:

        * `apiKey`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        Indexes help you organize and search through related videos efficiently. This example creates a new index, but you can also use an existing index by specifying its unique identifier. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.

        **Function call**: You call the [`index.create`](/v1.3/sdk-reference/node-js/manage-indexes#create-an-index) function.

        **Parameters**:

        * `name`: The name of the index.
        * `models`: An object specifying your model configuration. This example enables the Pegasus video understanding model and the `visual` and `audio` model options.


        **Return value**: An object containing, among other information, a field named `id` representing the unique identifier of the newly created index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.

        **Function call**: You call the [`task.create`](/v1.3/sdk-reference/node-js/upload-videos#create-a-video-indexing-task) function.

        **Parameters**:

        * `indexId`: The unique identifier of your index.
        * `url` or `file`: The publicly accessible URL or the path of your video file.


        **Return value**: An object that contains the unique identifier of your video and the status of the associated video indexing task. You can use this object to track the status of your video upload and indexing process.

        
          You can also upload multiple videos in a single API call. For details, see the [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations) page.
        
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.

        **Function call**: You call the [`task.waitForDone`](/v1.3/sdk-reference/node-js/upload-videos#wait-for-a-video-indexing-task-to-complete) function.

        **Parameters**:

        * `sleepInterval`: The time interval, in milliseconds, between successive status checks. In this example, the method checks the status every five seconds.
        * `callback`:  A callback function that the SDK executes each time it checks the status.


        **Return value**: An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        **Function call**: You call the [`summarize`](/v1.3/sdk-reference/node-js/analyze-videos#summaries-chapters-and-highlights) method.

        **Parameters**:

        * `video_id`: The unique identifier of the video for which you want to generate text.
        * `type`: Type of text you want to generate. It can take one of the following values: "summary", "chapter", or "highlight".
        * *(Optional)* `prompt`: A string you can use to provide context for the summarization task. The maximum length of a prompt is 2,000 tokens. Example: "Generate chapters using casual and conversational language to match the vlogging style of the video."
        * *(Optional)*  `temperature`: A number that controls the randomness of the text. A higher value generates more creative text, while a lower value produces more deterministic text.

          **Return value**: A string for summaries and a list of objects for chapters and highlights.
      

      
        For summaries, you can directly print the result. You must iterate over the list and print each item individually for chapters, and highlights.
      
    
  



# Open-ended analysis

This guide shows how you can use the Analyze API to perform open-ended analysis on video content, generating tailored text outputs based on your prompts. This feature provides more customization options than the summarization feature. It supports generating various content types based on your prompts, including, but not limited to, tables of content, action items, memos, reports, and comprehensive analyses.

The platform provides two distinct methods for retrieving the results of the open-ended analysis:


  
    Streaming responses deliver text fragments in real-time as they are generated, enabling immediate processing and feedback. This method is the default behavior of the platform and is ideal for applications requiring incremental updates.


    * **Response format**: A stream of JSON objects in NDJSON format, with three event types:
      * `stream_start`: Marks the beginning of the stream.
      * `text_generation`: Delivers a fragment of the generated text.
      * `stream_end`: Signals the end of the stream.

    * **Response handling**:
      * Iterate over the stream to process text fragments as they arrive.
      * Use `text_stream.aggregated_text` to access the complete text after streaming ends.

    * **Advantages**:
      * Real-time processing of partial results.
      * Reduced perceived latency.
    * **Use case**: Live transcription, real-time analysis, or applications needing instant updates.
  

  
    Non-streaming responses deliver the complete generated text in a single response, simplifying processing when the full result is needed.

    * **Response format**: A single string containing the full generated text.
    * **Response handling**:
      * Access the complete text directly from the response.
    * **Advantages**:
      * Simplicity in handling the full result.
      * Immediate access to the entire text.
    * **Use case**: Generating reports, summaries, or any scenario where the whole text is required at once.
  


This guide provides a complete example. For a simplified introduction with just the essentials, see the [Analyze videos](/v1.3/docs/get-started/quickstart/analyze-videos) quickstart guide.

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The videos you wish to use must meet the following requirements:
  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.

  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.

  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  * **Duration**: Must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration will be 2 hours (7,200 seconds).

  * **File size**:  Must not exceed 2 GB.\
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

# Complete example

This complete example shows how to create an index, upload a video, and perform open-ended analysis to generate text based on the content of your video. Ensure you replace the placeholders surrounded by `<>` with your values.


  
    
      ```Python Python maxLines=8
      from twelvelabs import TwelveLabs
      from twelvelabs.models.task import Task

      # 1. Initialize the client
      client = TwelveLabs(api_key="")

      # 2. Create an index
      models = [
          {
              "name": "pegasus1.2",
              "options": ["visual", "audio"]
          }
      ]
      index = client.index.create(name="", models=models)
      print(f"Index created: id={index.id}, name={index.name}")

      # 3. Upload a video
      task = client.task.create(index_id=index.id, url="")
      print(f"Task id={task.id}, Video id={task.video_id}")

      # 4. Monitor the indexing process
      def on_task_update(task: Task):
          print(f"  Status={task.status}")
      task.wait_for_done(sleep_interval=5, callback=on_task_update)

      if task.status != "ready":
          raise RuntimeError(f"Indexing failed with status {task.status}")
      print(f"The unique identifier of your video is {task.video_id}.")

      # 5. Perform open-ended analysis
      text_stream = client.analyze_stream(
          video_id=task.video_id,
          prompt="",
          # temperature=0.2
      )

      # 6. Process the results
      for text in text_stream:
          print(text)
      print(f"Aggregated text: {text_stream.aggregated_text}")
      ```

      ```JavaScript Node.js maxLines=8
      import { TwelveLabs } from "twelvelabs-js";

      // 1. Initialize the client
      const client = new TwelveLabs({ apiKey: "" });

      // 2. Create an index
      const models = [
        {
          name: "pegasus1.2",
          options: ["visual", "audio"],
        },
      ];
      const index = await client.index.create({
        name: "",
        models: models,
      });
      console.log(
        `A new index has been created: id=${index.id} name=${
          index.name
        } models=${JSON.stringify(index.models)}`
      );

      // 3. Upload a video
      const task = await client.task.create({
        indexId: index.id,
        url: "",
      });
      console.log(`Task id=${task.id} Video id=${task.videoId}`);

      // 4. Monitor the indexing process
      await task.waitForDone(5000, (task) => {
        console.log(`  Status=${task.status}`);
      });

      if (task.status !== "ready") {
        throw new Error(`Indexing failed with status ${task.status}`);
      }
      console.log(`The unique identifier of your video is ${task.videoId}`);

      // 5. Perform open-ended analysis
      const textStream = await client.analyzeStream({
        videoId: task.videoId,
        prompt: "",
        // temperature: 0.2
      });

      // 6. Process the results
      for await (const text of textStream) {
        console.log(text);
      }
      console.log(`Aggregated text: ${textStream.aggregatedText}`);

      ```
    
  

  
    
      ```Python Python maxLines=8
      from twelvelabs import TwelveLabs
      from twelvelabs.models.task import Task

      # 1. Initialize the client
      client = TwelveLabs(api_key="")

      # 2. Create an index
      models = [
          {
              "name": "pegasus1.2",
              "options": ["visual", "audio"]
          }
      ]
      index = client.index.create(name="", models=models)
      print(f"Index created: id={index.id}, name={index.name}")

      # 3. Upload a video
      task = client.task.create(index_id=index.id, url="")
      print(f"Task id={task.id}, Video id={task.video_id}")

      # 4. Monitor the indexing process
      def on_task_update(task: Task):
          print(f"  Status={task.status}")
      task.wait_for_done(sleep_interval=5, callback=on_task_update)
      if task.status != "ready":
          raise RuntimeError(f"Indexing failed with status {task.status}")
      print(f"The unique identifier of your video is {task.video_id}.")

      # 5. Perform open-ended analysis
      text = client.analyze(
        video_id=task.video_id,
        prompt="",
        # temperature=0.2
      )

      # 6. Process the results
      print(f"{text.data}")
      ```

      ```JavaScript Node.js maxLines=8
      import { TwelveLabs } from "twelvelabs-js";

      // 1. Initialize the client
      const client = new TwelveLabs({ apiKey: "" });

      // 2. Create an index
      const models = [
        {
          name: "pegasus1.2",
          options: ["visual", "audio"],
        },
      ];
      const index = await client.index.create({
        name: "",
        models: models,
      });
      console.log(
        `A new index has been created: id=${index.id} name=${
          index.name
        } models=${JSON.stringify(index.models)}`
      );

      // 3. Upload a video
      const task = await client.task.create({
        indexId: index.id,
        url: "",
      });
      console.log(`Task id=${task.id} Video id=${task.videoId}`);

      // 4. Monitor the indexing process
      await task.waitForDone(5000, (task) => {
        console.log(`  Status=${task.status}`);
      });

      if (task.status !== "ready") {
        throw new Error(`Indexing failed with status ${task.status}`);
      }
      console.log(`The unique identifier of your video is ${task.videoId}`);

      // 5. Perform open-ended analysis
      const text = await client.analyze(
        task.videoId,
        "",
        // 0.2
      );

      //6. Process the results
      console.log(`${text.data}`);
      ```
    
  


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/python/the-twelve-labs-class#the-initializer) of the `TwelveLabs` class.

        **Parameters**:

        * `api_key`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        Indexes help you organize and search through related videos efficiently. This example creates a new index, but you can also use an existing index by specifying its unique identifier. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.

        **Function call**: You call the [`index.create`](/v1.3/sdk-reference/python/manage-indexes#create-an-index) function.

        **Parameters**:

        * `name`: The name of the index.
        * `models`: An object specifying your model configuration. This example enables the Pegasus video understanding model and the `visual` and `audio` model options.


        **Return value**: An object containing, among other information, a field named `id` representing the unique identifier of the newly created index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.

        **Function call**: You call the [`task.create`](/v1.3/sdk-reference/python/upload-videos#create-a-video-indexing-task) function.

        **Parameters**:

        * `index_id`: The unique identifier of your index.
        * `url` or `file`: The publicly accessible URL or the path of your video file.


        **Return value**: An object that contains the unique identifier of your video and the status of the associated video indexing task. You can use this object to track the status of your video upload and indexing process.

        
          You can also upload multiple videos in a single API call. For details, see the [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations) page.
        
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.

        **Function call**: You call the [`task.wait_for_done`](/v1.3/sdk-reference/python/upload-videos#wait-for-a-video-indexing-task-to-complete) function.

        **Parameters**:

        * `sleep_interval`: The time interval, in seconds, between successive status checks. In this example, the method checks the status every five seconds.
        * `callback`:  A callback function that the SDK executes each time it checks the status.


        **Return value**: An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        
          
            **Function call**: You call the [`analyze_stream`](/v1.3/sdk-reference/python/analyze-videos#open-ended-analysis-with-streaming-responses) method.

            **Parameters**:

            * `video_id`: The unique identifier of the video for which you want to generate text.
            * `prompt`: A string that guides the model on the desired format or content. The maximum length of a prompt is 2,000 tokens.
            * *(Optional)* `temperature`: A number that controls the randomness of the text. A higher value generates more creative text, while a lower value produces more deterministic text.


            **Return value**: An object that handles streaming HTTP responses and provides an iterator interface allowing you to process text fragments as they arrive. It contains the following fields, among other information:

            * `texts`: A list accumulating individual text fragments received from the stream.
            * `aggregated_text`: A concatenated string of all text fragments received so far.

          

          
            **Function call**: You call the [`analyze`](/v1.3/sdk-reference/python/analyze-videos#open-ended-analysis) method.

            **Parameters**:

            * `video_id`: The unique identifier of the video for which you want to generate text.
            * `prompt`: A string that guides the model on the desired format or content. The maximum length of a prompt is 2,000 tokens.
            * *(Optional)* `temperature`: A number that controls the randomness of the text. A higher value generates more creative text, while a lower value produces more deterministic text.


            **Return value**: An object containing a field named `data` of type `string` representing the generated text.

          
        
      

      
        
          
            Use a loop to iterate over the stream. Inside the loop, handle each text fragment as it arrives. This example prints each fragment to the standard output. After the stream ends, use the `aggregated_text` field if you need the full generated text.
          

          
            This example prints the generated text to the standard output.
          
        
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/node-js/the-twelve-labs-class#the-constructor) of the `TwelveLabs` class.

        **Parameters**:

        * `apiKey`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        Indexes help you organize and search through related videos efficiently. This example creates a new index, but you can also use an existing index by specifying its unique identifier. See the [Indexes](/v1.3/docs/concepts/indexes) page for more details on creating an index.

        **Function call**: You call the [`index.create`](/v1.3/sdk-reference/node-js/manage-indexes#create-an-index) function.

        **Parameters**:

        * `name`: The name of the index.
        * `models`: An object specifying your model configuration. This example enables the Pegasus video understanding model and the `visual` and `audio` model options.


        **Return value**: An object containing, among other information, a field named `id` representing the unique identifier of the newly created index.
      

      
        To perform any downstream tasks, you must first upload your videos, and the platform must finish indexing them.

        **Function call**: You call the [`task.create`](/v1.3/sdk-reference/node-js/upload-videos#create-a-video-indexing-task) function.

        **Parameters**:

        * `indexId`: The unique identifier of your index.
        * `url` or `file`: The publicly accessible URL or the path of your video file.


        **Return value**: An object that contains the unique identifier of your video and the status of the associated video indexing task. You can use this object to track the status of your video upload and indexing process.

        
          You can also upload multiple videos in a single API call. For details, see the [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations) page.
        
      

      
        The platform requires some time to index videos. Check the status of the video indexing task until it's completed.

        **Function call**: You call the [`task.waitForDone`](/v1.3/sdk-reference/node-js/upload-videos#wait-for-a-video-indexing-task-to-complete) function.

        **Parameters**:

        * `sleepInterval`: The time interval, in milliseconds, between successive status checks. In this example, the method checks the status every five seconds.
        * `callback`:  A callback function that the SDK executes each time it checks the status.


        **Return value**: An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        
          
            **Function call**: You call the [`analyzeStream`](/v1.3/sdk-reference/node-js/analyze-videos#open-ended-analysis-with-streaming-responses) method.


            **Parameters**:

            * `videoId`: The unique identifier of the video for which you want to generate text.
            * `prompt`: A string that guides the model on the desired format or content. The maximum length of a prompt is 2,000 tokens.
            * *(Optional)* `temperature`: A number that controls the randomness of the text. A higher value generates more creative text, while a lower value produces more deterministic text.


            **Return value**: An object that handles streaming HTTP responses and implements the async iterator protocol, allowing you to process text fragments as they arrive. It contains the following fields, among other information:

            * `texts`: A list accumulating individual text fragments received from the stream.
            * `aggregatedText`: A concatenated string of all text fragments received so far.

          

          
            **Function call**: You call the [`analyze`](/v1.3/sdk-reference/node-js/analyze-videos#open-ended-analysis) method.


            **Parameters**:

            * `videoId`: The unique identifier of the video for which you want to generate text.
            * `prompt`: A string that guides the model on the desired format or content. The maximum length of a prompt is 2,000 tokens.
            * *(Optional)* `temperature`: A number that controls the randomness of the text. A higher value generates more creative text, while a lower value produces more deterministic text.


            **Return value**: An object containing a field named `data` of type `string` representing the generated text.
          
        
      

      
        
          
            Use a loop to iterate over the stream. Inside the loop, handle each text fragment as it arrives. This example prints each fragment to the standard output. After the stream ends, use the `aggregatedText` field if you need the full generated text.
          

          
            This example prints the generated text to the standard output.
          
        
      
    
  



# Prompt examples

Below are examples of prompts designed to generate specific content types. To view them in the Playground, ensure you're logged in, then open the URLs in the **Playground** column.

| Content type             | Prompt example                                                                                                                  | Playground                                                                                                                                                                                                                                                                                            |
| :----------------------- | :------------------------------------------------------------------------------------------------------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Table of contents        | Provide a table of contents detailing the main sections of this video.                                                          | [See in Playground](https://playground.twelvelabs.io/indexes/6785dfa949d9c923603e5267/generate?v=6785ee30027eec4fa50e80c2\&mode=custom\&temp=0.2\&prompt=Provide+a+table+of+contents+detailing+the+main+sections+of+this+video.)                                                                      |
| Memo                     | Generate a company-wide memo based on the announcements made in the video.                                                      | [See in Playground](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?v=6785eedf027eec4fa50e8103\&mode=custom\&temp=0.2\&prompt=Generate+a+company-wide+memo+based+on+the+announcements+made+in+the+video.)                                                                  |
| Police report            | Create a police report based on what happened in the video. Provide the exact time range where the suspect appears in the video | [See in Playground](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?v=6791d98ecf67133816b2e01e\&prompt=Create+a+police+report+based+on+what+happened+in+the+video.+Provide+the+exact+time+range+where+the+suspect+appears+in+the+video\&cache=true\&mode=custom\&temp=0.2) |
| Video annotations        | Identify and list key visual elements, scene changes, and notable events in the video, briefly describing each.                 | [See in Playground](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?v=6785eeeb027eec4fa50e810a\&mode=custom\&temp=0.2\&prompt=Identify+and+list+key+visual+elements%2C+scene+changes%2C+and+notable+events+in+the+video%2C+briefly+describing+each.)                       |
| Video question answering | What are the key takeaways of this video?                                                                                       | [See in Playground](https://playground.twelvelabs.io/indexes/6785dfa949d9c923603e5267/generate?v=6785ee313d8c70ded56640b4\&mode=custom\&temp=0.2\&prompt=What+are+the+key+takeaways+of+this+video%3F)                                                                                                 |
| Timestamp breakdown      | Tell me all the timestamps in the advertisement where a closeup of the product appears.                                         | [See in Playground](https://playground.twelvelabs.io/indexes/6785dfa94cdd7e895ce16a24/generate?v=6785edf63d8c70ded566409f\&prompt=Tell+me+all+the+timestamps+in+the+advertisement+where+a+closeup+of+the+product+appears.\&cache=true\&mode=custom\&temp=0.2)                                         |


# Tune the temperature

Temperature is a configurable parameter that controls the randomness of the text output generated by the [`/summarize`](/v1.3/api-reference/analyze-videos/summarize) and [`/analyze`](/v1.3/api-reference/analyze-videos/analyze) endpoints . This parameter ranges from 0 to 1, inclusive, and the default value is 0.2.  A higher value generates more creative text, while a lower value produces more deterministic text output.

Common use cases include:

* **Writers**: Choose a higher temperature when seeking inspiration or a lower temperature for factual or technical writing.
* **Marketing professionals**: Choose a higher temperature to generate more unique and eye-catching content.
* **Educators and students**: Choose a lower temperature for summarizing educational content or a higher temperature for more engaging results.
* **Law enforcement and security**: Choose a lower temperature for generating factual reports, such as creating detailed and accurate police reports based on video evidence or drafting documents where precision is important.
* **Meeting facilitators**: Choose a lower temperature for taking accurate and concise meeting minutes. This setting helps capture key points, decisions, and action items.

The examples below are specific to using the [`/analyze`](/v1.3/api-reference/analyze-videos/analyze)  endpoint. However, the principles demonstrated are similar when using the [`/summarize`](/v1.3/api-reference/analyze-videos/summarize) endpoint.

# Prerequisites

* You’ve already [created an index](/v1.3/docs/concepts/indexes#create-an-index), and the Pegasus video understanding engine is enabled for this index.
* You've uploaded a video to your index, and the platform has finished indexing it.

# Example

The example code below demonstrates setting the value of the `temperature` parameter to 0.1 in the body of the request for this video: [Tim Urban: Inside the mind of a master procrastinator | TED](https://www.youtube.com/watch?v=arj7oStGLkU). This results in a more deterministic result than the default settings.


  ```python Python
  from twelvelabs import TwelveLabs

  client = TwelveLabs(api_key="")

  res = client.text(
    video_id='',
    prompt="I want to generate a description for my video with the following format: Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
    temperature=0.1,
    stream=False
  )
  print(f"Open-ended text: {res.data}")
  ```

  ```javascript Node.js
  import { TwelveLabs } from 'twelvelabs-js';

  const client = new TwelveLabs({ apiKey: 'YOUR_API_KEY'});

  const text = await client.analyze.text(
    '65e9487948db9fa780cb46c0',
    'I want to generate a description for my video with the following format: Title of the video, followed by a summary, highlighting the main topic, key events, and concluding remarks.',
    0.1,
    false
    );
  console.log(`Open-ended Text: ${text.data}`);
  ```


The result should look similar to the following one:

```text
Open-ended text: Title: TEDx Speaker Delivers an Inspiring Talk on Innovation and Creativity\n\n
Summary:\nIn this TEDx video, a speaker delivers an engaging and inspiring talk on the topics of 
innovation and creativity. The speaker, dressed in a black shirt, uses hand gestures and a remote 
control to navigate through his presentation slides, which include a blank grid and a TED logo. 
Throughout the talk, the speaker emphasizes the importance of embracing creativity and innovation 
in various aspects of life. He encourages the audience to stay aware of their own tendencies 
towards procrastination and the instant gratification monkey, and to start addressing their 
procrastination tasks as soon as possible. The audience is attentively listening and responding 
positively to the speaker's message, and the video concludes with the speaker smiling and 
acknowledging their applause.\n\nMain Topics:\n1. The importance of creativity and innovation\n2. 
Overcoming procrastination and the instant gratification monkey\n\nKey Events:\n1. The speaker uses 
hand gestures and a remote control to navigate through his presentation slides\n2. He emphasizes 
the importance of embracing creativity and innovation\n3. The speaker encourages the audience to 
stay aware of their own procrastination tendencies\n4. He concludes the talk by acknowledging the 
audience's applause\n\nConcluding Remarks:\nThe speaker's inspiring talk on innovation and 
creativity encourages the audience to embrace these concepts in their own lives and to be aware of 
their tendencies towards procrastination. By staying focused and addressing tasks as soon as 
possible, individuals can make significant progress and achieve their goals. The video concludes 
with the speaker's positive and uplifting message, leaving the audience feeling motivated and 
inspired.

```

Setting the `temperature` parameter to 0.9 would result in a more creative response, similar to the following one:

```text
Open-ended text: In the provided video clips, there are several TED Talks occurring, each 
featuring a speaker delivering engaging and thought-provoking presentations. The first talk, 
titled 'TED: Ideas Worth Spreading', showcases a speaker presenting on the importance of the 
spread of ideas and their impact on the world. He uses visual aids such as graphs and charts to 
emphasize his points.\n\nNext, in a talk titled 'The 5 Day Startup', a speaker delivers a 
presentation on starting a business within a five-day period. He highlights key steps and 
considerations involved in launching a successful venture.\n\nAnother talk, named 
'The Art of Public Speaking', focuses on the topic of effective communication through public 
speaking. The speaker encourages the audience to understand and harness the power of storytelling 
and persuasive language in their presentations. In this clip, the speaker's dynamic and expressive 
presentation style is evident.\n\nFinally, in a talk titled 'Easy and Fun', a speaker discusses 
the importance of balance between work and play, emphasizing that it is essential to prioritize 
both. The speaker uses various visual aids, including a monkey and a ship's wheel, to illustrate 
the points.\n\nAll the speakers in these talks are passionate and knowledgeable on their respective 
topics, ensuring an insightful and captivating delivery. The videos are well produced, with clear 
visuals and audio, further enhancing the audience's experience.

```

Using the default value for the `temperature` parameter would result in a response similar to the following one:

```text
Open-ended text:: Title: TEDx Talk - The Art of Procrastination\n\nSummary: In this TEDx talk, 
the speaker delves into the topic of procrastination, sharing insights into the origins of this 
human behavior and its psychological implications. The speaker uses vivid storytelling, humor, 
and relatable examples to engage the audience and encourage them to reevaluate their relationship 
with procrastination. The talk concludes with a call to action for viewers to recognize the role of 
the \"instant gratification monkey\" in their lives and take steps towards overcoming their 
procrastination habits.\n\nThe video opens with the speaker sharing his personal experience of 
procrastination, starting from his college days when he had to write a 90-page thesis. He describes 
how he came up with a plan to spread the work out evenly, but ended up procrastinating and having 
to write the entire thesis in just a few days before the deadline. The speaker then discusses the 
concept of short-term and long-term procrastination, explaining how the latter can be more insidious 
as it often goes unnoticed and can lead to significant regret over missed opportunities.\n\nThe 
speaker then introduces the idea of the \"instant gratification monkey,\" which is the part of our 
brain that seeks pleasure and avoids pain in the present moment. He shares that this monkey can 
cause us to delay important tasks and instead engage in activities that provide immediate 
satisfaction, such as browsing social media or watching TV. However, the speaker emphasizes that 
this behavior can lead to feelings of guilt, anxiety, and regret in the long term.\n\nTo illustrate 
the concept further, the speaker uses the analogy of a life calendar, emphasizing that each week 
of a 90-year life represents only one box. He encourages viewers to think about what they are truly 
procrastinating on and to be aware of the instant gratification monkey, as everyone experiences it 
to some degree. The speaker concludes his talk by thanking the audience for their attention and 
encouraging them to start addressing their procrastination habits sooner rather than later.\n\n
Throughout the talk, the speaker uses body language, facial expressions, and hand gestures to 
engage the audience and emphasize key points. He also uses visual aids, such as the life calendar 
and a blank grid, to illustrate his concepts. The audience is attentively listening and responding 
positively to the speaker's message, as evidenced by their nodding and applause. Overall, this 
TEDx talk provides valuable insights into the psychology of procrastination and offers practical 
advice for overcoming it.
```


# Prompt engineering

After your videos have been indexed by the Pegasus video language model, you can prompt the platform to analyze your video content and generate text outputs. Prompt engineering is the process of iteratively refining how you craft your instructions or questions to the model to improve the quality, relevance, and precision of the responses. Prompt engineering is important for enhancing the effectiveness of the model in various use cases, from content creation and summarization to question-answering, as shown below:

* **Improves accuracy**: Tailored prompts produce more precise outputs by clearly specifying the task.
* **Reduces ambiguity**: Well-designed prompts limit the model's scope for interpretation, ensuring relevant responses.
* **Enhances efficiency**: Effective prompts reduce the need for post-processing, saving time and resources.
* **Customizes outputs**: Through prompt engineering, you can tailor the outputs to specific tones, styles, or formats, meeting diverse requirements.
* **Provides context**: Prompts can provide essential context to the model, ensuring the output is relevant and appropriate for the given situation or domain.

# Steps in prompt engineering

The typical steps involved in prompt engineering are as follows:

1. **Define the objective**: Identify what you need from the model, such as generating a description of a video segment or answering a question based on video content.
2. **Craft the initial prompt**: Based on your objective, develop the initial version of your prompt. Include all necessary details and context, and specify the expected output format.
3. **Test and iterate**: Analyze the output and refine your prompt based on the results. This step may involve several iterations.

# Tips for writing better prompts

Crafting the perfect prompt is not achieved through a universal solution, as the effectiveness of a specific method can vary widely depending on the task at hand. However, the tips provided in this section can help enhance your prompt-writing skills. By experimenting with them, you can discover approaches that lead to more accurate and relevant responses.

## Provide examples

Examples guide the model in generating the expected output, reducing ambiguity, and ensuring the platform generates relevant responses. The following example creates a police report based on surveillance footage. It includes an example of a similar report to guide the model's response.

![](file:0edf672c-9cda-47c4-bf04-907f710e057f)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?mode=custom\&temp=0.2\&prompt=Write+a+police+report+based+on+this+video+using+the+following+example%3A%0A%0ADate%3A+11%2F01%2F2020%0ALocation%3A+San+Francisco+Police+Department%0AWitnesser’s+full+name%3A+John+Smith%0AReporter%3A+Barbara+Lim%0AOn+11%2F01%2F2020+around+5+PM%2C+I+saw+a+suspect+walking+in+a+retail+store+on+Height+Street…\&v=6791d98ecf67133816b2e01e) in your browser.

## Provide context

Providing context in prompts helps the platform understand your requirements, ensuring the generated response is accurately tailored to your needs. By providing context, you reduce the chances of irrelevant outputs. The following example provides the required context to customize the generated response according to your needs.

![](file:b3f73603-8d9f-4916-9436-3a1a0ddaf2c6)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?v=6785ee38027eec4fa50e80c5\&mode=custom\&temp=0.2\&prompt=Write+a+script+for+a+reporter+to+read+on+the+news+about+the+event+shown+in+this+video.) in your browser.

## Be specific

Specificity guides the model in producing highly relevant and targeted responses by aligning the output with your intentions. The following example indicates the exact aspect of the video you want the model to focus on - creating a daily workout plan for this week based on the workout routine mentioned in this video. This helps the model understand the scope of the prompt and generate a targeted response.

![](file:92431965-7c19-43a4-a449-4e309d724f7f)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?mode=custom\&temp=0.2\&prompt=Create+a+daily+workout+plan+for+this+week+based+on+based+on+the+workout+routine+mentioned+in+this+video.\&v=6785ee683d8c70ded56640c6) in your browser.

## Choose the type of prompt

Based on your requirements, differentiate between question-answering and description-based prompts, as each will guide the model's focus differently. The example prompt below is phrased as a question and instructs the model to list the filming techniques used in a video.

![](file:bbc4b5bc-d4e9-40f7-8906-f6614d22e4d0)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?mode=custom\&temp=0.2\&prompt=What+kind+of+filming+techniques+were+used+in+this+video%3F\&v=6785eecc027eec4fa50e80fb) in your browser.

## Specify the desired style and format for the output

Clearly state the desired output's length, style, and format (examples: JSON format, email) to ensure the output meets your requirements. The example below summarizes a video as an email, focusing on the five most important points.

![](file:8a9740a5-d2f1-4e58-8795-de4d8721d03e)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfa949d9c923603e5267/generate?mode=custom\&temp=0.2\&prompt=Write+a+summary+in+the+form+of+an+email.+In+your+summary%2C+focus+on+the+most+important+points+presented+in+the+video.\&v=6785ee23027eec4fa50e80bd) in your browser.

## Choose the language for the output

Specify if you want the output in a different language. The following example summarizes a video, indicating that the response should be in Spanish.

![](file:93f4669e-e2bd-4c92-b323-af8ab2594bc0)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?mode=custom\&temp=0.2\&prompt=Write+a+summary+in+Spanish.\&v=6785eec13d8c70ded56640e7) in your browser.

## Be concise

Being concise helps the model focus on the essential information. This speeds up processing and increases the likelihood of generating precise, relevant responses.

## Tune the temperature

Tuning the temperature controls the randomness of the text output. A lower temperature results in more deterministic results, which is ideal for tasks requiring high accuracy and specificity. In contrast, a higher temperature produces more creative text, which is suitable for brainstorming or creative writing tasks. Experiment with this setting to find the optimal balance that meets your objectives. For details, see the [Tune the temperature](/v1.3/docs/guides/analyze-videos/tune-the-temperature) page.

# Practical examples of prompt engineering

The examples in this section are from the Playground. However, the principles demonstrated are similar when invoking the API programmatically.

## From overview to detailed scene-by-scene descriptions

**Objective**: Move from an overview to precise scene-by-scene descriptions with timestamps.

**Initial prompt**:  Use "What is this video about?" as the initial prompt. This provides a general overview:

![](file:71285021-2c2d-4a1e-badf-d13107a86b00)

**Step-by-step refinement**:

1. Request structured breakdown - "Break down the main sections of this video with approximate timings." This provides a basic structure:

![](file:cc7b133a-29a4-4084-a555-e2254451fd31)

2. Retrieve detailed scene descriptions including timestamps - "Generate a detailed description of the video by scene and timestamp. The descriptions must be in the format of \[start\_time, end\_time] in seconds, followed by the description. Keep each description short." This provides precise timestamps and descriptions:

![](file:64058873-d924-4331-9f28-c787f29b7d78)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa4cdd7e895ce16a25/generate?mode=custom\&temp=0.2\&prompt=Generate+a+detailed+description+of+the+video+by+scene+and+timestamp.+The+descriptions+must+be+in+the+format+of+%5Bstart_time%2C+end_time%5D+in+seconds%2C+followed+by+the+description.+Keep+each+description+short.\&v=6785ef333d8c70ded5664109) in your browser.

## Find segments with precise timestamps

**Objective**: Locate specific moments or segments from a video with precise timestamps.

**Initial prompt**: Use "Show me the replays in this video" as the initial prompt. This identifies multiple replay segments:

![](file:22e10255-8d85-4f01-a757-00de41aca55f)

**Step-by-step refinement**:

1. Focus on a specific type of replay - "Show me the slow-motion replays only." This narrows down the results:
   ![](file:efd046fe-1ce2-472a-a12f-4bac2fb2f721)

2. Pinpoint the exact moment - "When does the slow-motion replay show the runners leaving their blocks?" This prompt identifies the moment:
   ![](file:705063af-d858-4b96-8898-a07842bc1dbc)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa4cdd7e895ce16a26/generate?mode=custom\&temp=0.2\&prompt=When+does+the+slow-motion+replay+show+the+runners+leaving+their+blocks%3F\&v=6785ef763d8c70ded5664116) in your browser.

## Write recipes from videos

**Objective**: Write a recipe for cooking macaroni and cheese based on a video.

**Initial prompt**: Use "Describe the recipe shown in this video." as the initial prompt. This yields a basic summary without detailed steps or quantities for each ingredient.

![](file:4307d818-ef7c-454e-a713-137ae456a496)

**Step-by-step refinement**:

1. Modify the prompt to specify the need for more detail - "List all ingredients and steps mentioned in the video for cooking macaroni and cheese." The response now contains a detailed list of ingredients and a step-by-step process. Still, it needs to capture all the nuances of the cooking technique and measurements.

![](file:18d3a80d-6d73-44a0-af07-85d1f2c2f1b1)

2. Further, refine your prompt to include cooking techniques, measurements, and any tips - "List all ingredients with measurements and detailed steps for cooking macaroni and cheese as shown in the video. Include any techniques or tips to ensure the dish turns out perfectly." This prompt captures the basic recipe and the specific nuances and precise measurements that make the dish special.

![](file:3cd63de5-0131-46a9-bce8-b8197a665026)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?mode=custom\&temp=0.2\&prompt=List+all+ingredients+with+measurements+and+detailed+steps+for+cooking+macaroni+and+cheese+as+shown+in+the+video.+Include+any+techniques+or+tips+to+ensure+the+dish+turns+out+perfectly.\&v=6785ee3d027eec4fa50e80c6) in your browser.

## Write a workout plan

**Objective**:  Write an email containing a workout plan based on a video.

**Initial prompt**: Use "List the workouts mentioned in this video." as the initial prompt. The screenshot below shows that the initial response doesn't provide details about the workouts, and it's not suitable to be used in an email:

![](file:269ce278-87ec-4708-bf46-8dfcab42690c)

**Step-by-step refinement**:

1. Refine your prompt by specifying that the model should briefly describe each workout - "List the workouts mentioned in this video. Add a brief description of up to two short sentences for each workout." The screenshot below shows that the refined prompt produces a more targeted and helpful response:

![](file:bbb23947-0082-4a57-acfd-0cba53e5c2d2)

2. Further, refine your prompt by specifying the format - "List the workouts mentioned in this video. Add a brief description of up to two short sentences for each workout. Format your response as an email." Now, the response should look like this:

   ![](file:b4d563c2-727f-41dd-b64e-eda5b5b218dd)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?mode=custom\&temp=0.2\&prompt=List+the+workouts+mentioned+in+this+video.+Add+a+brief+description+of+up+to+two+short+sentences+for+each+workout.+Format+your+response+as+an+email.\&v=6785ee683d8c70ded56640c6) in your browser.


# Create embeddings

Use the Embed API to create multimodal embeddings for videos, texts, images, and audio files. These embeddings are contextual vector representations that capture interactions between modalities, such as visual expressions, body language, spoken words, and video context. You can apply these embeddings to downstream tasks like training custom multimodal models for anomaly detection, diversity sorting, sentiment analysis, recommendations, or building Retrieval-Augmented Generation (RAG) systems.

Key features:

* **Native multimodal support**: Process all modalities natively without separate models or frame conversion.
* **State-of-the-art performance**: Captures motion and temporal information for accurate video interpretation.
* **Unified vector space**: Combines embeddings from different modalities for holistic understanding.
* **Fast and reliable**: Reduces processing time for large video sets.
* **Flexible segmentation**: Generate embeddings for video segments or the entire video.

Use cases:

* **Anomaly detection**: Identify unusual patterns, such as corrupt videos with black backgrounds, to improve data set quality.
* **Diversity sorting**: Organize data for broad representation, reducing bias and improving AI model training.
* **Sentiment analysis**: Combine vocal tone, facial expressions, and spoken language for accurate insights, which particularly useful for customer service.
* **Recommendations**: Use embeddings in similarity-based retrieval and ranking systems for recommendations.

To understand how your usage is measured and billed, see the [Pricing](https://www.twelvelabs.io/pricing) page.


  The platform can generate embeddings for text, audio, and image content types individually or in any combination within a single API call.










{/* # Prerequisites

- To use the platform, you need an API key:
     
         
             If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
         
         
             Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
         
         
             Select the **Copy** icon next to your key.
         
     
- Ensure the TwelveLabs SDK is installed on your computer:
 
     
         ```shell Python
         pip install twelvelabs
         ```
         ```shell Node.js
         yarn add twelvelabs-js
         ```
     
 
- To use the platform, you need an API key:
     
         
             If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
         
         
             Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
         
         
             Select the **Copy** icon next to your key.
         
     
- Ensure the TwelveLabs SDK is installed on your computer:
 
     
         ```shell Python
         pip install twelvelabs
         ```
         ```shell Node.js
         yarn add twelvelabs-js
         ```
     
 
- The videos you wish to use must meet the following requirements:
    - **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
    - **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  
    - **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).
    - **Duration**: Must be between 4 seconds and 2 hours (7,200s).
    - **File size**:  Must not exceed 2 GB.  
     If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).
- The audio files you wish to use use must meet the following requirements:
    - **Format**: WAV (uncompressed), MP3 (lossy), and FLAC (lossless)
    -  **File size**: Must not exceed 10MB.
- The images you wish to use use must meet the following requirements:
    - **Format**: JPEG and PNG.
    - **Dimension**:  Must be at least 128 x 128 pixels.
    - **Size**: Must not exceed 5MB. */}

{/* # Complete example

This complete example shows how you can create embeddings. Ensure you replace the placeholders surrounded by `<>` with your values. */}

{/* 
    
        
            ```Python Python
            ```
            ```JavaScript Node.js
            ```
        
    
    
            ```Python Python
            ```
            ```JavaScript Node.js
            ```
    
    
            ```Python Python
            ```
            ```JavaScript Node.js
            ```
    
    
            ```Python Python
            ```
            ```JavaScript Node.js
            ```
    
 */}

{/* 

Use the Embed API to create multimodal embeddings that are contextual vector representations for your videos,  texts, images, and audio files. TwelveLabs video embeddings capture all the subtle cues and interactions between different modalities, including the visual expressions, body language, spoken words, and the overall context of the video, encapsulating the essence of all these modalities and their interrelations over time.

You can utilize multimodal embeddings in various downstream tasks, including but not limited to training custom multimodal models for applications such as anomaly detection, diversity sorting, sentiment analysis, and recommendations. You can also use multimodal embeddings to construct Retrieval-Augmented Generation (RAG) systems.

The Embed API provides the following benefits:

- **Flexibility for any modality**: The API supports native processing of all modalities present in videos, eliminating the need for text-only or image-only models or converting videos into frames for image-based models.
- **State-of-the-art performance**: Unlike traditional approaches that use CLIP-like models, which do not account for motion, action, or temporal information in videos, TwelveLabs' video-native approach ensures a more accurate and temporally coherent interpretation of your video content.
- **Unified vector space**: You can integrate embeddings specific to each modality into a single, unified vector space, facilitating a more holistic understanding across all modalities. This approach surpasses traditional methods, offering a video-native understanding similar to human perception.
- **Fast and reliable**: With native support for video processing, the API significantly reduces the time required for processing. This is particularly beneficial if you have a large set of videos requiring high throughput and low latency.
- **Flexible video segmentation**: The API allows you to create multiple embeddings from different segments of a video or a single embedding for the entire video.


The platform can generate embeddings for text, audio, and image content types individually or in any combination within a single API call.


# Use cases

Below are several notable use cases:

- **Anomaly detection**: You can use the platform to identify unusual patterns or anomalies in diverse data types. For example, you can detect and remove corrupt videos that only display a black background, thereby enhancing the quality of data set curation.
- **Diversity sorting**: The platform helps you organize data to ensure a broad representation across various features, characteristics, or modalities. For example, in AI model training, especially with multimodal data, maintaining a diverse training set is crucial to minimize bias and enhance model generalization.
- **Sentiment analysis**: By integrating vocal tone, facial expressions, and spoken language from the video content, the platform provides more accurate insights than traditional text-only methods. This is particularly useful in customer service to effectively gauge client satisfaction.
- **Recommendations**: Use embeddings in embedding-similarity scores-based retrieval and ranking recommendation systems. */}


# Video embeddings

Video embeddings are vector representations of video content that enable various downstream tasks, such as search, classification, and recommendation. The platform provides two methods to create embeddings for your videos, each suited to different use cases.


  
    Upload your videos individually and create embeddings with a range of customization options. Ideal when you need fine-grained control over how embeddings are generated.
  

  
    It offers fewer customization options. Convenient when generating embeddings for videos that have already been indexed.
  



# Embeddings for new videos

This guide shows how you can create video embeddings.

The following table lists the available models for generating video embeddings and their key characteristics:

| Model                 | Description                                                                      | Dimensions | Clip length     | Similarity metric |
| --------------------- | -------------------------------------------------------------------------------- | ---------- | --------------- | ----------------- |
| Marengo-retrieval-2.7 | Use this model to create embeddings that you can use in various downstream tasks | 1024       | 2 to 10 seconds | Cosine similarity |

The “Marengo-retrieval-2.7” video understanding model generates embeddings for all modalities in the same latent space. This shared space enables any-to-any searches across different types of content.

The platform allows the creation of a single embedding for the entire video and multiple embeddings for specific segments. The default behavior is to create multiple embeddings, each 6 seconds long, for each video. You can modify the default behavior when you upload a video. For details, see the [Step 2: Upload a video](/v1.3/docs/guides/create-embeddings/video#upload-a-video) section below.

{/* This guide provides a complete example. For a simplified introduction with just the essentials, see the [Create embeddings](/v1.3/docs/get-started/quickstart/create-embeddings) quickstart guide. */}

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The videos you wish to use must meet the following requirements:
  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.

  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.

  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  * **Duration**: Must be between 4 seconds and 2 hours (7,200s).

  * **File size**:  Must not exceed 2 GB.\
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

# Complete example

This complete example illustrates creating video embeddings. Upload your videos and wait for the platform to process them, which takes some time. As a result, embedding creation is an asynchronous process. Ensure you replace the placeholders surrounded by `<>` with your values.


  ```Python Python maxLines=8
  from twelvelabs import TwelveLabs
  from typing import List
  from twelvelabs.models.embed import EmbeddingsTask, SegmentEmbedding

  # 1. Initialize the client
  client = TwelveLabs(api_key="")

  # 2. Upload a video
  task = client.embed.task.create(
      model_name="Marengo-retrieval-2.7",
      video_url="",
      # video_clip_length=5,
      # video_start_offset_sec=30,
      # video_end_offset_sec=60,
      # video_embedding_scopes=["clip" ,"video"]
  )
  print(
      f"Created task: id={task.id} model_name={task.model_name} status={task.status}")

  # 3. Monitor the status
  def on_task_update(task: EmbeddingsTask):
      print(f"  Status={task.status}")
  status = task.wait_for_done(sleep_interval=5, callback=on_task_update)
  print(f"Embedding done: {status}")

  # 4. Retrieve the embeddings
  task = task.retrieve(embedding_option=["visual-text", "audio"])

  # 5. Process the results
  def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
      for segment in segments:
          print(
              f"  embedding_scope={segment.embedding_scope} embedding_option={segment.embedding_option} start_offset_sec={segment.start_offset_sec} end_offset_sec={segment.end_offset_sec}"
          )
          print(f"  embeddings: {segment.embeddings_float[:max_elements]}")

  if task.video_embedding is not None and task.video_embedding.segments is not None:
      print_segments(task.video_embedding.segments)

  ```

  ```JavaScript Node.js maxLines=8
  import { TwelveLabs, EmbeddingsTask, SegmentEmbedding } from "twelvelabs-js";

  // 1. Initialize the client
  const client = new TwelveLabs({ apiKey: "" });

  // 2. Upload a video
  let task = await client.embed.task.create(
    "Marengo-retrieval-2.7",
    {
      url: "",
      // clipLength: 5,
      // startOffsetSec: 30,
      // endOffsetSec: 60,
      // videoEmbeddingScopes:["clip", "video"]
      }
  );
  console.log(`Created task: id=${task.id} modelName=${task.modelName} status=${task.status}`);

  // 3. Monitor the status
  const status = await task.waitForDone(5000, (task) => { console.log(`  Status=${task.status}`);});
  console.log(`Embedding done: ${status}`);

  // 4. Retrieve the embeddings
  task = await task.retrieve({ embeddingOption: ["visual-text", "audio"] });

  // 5. Process the results
  const printSegments = (segments, maxElements = 5) => {
    segments.forEach((segment) => {
      console.log(`embeddingScope=${segment.embeddingScope} embeddingOption=${segment.embeddingOption} startOffsetSec=${segment.startOffsetSec} endOffsetSec=${segment.endOffsetSec}`);
      console.log("embeddings: ", segment.embeddingsFloat.slice(0, maxElements));
    });
  };
  if (task.videoEmbedding) {
    if (task.videoEmbedding.segments) {
      printSegments(task.videoEmbedding.segments);
    }
  }
  ```


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/python/the-twelve-labs-class#the-initializer) of the `TwelveLabs` class.

        **Parameters**:

        * `api_key`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        To create video embeddings, you must first upload your videos, and the platform must finish processing them.

        **Function call**: You call the [`embed.task.create`](/v1.3/sdk-reference/python/create-video-embeddings#create-a-video-embedding-task) function.

        **Parameters**:

        * `model_name`: The name of the model you want to use ("Marengo-retrieval-2.7").
        * `video_url` or `video_file`: The publicly accessible URL or the path of your video file.
          
            * The platform supports uploading video files that can play without additional user interaction or custom video players. Ensure your URL points to the raw video file, not a web page containing the video. Links to third-party hosting sites, cloud storage services, or videos requiring extra steps to play are not supported.
            * Youtube URLs are not supported for Embed API at this time.
          
        * *(Optional)* `video_start_offset_sec`: The start offset in seconds from the beginning of the video where processing should begin.
        * *(Optional)* `video_end_offset_sec`: The end offset in seconds from the beginning of the video where processing should end.
        * *(Optional)* `video_clip_length`: The desired duration in seconds for each clip for which the platform generates an embedding. It can be between 2 and 10 seconds. Note that the platform automatically truncates video segments shorter than 2 seconds. This truncation only applies to the last segment if it does not meet the minimum length requirement of 2 seconds. Example: for a 31-second video divided into 6-second segments, the final 1-second segment will be truncated.
        * *(Optional)* `video_embedding_scopes`: The scope of the generated embeddings. Valid values are the following:
          * `["clip"]`: Creates embeddings for multiple clips, as specified by the video\_start\_offset\_sec, video\_end\_offset\_sec, video\_clip\_length parameters described below. This is the default value.
          * `["clip", "video"]`: Creates embeddings for specific video segments and the entire video in a single request.


        **Return value**: An object containing, among other a information, a field named `id`, which represents the unique identifier of your video embedding task. You can use this object to track the status of your video embedding task.
      

      
        The platform requires some time to process videos. Check the status of the video embedding task until it's completed.

        **Function call**: You call the [`embed.task.wait_for_done`](/v1.3/sdk-reference/python/create-video-embeddings#wait-for-a-video-embedding-task-to-complete) function.

        **Parameters**:

        * `sleep_interval`: The time interval, in seconds, between successive status checks. In this example, the method checks the status every two seconds. Adjust this value to control how frequently the method checks the status.
        * `callback`: A callback function that the SDK executes each time it checks the status. In this example, `on_task_update` is the callback function.

          **Return value**:
 An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        Once the platform has finished processing your video, you can retrieve the embeddings.

        **Function call**: You call the [`embed.task.retrieve`](/v1.3/sdk-reference/python/create-video-embeddings#retrieve-video-embeddings) function.

        **Parameters**:

        * *(Optional)* `embedding_option`: An array of strings that can contain one or more of the following values:
          * `visual-text`: Returns visual embeddings optimized for text search.
          * `audio`: Returns audio embeddings.

            The default value is `embedding_option=["visual-text", "audio"]`.


        **Return value**: The response contains, among other information, an object named `video_embedding` that contains the embedding data for your video. This object includes the following fields:

        * `segments`: An array of objects, each representing a segment of the video with its embedding data. Each item contains:
          * `embeddings_float`: An array of numbers representing the embedding vector for the segment.
          * `start_offset_sec`: The start time of the segment in seconds.
          * `end_offset_sec`: The end time of the segment in seconds.
          * `embedding_ scope`: The scope of the embedding.
          * `embedding_option`: The type of the embedding.
        * `metadata`: An object containing metadata associated with the embedding.
      

      
        This example iterates over the results and prints the key properties and a portion of the embedding vectors for each segment.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/node-js/the-twelve-labs-class#the-constructor) of the `TwelveLabs` class.

        **Parameters**:

        * `apiKey`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        To create video embeddings, you must first upload your videos, and the platform must finish processing them.

        **Function call**: You call the [`embed.task.create`](/v1.3/sdk-reference/node-js/create-video-embeddings#create-a-video-embedding-task) function.


        **Parameters**:

        * `modelName`: The name of the model you want to use ("Marengo-retrieval-2.7").
        * `options`: An object that specifies the source of the video file and the options for processing. This object can include the following properties:
          * `url` or `file`: The publicly accessible URL or the path of your video file.
            
              * The platform supports uploading video files that can play without additional user interaction or custom video players. Ensure your URL points to the raw video file, not a web page containing the video. Links to third-party hosting sites, cloud storage services, or videos requiring extra steps to play are not supported.
              * Youtube URLs are not supported for Embed API at this time.
            
          * *(Optional)* `startOffsetSec`: The start offset in seconds from the beginning of the video where processing should begin.
          * *(Optional)* `endOffsetSec`: The end offset in seconds from the beginning of the video where processing should end.
          * *(Optional)* `clipLength`: The desired duration in seconds for each clip for which the platform generates an embedding. It can be between 2 and 10 seconds. Note that the platform automatically truncates video segments shorter than 2 seconds. This truncation only applies to the last segment if it does not meet the minimum length requirement of 2 seconds. Example: for a 31-second video divided into 6-second segments, the final 1-second segment will be truncated.
          * *(Optional)* `scopes`: The scope of the generated embeddings. Valid values are the following:
            * `["clip"]`: Creates embeddings for multiple clips, as specified by the video\_start\_offset\_sec, video\_end\_offset\_sec, video\_clip\_length parameters described below. This is the default value.
            * `["clip", "video"]`: Creates embeddings for specific video segments and the entire video in a single request.


        **Return value**: An object containing, among other a information, a field named `id`, which represents the unique identifier of your video embedding task. You can use this object to track the status of your video embedding task.
      

      
        The platform requires some time to process videos. Check the status of the video embedding task until it's completed.

        **Function call**: You call the [`embed.task.waitForDone`](/v1.3/sdk-reference/node-js/create-video-embeddings#wait-for-a-video-embedding-task-to-complete) function.

        **Parameters**:

        * `sleepInterval`: The time interval, in milliseconds, between successive status checks. In this example, the method checks the status every two seconds. Adjust this value to control how frequently the method checks the status.
        * `callback`: A callback function that the SDK executes each time it checks the status.

          **Return value**:
 An object containing, among other information, a field named `status` representing the status of your task. Wait until the value of this field is `ready`.
      

      
        Once the platform has finished processing your video, you can retrieve the embeddings.

        **Function call**: You call the [`embed.task.retrieve`](/v1.3/sdk-reference/node-js/create-video-embeddings#retrieve-video-embeddings) function.

        **Parameters**:

        * *(Optional)* `embedding_option`: An array of strings that can contain one or more of the following values:
          * `visual-text`: Returns visual embeddings optimized for text search.
          * `audio`: Returns audio embeddings.

            The default value is `embeddingOption=["visual-text", "audio"]`.


        **Return value**: The response contains, among other information, an object named `videoEmbedding` that contains the embedding data for your video. This object includes the following fields:

        * `segments`: An array of objects, each representing a segment of the video with its embedding data. Each item contains:
          * `float`: An array of numbers representing the embedding vector for the segment.
          * `startOffsetSec`: The start time of the segment in seconds.
          * `endOffsetSec`: The end time of the segment in seconds.
          * `embeddingScope`: The scope of the embedding.
          * `embeddingOption`: The type of the embedding.
        * `metadata`: An object containing metadata associated with the embedding.
      

      
        This example iterates over the results and prints the key properties and a portion of the embedding vectors for each segment.
      
    
  



# Embeddings for indexed videos

The platform allows you to retrieve embeddings for videos you've already uploaded and indexed. The embeddings are generated using video scene detection. Video scene detection enables the segmentation of videos into semantically meaningful parts. It involves identifying boundaries between scenes, defined as a series of frames depicting a continuous action or theme. Each segment is between 2 and 10 seconds.

# Prerequisites

Your video must be indexed with the Marengo video understanding model version 2.7 or later. For details on enabling this model for an index, see the [Create an index](/v1.3/docs/concepts/indexes#create-an-index) page.

# Complete example

{/* This complete example illustrates creating embeddings for videos that have already been indexed. Ensure you replace the placeholders surrounded by `<>` with your values. */}


  ```python Python
  from twelvelabs import TwelveLabs
  from typing import List
  from twelvelabs.models.embed import SegmentEmbedding

  #1. Retrieve the embeddings
  video = client.index.video.retrieve(
      index_id="", id="", embedding_option=["visual-text", "audio"])

  #2. Process the results
  def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
      for segment in segments:
          print(
              f"  embedding_scope={segment.embedding_scope} embedding_option={segment.embedding_option} start_offset_sec={segment.start_offset_sec} end_offset_sec={segment.end_offset_sec}"
          )
          print(f"  embeddings: {segment.embeddings_float[:max_elements]}")

  if video.embedding:
      print(f"Model_name={video.embedding.model_name}")
      print("Embeddings:")
      print_segments(video.embedding.video_embedding.segments)
  ```

  ```javascript Node.js
  import { TwelveLabs, SegmentEmbedding } from "twelvelabs-js";

  //1. Retrieve the embeddings
  const video = await client.index.video.retrieve(
      "",
      "",
      { embeddingOption: ['visual-text', 'audio'] }
  );

  //2. Process the results
  const printSegments = (segments: SegmentEmbedding[], maxElements = 5) => {
    segments.forEach((segment) => {
      console.log(
        `  embedding_scope=${segment.embeddingScope} embedding_option=${segment.embeddingOption} start_offset_sec=${segment.startOffsetSec} end_offset_sec=${segment.endOffsetSec}`
      );
      console.log(
        "  embeddings: ",
        segment.embeddingsFloat.slice(0, maxElements)
      );
    });
  };

  if (video.embedding) {
      console.log(`Model name: ${video.embedding.modelName}`);
      console.log("Embeddings:");
      printSegments(video.embedding.videoEmbedding.segments);
  }
  ```


# Step-by-step guide


  
    
      
        **Function call**: You call the [`index.video.retrieve`](/v1.3/sdk-reference/python/manage-videos#retrieve-video-information) function.

        **Parameters**:

        * `index_id`: The unique identifier of the index containing your video.
        * `id`: The unique identifier of your video.
        * `embedding_option`: The types of embeddings to retrieve. This example uses `["visual-text", "audio"]`.

          **Return value**: The response contains, among other information, an object named `embedding` that contains the embedding data for your video.
      

      
        This example iterates over the results and prints the key properties and a portion of the embedding vectors for each segment.
      
    
  

  
    
      
        **Function call**: You call the [`index.video.retrieve`](/v1.3/sdk-reference/node-js/manage-videos#retrieve-video-information) function.

        **Parameters**:

        * `indexId`: The unique identifier of the index containing your video.
        * `id`: The unique identifier of your video.
        * `embeddingOption`: The types of embeddings to retrieve. This example uses `['visual-text', 'audio']`.

          **Return value**: The response contains, among other information, an object named `embedding` that contains the embedding data for your video.
      

      
        This example iterates over the results and prints the key properties and a portion of the embedding vectors for each segment.
      
    
  



# Text embeddings

This guide shows how you can create text embeddings.

The following table lists the available models for generating text embeddings and their key characteristics:

| Model                 | Description                                                                      | Dimensions | Max tokens | Similarity metric |
| --------------------- | -------------------------------------------------------------------------------- | ---------- | ---------- | ----------------- |
| Marengo-retrieval-2.7 | Use this model to create embeddings that you can use in various downstream tasks | 1024       | 77         | Cosine similarity |

The “Marengo-retrieval-2.7” video understanding model generates embeddings for all modalities in the same latent space. This shared space enables any-to-any searches across different types of content.

{/* This guide provides a complete example. For a simplified introduction with just the essentials, see the [Create embeddings](/v1.3/docs/get-started/quickstart/create-embeddings) quickstart guide. */}

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  
* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

# Complete example

This complete example shows how you can create text embeddings. Ensure you replace the placeholders surrounded by `<>` with your values.


  ```Python Python maxLines=8
  from twelvelabs import TwelveLabs
  from typing import List
  from twelvelabs.models.embed import SegmentEmbedding

  # 1. Initialize the client
  client = TwelveLabs(api_key="")

  # 2. Create text embeddings
  res = client.embed.create(
      model_name="Marengo-retrieval-2.7",
      text="",
      # text_truncate="start"
  )

  # 3. Process the results
  print(f"Created text embedding: model_name={res.model_name}")
  def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
      for segment in segments:
          print(f"  embeddings: {segment.embeddings_float[:max_elements]}")
  if res.text_embedding is not None and res.text_embedding.segments is not None:
      print_segments(res.text_embedding.segments)
  ```

  ```JavaScript Node.js maxLines=8
  import { TwelveLabs } from "twelvelabs-js";

  // 1. Initialize the client
  const client = new TwelveLabs({ apiKey: "" });

  // 2. Create text embeddings
  let res = await client.embed.create({
    modelName: "Marengo-retrieval-2.7",
    text: "",
    // textTruncate: "start",
  });

  // 3. Process the results
  console.log(`Created text embedding: modelName=${res.modelName}`);
  const printSegments = (segments, maxElements = 5) => {
    segments.forEach((segment) => {
      console.log(
        "  embeddings: ",
        segment.embeddingsFloat.slice(0, maxElements)
      );
    });
  };
  if (res.textEmbedding?.segments) {
    printSegments(res.textEmbedding.segments);
  }

  ```


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/python/the-twelve-labs-class#the-initializer) of the `TwelveLabs` class.

        **Parameters**:

        * `api_key`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        **Function call**: You call the [`embed.create`](/v1.3/sdk-reference/python/create-text-image-and-audio-embeddings#create-text-image-and-audio-embeddings) function.

        **Parameters**:

        * `model_name`: The name of the model you want to use ("Marengo-retrieval-2.7").
        * `text`: The text for which you wish to create an embedding.
        * *(Optional)* `text_truncate`: A string that specifies how the platform truncates text that exceeds 77 tokens to fit the maximum length allowed for an embedding. This parameter can take one of the following values:
          * `start`: The platform will truncate the start of the provided text.
          * `end`: The platform will truncate the end of the provided text. This is the default value.
          * `none`: The platform will return an error if the text is longer than the maximum token limit.


        **Return value**: The response contains the following fields:

        * `text_embedding`: An object that contains the embedding data for your text. It includes the following fields:
          * `segments`: An object that contains the following:
            * `float`: An array of floats representing the embedding
          * `metadata`: An object that contains metadata about the embedding.
        * `model_name`: The name ofhe video understanding model the platform has used to create this embedding.
      

      
        This example prints the results to the standard output.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/node-js/the-twelve-labs-class#the-constructor) of the `TwelveLabs` class.

        **Parameters**:

        * `apiKey`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        **Function call**: You call the [`embed.create`](/v1.3/sdk-reference/node-js/create-text-image-and-audio-embeddings) function.

        **Parameters**:

        * `modelName`: The name of the model you want to use ("Marengo-retrieval-2.7").
        * `text`: The text for which you wish to create an embedding.
        * *(Optional)* `textTruncate`: A string that specifies how the platform truncates text that exceeds 77 tokens to fit the maximum length allowed for an embedding. This parameter can take one of the following values:
          * `start`: The platform will truncate the start of the provided text.
          * `end`: The platform will truncate the end of the provided text. This is the default value.
          * `none`: The platform will return an error if the text is longer than the maximum token limit.


        **Return value**: The response contains the following fields:

        * `textEmbedding`: An object that contains the embedding data for your text. It includes the following fields:
          * `segments`: An object that contains the following:
            * `float`: An array of floats representing the embedding
          * `metadata`: An object that contains metadata about the embedding.
        * `modelName`: The name ofhe video understanding model the platform has used to create this embedding.
      

      
        This example prints the results to the standard output.
      
    
  



# Audio embeddings

This guide shows how you can create audio embeddings.

The following table lists the available models for generating audio embeddings and their key characteristics:

| Model                 | Description                                                                      | Dimensions | Max length | Similarity metric |
| :-------------------- | :------------------------------------------------------------------------------- | :--------- | :--------- | :---------------- |
| Marengo-retrieval-2.7 | Use this model to create embeddings that you can use in various downstream tasks | 1024       | 10 seconds | Cosine similarity |

Note that the “Marengo-retrieval-2.7” video understanding model generates embeddings for all modalities in the same latent space. This shared space enables any-to-any searches across different types of content.

The platform processes audio files up to 10 seconds in length. Files longer than 10 seconds are automatically truncated.

{/* This guide provides a complete example. For a simplified introduction with just the essentials, see the [Create embeddings](/v1.3/docs/get-started/quickstart/create-embeddings) quickstart guide. */}

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The audio files you wish to use must meet the following requirements:
  * **Format**: WAV (uncompressed), MP3 (lossy), and FLAC (lossless)
  * **File size**: Must not exceed 10MB.

# Complete example

This complete example shows how you can create audio embeddings. Ensure you replace the placeholders surrounded by `<>` with your values.


  ```Python Python maxLines=8
  from twelvelabs import TwelveLabs
  from typing import List
  from twelvelabs.models.embed import SegmentEmbedding

  # 1. Initialize the client
  client = TwelveLabs(api_key="")

  # 2. Create audio embeddings
  res = client.embed.create(
    model_name="Marengo-retrieval-2.7",
    audio_url="",
    # audio_start_offset_sec=2
  )

  # 3. Process the results
  print(f"Created audio embedding: model_name={res.model_name}")
  def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
      for segment in segments:
          print(f"  start_offset_sec={segment.start_offset_sec}")
          print(f"  embeddings: {segment.embeddings_float[:max_elements]}")

  if res.audio_embedding is not None and res.audio_embedding.segments is not None:
      print_segments(res.audio_embedding.segments)
  ```

  ```JavaScript Node.js maxLines=8
  import { TwelveLabs } from "twelvelabs-js";

  // 1. Initialize the client
  const client = new TwelveLabs({ apiKey: "" });

  // 2. Create audio embeddings
  const res = await client.embed.create({
    modelName: "Marengo-retrieval-2.7",
    audioUrl: "",
    // audioStartOffsetSec: 2,
  });

  // 3. Process the results
  console.log(`Created audio embedding: modelName=${res.modelName}`);
  const printSegments = (segments, maxElements = 5) => {
    segments.forEach((segment) => {
      console.log(`  start_offset_sec=${segment.startOffsetSec}`);
      console.log(
        "  embeddings: ",
        segment.embeddingsFloat.slice(0, maxElements)
      );
    });
  };
  if (res.audioEmbedding?.segments) {
    printSegments(res.audioEmbedding.segments);
  }
  ```


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/python/the-twelve-labs-class#the-initializer) of the `TwelveLabs` class.

        **Parameters**:

        * `api_key`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        **Function call**: You call the [`embed.create`](/v1.3/sdk-reference/python/create-text-image-and-audio-embeddings#create-text-image-and-audio-embeddings) function.

        **Parameters**:

        * `model_name`: The name of the model you want to use ("Marengo-retrieval-2.7").
        * `audio_url` or `audio_file`: The publicly accessible URL or the path of your audio file.
        * *(Optional)* `audio_start_offset_sec`: The start time, in seconds, from which the platform generates the audio embeddings. This parameter allows you to skip the initial portion of the audio during processing.


        **Return value**: The response contains the following fields:

        * `audio_embedding`: An object that contains the embedding data for your audio file. It includes the following fields:
          * `segments`: An object that contains the following:
          * `float`: An array of floats representing the embedding
          * `start_offset_sec`: The start time.
          * `metadata`: An object that contains metadata about the embedding.
        * `model_name`: The name ofhe video understanding model the platform has used to create this embedding.
      

      
        This example prints the results to the standard output.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/node-js/the-twelve-labs-class#the-constructor) of the `TwelveLabs` class.

        **Parameters**:

        * `apiKey`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        **Function call**: You call the [`embed.create`](/v1.3/sdk-reference/node-js/create-text-image-and-audio-embeddings) function.

        **Parameters**:

        * `modelName`: The name of the model you want to use ("Marengo-retrieval-2.7").
        * `audioUrl` or `audioFile`: The publicly accessible URL or the path of your audio file.
        * `audioStartOffsetSec`: The start time, in seconds, from which the platform generates the audio embeddings. This parameter allows you to skip the initial portion of the audio during processing.


        **Return value**: The response contains the following fields:
        -`audioEmbedding`: An object that contains the embedding data for your audio file. It includes the following fields:

        * `segments`: An object that contains the following:
        * `float`: An array of floats representing the embedding
        * `startOffsetSec`: The start time.
        * `metadata`: An object that contains metadata about the embedding.
        * `modelName`: The name ofhe video understanding model the platform has used to create this embedding.
      

      
        This example prints the results to the standard output.
      
    
  



# Image embeddings

This guide shows how you can create image embeddings.

The following table lists the available models for generating text embeddings and their key characteristics:

| Model                 | Description                                                                      | Dimensions | Max size | Similarity metric |
| :-------------------- | :------------------------------------------------------------------------------- | :--------- | :------- | :---------------- |
| Marengo-retrieval-2.7 | Use this model to create embeddings that you can use in various downstream tasks | 1024       | 5 MB     | Cosine similarity |

The “Marengo-Retrieval-2.7” video understanding model generates embeddings for all modalities in the same latent space. This shared space enables any-to-any searches across different types of content.

{/* This guide provides a complete example. For a simplified introduction with just the essentials, see the [Create embeddings](/v1.3/docs/get-started/quickstart/create-embeddings) quickstart guide. */}

# Prerequisites

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

* Ensure the TwelveLabs SDK is installed on your computer:

  
    ```shell Python
    pip install twelvelabs
    ```

    ```shell Node.js
    yarn add twelvelabs-js
    ```
  

* The images you wish to use use must meet the following requirements:
  * **Format**: JPEG and PNG.
  * **Dimension**:  Must be at least 128 x 128 pixels.
  * **Size**: Must not exceed 5MB.

# Complete example

This complete example shows how you can create image embeddings. Ensure you replace the placeholders surrounded by `<>` with your values.


  ```Python Python maxLines=8
  from twelvelabs import TwelveLabs
  from typing import List
  from twelvelabs.models.embed import SegmentEmbedding

  # 1. Initialize the client
  client = TwelveLabs(api_key="")

  # 2. Create image embeddings
  res = client.embed.create(
      model_name="Marengo-retrieval-2.7", image_url="")

  # 3. Process the results
  print(f"Created image embedding: model_name={res.model_name}")
  def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
      for segment in segments:
          print(f"  embeddings: {segment.embeddings_float[:max_elements]}")

  if res.image_embedding is not None and res.image_embedding.segments is not None:
      print_segments(res.image_embedding.segments)
  ```

  ```JavaScript Node.js maxLines=8
  import { TwelveLabs } from "twelvelabs-js";

  // 1. Initialize the client
  const client = new TwelveLabs({ apiKey: "",
  });

  // 3. Process the results
  console.log(`Created image embedding: modelName=${res.modelName}`);
  const printSegments = (segments, maxElements = 5) => {
    segments.forEach((segment) => {
      console.log(
        "  embeddings: ",
        segment.embeddingsFloat.slice(0, maxElements)
      );
    });
  };
  if (res.imageEmbedding?.segments) {
    printSegments(res.imageEmbedding.segments);
  }
  ```


# Step-by-step guide


  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/python/the-twelve-labs-class#the-initializer) of the `TwelveLabs` class.

        **Parameters**:

        * `api_key`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        **Function call**: You call the [`embed.create`](/v1.3/sdk-reference/python/create-text-image-and-audio-embeddings#create-text-image-and-audio-embeddings) function.

        **Parameters**:

        * `model_name`: The name of the model you want to use ("Marengo-retrieval-2.7").
        * `image_url` or `image_file`: The publicly accessible URL or the path of your image file. 


        **Return value**: The response contains the following fields:

        * `image_embedding`: An object that contains the embedding data for your image file. It includes the following fields:
          * `segments`: An object that contains the following:
            * `float`: An array of floats representing the embedding
          * `metadata`: An object that contains metadata about the embedding.
        * `model_name`: The name ofhe video understanding model the platform has used to create this embedding.
      

      
        This example prints the results to the standard output.
      
    
  

  
    
      
        Create a client instance to interact with the TwelveLabs Video Understanding Platform.

        **Function call**: You call the [constructor](/v1.3/sdk-reference/node-js/the-twelve-labs-class#the-constructor) of the `TwelveLabs` class.

        **Parameters**:

        * `apiKey`: The API key to authenticate your requests to the platform.


        **Return value**: An object of type `TwelveLabs` configured for making API calls.
      

      
        **Function call**: You call the [`embed.create`](/v1.3/sdk-reference/node-js/create-text-image-and-audio-embeddings) function.

        **Parameters**:

        * `modelName`: The name of the model you want to use ("Marengo-retrieval-2.7").
        * `imageUrl` or `imageFile`: The publicly accessible URL or the path of your image file. 


        **Return value**: The response contains the following fields:

        * `imageEmbedding`: An object that contains the embedding data for your image file. It includes the following fields:
          * `segments`: An object that contains the following:
            * `float`: An array of floats representing the embedding
          * `metadata`: An object that contains metadata about the embedding.
        * `modelName`: The name ofhe video understanding model the platform has used to create this embedding.
      

      
        This example prints the results to the standard output.
      
    
  



# Models

TwelveLabs' video understanding models consist of a family of deep neural networks built on our multimodal foundation model for video understanding that you can use for the following downstream tasks:

* Search using natural language queries
* Analyze videos to generate text

Videos contain multiple types of information, including visuals, sounds, spoken words, and texts. The human brain combines all types of information and their relations with each other to comprehend the overall meaning of a scene. For example, you're watching a video of a person jumping and clapping, both visual cues, but the sound is muted. You might realize they're happy, but you can't understand why they're happy without the sound. However, if the sound is unmuted, you could realize they're cheering for a soccer team that scored a goal.

Thus, an application that analyzes a single type of information can't provide a comprehensive understanding of a video. TwelveLabs' video understanding models, however, analyze and combine information from all the modalities to accurately interpret the meaning of a video holistically, similar to how humans watch, listen, and read simultaneously to understand videos.

Our video understanding models have the ability to identify, analyze, and interpret a variety of elements, including but not limited to the following:

| Element                              | Modality | Example                                                            |
| :----------------------------------- | :------- | :----------------------------------------------------------------- |
| People, including famous individuals | Visual   | Michael Jordan, Steve Jobs                                         |
| Actions                              | Visual   | Running, dancing, kickboxing                                       |
| Objects                              | Visual   | Cars, computers, stadiums                                          |
| Animals or pets                      | Visual   | Monkeys, cats, horses                                              |
| Nature                               | Visual   | Mountains, lakes, forests                                          |
| Text displayed on the screen (OCR)   | Visual   | License plates, handwritten words, number on a player's jersey     |
| Brand logos                          | Visual   | Nike, Starbucks, Mercedes                                          |
| Shot techniques and effects          | Visual   | Aerial shots, slow motion, time-lapse                              |
| Counting objects                     | Visual   | Number of people in a crowd, items on a shelf, vehicles in traffic |
| Sounds                               | Audio    | Chirping (birds), applause, fireworks popping or exploding         |
| Human speech                         | Audio    | "Good morning. How may I help you?"                                |
| Music                                | Audio    | Ominous music, whistling, lyrics                                   |

# Supported languages

The platform supports the following languages for processing visual and audio content, understanding queries or prompts, and generating outputs:

* **Full support**: English
* **Partial support**: Arabic, Chinese, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Thai, Vietnamese


# Marengo

Marengo is an embedding model for comprehensive video understanding. The current version is **Marengo 2.7**.

Marengo analyzes multiple modalities in video content, including visuals, audio, and text, to provide a holistic understanding similar to human comprehension.

# Available models

| Model                 | Purpose                                |
| --------------------- | -------------------------------------- |
| Marengo 2.7           | Search using text or image queries     |
| Marengo-retrieval-2.7 | Create embeddings for downstream tasks |

# Key features

* **Multimodal processing**: Combines visual, audio, and text elements for comprehensive understanding
* **Fine-grained search**: Detects brand logos, text, and small objects (as small as 10% of the video frame)
* **Motion search**: Identifies and analyzes movement within videos
* **Counting capabilities**: Accurately counts objects in video frames
* **Audio comprehension**: Analyzes music, lyrics, sound, and silence

# Use cases

* **Search**: Use natural language queries to find specific content within videos
* **Embeddings**: Create video embeddings for various downstream applications

# Examples

This section contains examples of using the Marengo video understanding model.

## Steve Jobs introducing the iPhone

In the example screenshot below, the query was "How did Steve Jobs introduce the iPhone?".  The Marengo video understanding model used information found in the visual and conversation modalities to perform the following tasks:

* Visual recognition of a famous person (Steve Jobs)
* Joint speech and visual recognition to semantically search for the moment when Steve Jobs introduced the iPhone. Note that semantic search finds information based on the intended meaning of the query rather than the literal words you used, meaning that the platform identified the matching video fragments even if Steve Jobs didn't explicitly say the words in the query.

![](file:16261b8f-4fbe-4148-8538-77afac236ec3)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/search?qt=How+did+Steve+Jobs+introduce+the+iPhone%3F\&so=visual\&so=audio\&th=medium) in your browser.

## Polar bear holding a Coca-Cola bottle

In the example screenshot below, the query was "Polar bear holding a Coca-Cola bottle." The Marengo video understanding model used information found in the visual and logo modalities to perform the following tasks:

* Recognition of a cartoon character (polar bear)
* Identification of an object (bottle)
* Detection of a specific brand logo (Coca-Cola)
* Identification of an action (polar bear holding a bottle)

![](file:f8d6e6f8-1ab0-42f3-b507-4a14ffda165b)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/search?qt=Polar+bear+holding+a+Coca-Cola+bottle\&so=visual\&so=audio\&th=medium) in your browser.

## Using different languages

This section provides examples of using different languages to perform search requests.

### Spanish

In the example screenshot below, the query was "¿Cómo presentó Steve Jobs el iPhone?" ("How did Steve Jobs introduce the iPhone?"). The Marengo video understanding model used information from the visual and audio modalities.

![](file:05f9c3fd-6e35-47ff-9a14-6869fa4510e1)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/search?qt=¿Cómo+presentó+Steve+Jobs+el+iPhone%3F\&so=visual\&so=audio\&th=medium) in your browser.

### Chinese

In the example screenshot below, the query was "猫做有趣的事情" ("Cats doing funny things."). The Marengo video understanding model used information from the visual modality.

![](file:935aae08-5cf1-4f89-a1fa-0455c9f1a38d)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/search?qt=猫做有趣的事情\&so=visual\&so=audio\&th=medium) in your browser.

### French

In the example screenshot below, the query was "J'ai trouvé la solution" ("I found the solution."). The Marengo video understanding model used information from the visual modality (text displayed on the screen).

![](file:414bf3d4-dc74-48a1-b55f-362e7de261c9)

# Support

For support or feedback regarding Marengo, contact [support@twelvelabs.io](mailto:support@twelvelabs.io).


# Pegasus

Pegasus is a generative model for video-to-text generation. The current version is **Pegasus 1.2**.

Pegasus analyzes multiple modalities to generate contextually relevant text based on the content of your videos.

# Key features

* **Video-to-text generation**: Creates detailed textual descriptions based on video content
* **Extended processing capacity**: Processes videos up to 1 hour in length
* **Granular visual comprehension**: Analyzes objects, on-screen text, and numerical content
* **Temporal grounding**: Accurately identifies timestamps of specific events
* **Multimodal understanding**: Combines visual, audio, and textual information for comprehensive analysis

# Use cases

* **Content summarization**: Generate concise summaries of video content
* **Detailed descriptions**: Create comprehensive textual descriptions of visual scenes
* **Timestamp identification**: Answer questions about when specific events occur in videos
* **Content analysis**: Extract key information from video content for further processing

# Examples

This section contains examples of using the Pegasus video understanding model.

## Summarizing educational videos

In the example screenshot below, the platform has summarized an educational video using predefined templates without any customization:

![](file:cef63944-d79a-480d-9f57-18f2d9dfb6ec)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfa949d9c923603e5267/generate?v=6785ee30027eec4fa50e80c2\&mode=template\&st=summary\&sp=Summarize+this+video\&temp=0.2) in your browser.

## Generating captions for social media

In the example screenshot below, the prompt instructs the platform to generate a caption for a social media post:

![](file:157f2ea1-a9a3-46fd-b9f5-1293e6647dea)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfa949d9c923603e5267/generate?mode=custom\&temp=0.2\&v=6785ee183d8c70ded56640ad\&prompt=Generate+an+attention-grabbing+caption+for+a+social+media+post.+Keep+it+shorter+than+200+characters.) in your browser.

## Writing police reports

In the example screenshot below, the prompt instructs the platform to write a police report using a specific template for a video showing a robbery:

![](file:f5957a43-0b25-42eb-a577-c802934bbaf3)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfaa49d9c923603e5268/generate?mode=custom\&temp=0.2\&prompt=Write+a+police+report+based+on+this+video+using+the+following+example%3A%0A%0ADate%3A+11%2F01%2F2020%0ALocation%3A+San+Francisco+Police+Department%0AWitnesser’s+full+name%3A+John+Smith%0AReporter%3A+Barbara+Lim%0AOn+11%2F01%2F2020+around+5+PM%2C+I+saw+a+suspect+walking+in+a+retail+store+on+Height+Street…\&v=6791d98ecf67133816b2e01e) in your browser.

## Using different languages

This sections provides example of using different languages to analyze videos and generate text based on their content.

### Spanish

The following example summarizes a video, indicating that the response should be in Spanish. Note that the prompt is in English, and the output is in Spanish.

![](file:f1e8bc13-4025-4e97-805d-641031c63e9c)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfa949d9c923603e5267/generate?v=6785ee30027eec4fa50e80c2\&mode=custom\&temp=0.2\&prompt=Write+a+summary+in+Spanish.) in your browser.

### French

The following example summarizes the main three takeaways from this video. Note that the prompt and the output are in French.

![](file:f7f163e6-caa6-4a6a-af7a-b75f4ae9c6f1)

To see this example in the Playground, ensure you're logged in, and then open [this URL](https://playground.twelvelabs.io/indexes/6785dfa949d9c923603e5267/generate?v=6785ee313d8c70ded56640b4\&mode=custom\&temp=0.2\&prompt=Résumez+les+trois+principaux+points+à+retenir+de+cette+vidéo.) in your browser.

# Support

For support or feedback regarding Pegasus, contact [support@twelvelabs.io](mailto:support@twelvelabs.io).


# Indexes

An index is a basic unit for organizing and storing video data consisting of video embeddings and metadata. Indexes facilitate information retrieval and processing.

You can use indexes to group related videos. For example, if you want to upload multiple videos from a car race, you can create a single index and upload all the videos to it. After uploading, you can search for specific moments across all videos in that index in a single request.

When creating a new index, you must specify at least the following information:

* **Name**: Use a brief and descriptive name to facilitate future reference and management. Index names must be unique and cannot be duplicated.
* **Model configuration**: Provide a list containing the [video understanding models](/v1.3/docs/concepts/models) and the associated [model options](/v1.3/docs/concepts/modalities#model-options) you want to enable.

Note the following about model configurations:

* The model configuration determines the downstream tasks you can perform on the videos uploaded to this index. Use Pegasus to analyze videos and generate text based on their content. Use Marengo to search and create embeddings.
* The models and the model options specified when you create an index apply to all the videos you upload to that index and cannot be changed.
* To activate the thumbnail generation feature, include an array named `addons` in your request that contains the `thumbnail` value.

For a description of each field in the request and response, see the [API Reference > Create an index](/v1.3/api-reference/indexes/create) page.

## Create an index

To create a new index, you must provide the following parameters:

* `name`: A string representing the name of your new index. Choose a succinct and descriptive name for your index.
* `models`: An object specifying your model configuration. You constructed this object in the previous step.
* *(Optional)* `addons`: An array of strings specifying the add-ons you want to enable for your index. This example enables the thumbnail generation feature.


  
    The example below enables the Marengo video understanding model and the `visual` and `audio` model options:

    
      ```python Python
      models = [
              {
                "name": "marengo2.7",
                "options": ["visual", "audio"]
              }
        ]
      index = client.index.create(
        name="",
        models=models,
        addons=["thumbnail"] # Optional
      )
      print(f"A new index has been created: id={index.id} name={index.name} models={index.models}")
      ```

      ```javascript Node.js
      const models = [
        {
          name: "marengo2.7",
          options: ["visual", "audio"],
        },
      ];
      const index = await client.index.create({
        name: "",
        models: models,
        addons: ["thumbnail"], // Optional
      });
      console.log(`A new index has been created: id=${index.id} name=${index.name} models=${JSON.stringify(index.models)}`);
      ```
    
  

  
    The example below enables the Pegasus  video understanding model and the `visual` and  `audio` model options:

    
      ```python Python
      models = [
              {
                  "name": "pegasus1.2",
                  "options": ["visual", "audio"]
              }
          ]
      index = client.index.create(
        name="",
        models=models,
        addons=["thumbnail"] # Optional
      )
      print(f"A new index has been created: id={index.id} name={index.name} models={index.models}")
      ```

      ```javascript Node.js
      const models = [
        {
          name: "pegasus1.2",
          options: ["visual", "audio"],
        },
      ];
      const index = await client.index.create({
        name: "",
        models: models,
        addons: ["thumbnail"], // Optional
      });
      console.log(`A new index has been created: id=${index.id} name=${index.name} models=${JSON.stringify(index.models)}`);
      ```
    
  

  
    The example code below enables both the Marengo and Pegasus video understanding models:

    
      ```python Python
      models = [
              {
                "name": "marengo2.7",
                "options": ["visual", "audio"]
              },
              {
                  "name": "pegasus1.2",
                  "options": ["visual", "audio"]
              }
          ]
      index = client.index.create(
        name="",
        models=models,
        addons=["thumbnail"] # Optional
      )
      print(f"A new index has been created: id={index.id} name={index.name} models={index.models}")
      ```

      ```javascript Node.js
      const models = [
        {
          name: "marengo2.7",
          options: ["visual", "audio"],
        },
        {
          name: "pegasus1.2",
          options: ["visual", "audio"],
        },
      ];
      const index = await client.index.create({
        name: "",
        models: models,
        addons: [|thumbnail"], // Optional
      });
      console.log(`A new index has been created: id=${index.id} name=${index.name} models=${JSON.stringify(index.models)}`);
      ```
    
  


The response should look similar to the following one:

```
A new index has been created: id=65d345106efba5e3988d6d4b name=index-01 models=[Model(name='marengo2.7', options=['visual', 'audio'], addons=['thumbnail'])]
```

Note that the response contains, among other information, a field named `id`, representing the unique identifier of your new index.

# Related pages

* [API Reference > Manage indexes](/v1.3/api-reference/indexes)
* [Pyton SDK Reference > Manage indexes](/v1.3/sdk-reference/python/manage-indexes)
* [Node.js SDK Reference > Manage indexes](/v1.3/sdk-reference/node-js/manage-indexes)


# Tasks

Tasks handle the uploading and processing of videos. Because these operations take some time, video uploading and processing are asynchronous.

The platform utilizes two primary types of tasks:

* **Video indexing tasks**: These tasks upload and index videos, making their content accesible to the [Search](/v1.3/docs/guides/search) and [Analyze](/v1.3/docs/guides/analyze-videos) APIs.
* **Video embedding tasks**: These tasks upload videos and process videos, making their content accessible to the [Embed API](/v1.3/docs/guides/create-embeddings).

# Video indexing tasks

A video indexing task transitions through the following stages, each representing a phase in the platform's processing:

* `validating`: Ensures the video meets requirements.
* `pending`: Assigns a server to process the video.
* `queued`: Prepares the video for indexing.
* `indexing`: Converts the video into embeddings.
* `ready`: Completes the process, making the video usable.
* `failed`: Indicates an error occurred.

The example code below shows how you can track the progress of a task, checking its status until it's completed:


  ```Python Python maxLines=8
  from twelvelabs import TwelveLabs
  from twelvelabs.models.task import Task

  # Initialize the TwelveLabs client with your API key
  client = TwelveLabs(api_key="")

  # Upload a video file by creating a video indexing task 
  task = client.task.create(
      index_id="",
      file=""
  )
  print(f"Task id={task.id}")

  # Monitor the task status until the status is ready
  def on_task_update(task: Task):
      print(f"  Status={task.status}")

  task.wait_for_done(sleep_interval=5, callback=on_task_update)

  if task.status != "ready":
      raise RuntimeError(f"Indexing failed with status {task.status}")
  print(f"The unique identifier of your video is {task.video_id}.")
  ```

  ```javaScript Node.js maxLines=8
  import { TwelveLabs } from "twelvelabs-js";

  // Initialize the TwelveLabs client with your API key 
  const client = new TwelveLabs({ apiKey: "" });

  // Upload a video file by creating a video indexing task 
  const task = await client.task.create({
    indexId: "",
    file: "",
  });
  console.log(`Task id=${task.id} Video id=${task.videoId}`);

  // Monitor the task status until the status is ready 
  await task.waitForDone(5000, (task) => {
    console.log(`  Status=${task.status}`);
  });
  if (task.status !== "ready") {
    throw new Error(`Indexing failed with status ${task.status}`);
  }
  console.log(`The unique identifier of your video is ${task.videoId}`); 
  ```


# Video embedding tasks

A video embedding task progresses through the following stages:

* `processing`: The platform is creating the embeddings.
* `ready`: Processing is complete. You can now retrieve the embeddings.
* `failed`: The task could not be completed, and the embeddings haven't been created.

The example code below shows how you can track the progress of a video embedding task, checking its status until it's completed:


  ```Python Python maxLines=8
  from twelvelabs import TwelveLabs
  from typing import List
  from twelvelabs.models.embed import EmbeddingsTask, SegmentEmbedding

  # Initialize the TwelveLabs client with your API key
  client = TwelveLabs(api_key="")

  # Upload a video
  task = client.embed.task.create(model_name="Marengo-retrieval-2.7", video_file="")
  print(
      f"Created task: id={task.id} model_name={task.model_name} status={task.status}")
    
  # Monitor the status
  def on_task_update(task: EmbeddingsTask):
      print(f"  Status={task.status}")
  status = task.wait_for_done(sleep_interval=5, callback=on_task_update)
  print(f"Embedding done: {status}")
  ```

  ```Javascript Node.js maxLines=8
  import { TwelveLabs, EmbeddingsTask, SegmentEmbedding } from "twelvelabs-js";

  // Initialize the TwelveLabs client with your API key 
  const client = new TwelveLabs({ apiKey: "" });

  // Upload a video
  let task = await client.embed.task.create("Marengo-retrieval-2.7", { file: ""});
  console.log(`Created task: id=${task.id} modelName=${task.modelName} status=${task.status}`);

  // Monitor the status
  const status = await task.waitForDone(5000, (task) => { console.log(`  Status=${task.status}`);});
  console.log(`Embedding done: ${status}`);
  ```



# Modalities

Modalities represent the types of information that the platform processes and analyzes in a video. These modalities are central to both indexing and searching video content.

The platform supports the following modalities:

* **Visual**: Analyzes visual content in a video, including actions, objects, events, text (through Optical Character Recognition, or OCR), and brand logos.
* **Audio**: Analyzes audio content in a video, including ambient sounds, music, and human speech.

# Model options

When you create an index, you must specify the modalities that the platform processes. This determines what information is extracted and indexed from your videos. You can enable one or both modalities, depending on your needs.

See the [Create indexes]() page for details on selecting the desired models and model options.

# Search options

When you perform a search, you must specify the modalities that the video understanding model uses to find relevant information.

**Constraints**:

* Search options must be a subset of the model options specified when the index was created. For example, if only the visual modality was enabled during indexing, you cannot search using the audio modality.
* You can combine multiple search options with operators to broaden or narrow your search.

For examples on using search options, see the [Text queries]() page.


# Multimodal large language models

When you watch a movie, you typically use multiple senses to experience it. For example, you use your eyes to see the actors and objects on the screen and your ears to hear the dialogue and sounds. Using only one sense, you would miss essential details like body language or conversation. Furthermore, your brain processes how the visual and audio elements change over time, understanding the temporal relationship between frames to grasp the complete story. For example, you're watching a scene where a person appears to cry. If viewed in isolation, you might conclude the person is sad. However, these tears come after a sequence showing the character winning a hard-earned award. In this case, the interpretation changes: the tears are joy, not sorrow. This illustrates how the temporal aspect — the context and sequence of events leading up to the tears — is essential for correctly determining the character's emotions.

Using only one sense or just a static image, you would miss essential details like the evolution of emotions or the context of a situation. This is similar to how most language models operate - they are usually trained to understand either text, human speech, or separate images. Still, they cannot integrate multiple forms of information and understand the relationship between the visual and audio elements over time.

When a language model processes a form of information, such as a text, it generates a compact numerical representation that defines the meaning of that specific input. These numerical representations are named unimodal embeddings and take the form of real-valued vectors in a multi-dimensional space. They allow computers to perform various downstream tasks such as translation, question answering, or classification.

![](file:57c3bd7d-c198-4cd2-b3bd-9b8209645654)

In contrast, when a multimodal large language model processes a video, it captures and analyzes all the subtle cues and interactions between different modalities, including the visual expressions, body language, spoken words, and the overall context of the video. This allows the model to comprehensively understand the video and generate a multimodal embedding that represents all modalities and how they relate to one another over time. Once multimodal embeddings are created, you can use them for various downstream tasks such as visual question answering, classification, or sentiment analysis.

![](file:0a9817fd-93ba-49e8-aba5-2419efeebb57)

# TwelveLabs' multimodal large language models

TwelveLabs has developed two distinct models for different downstream tasks:

* **Video embedding model**: This model, named Marengo, converts videos into multimodal video embeddings that enable fast and scalable task execution without storing the entire video. Marengo has been trained on a vast amount of video data, and it can recognize entities, actions, patterns, movements, objects, scenes, and other elements present in videos. By integrating information from different modalities, the model can be used for several downstream tasks, such as [search](/v1.3/docs/guides/search) using natural language queries.

  Marengo uses a transformer-based architecture that processes video content through a single unified framework capable of understanding:

  * **Visual elements**: fine-grained object detection, motion dynamics, temporal relationships, and appearance features.
  * **Audio elements**: native speech understanding, non-verbal sound recognition, and music interpretation.

  ![](file:6067a257-6dbf-4e72-9aa5-7680b752613c)

* **Video language model**: This model, named Pegasus, bridges the gap between visual and textual understanding by integrating text and video data in a common embedding space. The platform uses this model for tasks that involve generating or understanding natural language in the context of video content, such as summarizing videos and answering questions.

  Pegasus uses an encoder-decoder architecture optimized for comprehensive video understanding, featuring three primary components: a video encoder, a video tokenizer, and a large language model. This architecture enables sophisticated visual and textual information processing while maintaining computational efficiency.

  ![](file:6c5c8af3-5fe9-4557-86a1-0bbf559c7d42)


# Organizations

The Organizations feature enables members to streamline workflows by sharing the following resources: indexes. videos, and S3 integrations.

Personal and organizational resources are entirely separate. The account you use (either personal or organization) determines which resources you can access and where new resources will be created.


  When using an organization account, rate limits apply in aggregate to all users in the organization. For details on the rate limits for each plan, see the [Rate limits](/docs/get-started/rate-limits) page.


Depending on your role, follow the steps in one of the guides below:


  Learn how to create and manage your organization, including user permissions, billing, and resource usage.






  Learn how to join an organization, create API keys, and use shared resources within an organization.



# Administrator's guide

The person who creates an organization automatically becomes its administrator. As an administrator, you'll follow a typical workflow to set up and manage your organization:


  
    Create your organization directly from the Playground.
  

  
    Ensure you've selected your organization account.
  

  
    Invite new members, reassign roles, and remove members.
  

  
    Review usage, update payment information, upgrade plans, and manage organization settings.
  


# Create an organization


  
    Open your web browser and visit the [Playground](http://playground.twelvelabs.io).
  

  
    Log in using your credentials. If you don't have an account, click the **Sign up** link below the **Continue** button and follow the instructions to register for a free account.
  

  
    From the sidebar, select **Settings** > **Organization**.
  

  
    Select the **Create Organization** button.
  

  
    Follow the instructions to create an organization.
  

  
    Invite team members to your organization and assign their roles.
  



  When you create an organization, it uses the Free plan. For details on the rate limits for each plan, see the [Rate limits](/docs/get-started/rate-limits) page. To upgrade, follow the steps in the [Upgrade your organization's plan](/v1.3/docs/advanced/organizations/administrators-guide#upgrade-your-organizations-plan) section.


# Switch between your accounts

{/*The steps in this section apply when managing team members or organization settings. Skip this section if you're creating your first organization.*/}

You must switch to your organization account before managing your organization or your team members. Skip these steps if you just created an organization - the platform automatically switches to your organization account. You'll need these steps when returning to the Playground later or switching between multiple organizations.

Your current account is displayed in the top-right corner of the screen. Personal accounts are marked with your initials, while organization accounts will show the initials of the respective organization.

To switch between your accounts:


  
    From any page of the [Playground](https://playground.twelvelabs.io), select your profile icon in the top-right corner.
  

  
    Select **Switch organization accounts** in the dropdown menu.
  

  
    Choose the account you want to use. A checkmark will appear next to your active account.
  


![](file:2732cc90-2bf9-4e03-8122-0dbebb189eec)

# Manage team members

This section covers all aspects of managing users within your organization, including inviting new members, reassigning roles, and removing members.

## Invite team members


  
    From the sidebar, select **Settings** > **Members**.
  

  
    Select the **Invite** button in the top-right corner.
  

  
    Enter the email addresses of the team members you wish to invite, separated by commas.
  

  
    Assign a role for the new team members.
  

  
    Select the **Add members** button. Each team member will receive an email invitation to join the organization.
  

  
    *(Optional)* You can view the status of invited members on the **Members** page. The status will show "Invited" until the member accepts the invitation.
  

  
    *(Optional)* If an invite expires or is not received, you can resend the invitation by selecting the **Resend invite** option next to the user's name.
  


## Reassign roles


  
    From the sidebar, select **Settings** > **Members**.
  

  
    Locate the user whose role you want to change.
  

  
    From the dropdown next to their current role, choose the new role.
  

  
    Confirm the role change.
  


## Remove users


  
    From the sidebar, select **Settings** > **Members**.
  

  
    Find the user you want to remove.
  

  
    From the dropdown menu next to their name, select **Remove**.
  

  
    Confirm the removal. The platform will revoke the user's API key for this organization, and they will no longer have access to the organization's resources.
  


# Manage your organization

The following sections show you how to complete common administrative tasks.

## View billing information


  
    From the sidebar, select **Settings** > **Billing & Plan**.
  

  
    Review the current usage and billing details.
  

  
    Update the payment information if needed.
  


## Monitor resource usage


  
    From the sidebar, select **Settings** > **Usage**.
  

  
    Review usage metrics and analyze consumption patterns. This helps you optimize the allocation of resources.
  


## Upgrade your organization's plan


  
    From the sidebar, select **Settings** > **Billing & plan**.
  

  
    Select the **Upgrade Plan** button and follow the instructions.
  


## Downgrade your organization's plan


  
    From the sidebar, select **Settings** > **Billing & plan**.
  

  
    Select the **Cancel Enrollment:** button and follow the instructions.
  


## Delete an organization


  Deleting an organization will permanently remove its all associated data, including indexes, videos, and S3 integrations. This action cannot be undone.



  
    Use your credentials to log into the [Playground](http://playground.twelvelabs.io). If you don't have an account, create one for free.
  

  
    From the sidebar, select **Settings** > **Profile**.
  

  
    At the bottom of the page, where you see "Would you like to delete your account?", select the **Contact us** button at the right.
  

  
    In the chat, provide the necessary details, including the name of the organization you wish to delete.
  



# User roles

Organizations support two types of user roles:

**Administrator**

* Create, update, and delete organizations
* Upgrade or downgrade the organization's plan
* Manage organization members
* View and modify billing information
* Monitor organization-wide resource usage
* Access all regular user capabilities

**Regular user**

* Generate and manage personal API keys
* Can view basic organization information
* Create, read, update, and delete shared resources
* Perform downstream tasks such as searching, creating embeddings, or generating text from video using the shared resources within the organization


# SSO configuration

The platform supports SAML and OIDC. To set up SSO for your organization, contact us at [sales@twelvelabs.io](mailto:sales@twelvelabs.io).


# User's guide

As a regular organization member, you'll follow this typical workflow to join and collaborate within your organization:


  
    Accept the email invitation from your administrator.
  

  
    Ensure you've selected your organization account.
  

  
    Generate API keys for programmatic access
  

  
    Access and manage organization-wide resources.
  


# Join an organization


  
    Check your email for an invitation.
  

  
    Select the **Accept invitation** link in the email.
  

  
    Follow the instructions.
  


# Switch between your accounts

You must switch to your organization account before creating API keys or working with resources. Skip these steps if you just joined an organization via an invitation email - the platform automatically selects your organization account on the first login. You'll need these steps when returning to the Playground later or when switching between multiple organizations.

Your current account is displayed in the top-right corner of the screen. Personal accounts are marked with your initials, while organization accounts will show the initials of the respective organization.

To switch between your accounts:


  
    From any page of the [Playground](https://playground.twelvelabs.io), select your profile icon in the top-right corner.
  

  
    Select **Switch organization accounts** in the dropdown menu.
  

  
    Choose the account you want to use. A checkmark will appear next to your active account.
  


![](file:2732cc90-2bf9-4e03-8122-0dbebb189eec)

# Create API keys


  
    From the sidebar of any page on the [Playground](https://playground.twelvelabs.io), choose **Settings** > **API Keys**.
  

  
    Select the **Create new API key** button.
  

  
    Enter a meaningful name for your key and select the **Create** button.
  

  
    Copy and save your API key securely, as it won't be shown again.
  


# Work with resources

All shared resources (indexes, videos, and S3 integrations) work exactly like those in your personal account but are automatically shared with all organization members. You can create, manage, and use these resources using the same workflows you’re familiar with from your personal account.


# Frequently asked questions

Below are answers to common questions about organizations, account management, and resource sharing. If you don't find an answer to your question here, contact our support team at [support@twelvelabs.io](mailto:support@twelvelabs.io) for further assistance.

## How is a personal account different from an organization account?

A personal account is your individual account that only you can access. Resources (such as indexes, videos, and S3 integrations) created in your personal account are private to you.

An organization account is a shared workspace where multiple team members can collaborate. Resources created within an organization account are automatically shared with all organization members. Organization accounts also provide additional features like SSO authentication, centralized billing, and usage monitoring.

## What happens to my personal account when I create an organization?

When you create an organization, a new organization account is established. Your personal account remains separate. Both accounts are associated with your email address. Your personal account and organization account each have distinct API keys.

## Can I create multiple organizations?

The platform allows you to create one organization per account.

## What plan do new organizations start on?

New organizations are created on the Free plan by default. For details on the rate limits for each plan, see the [Rate limits](/docs/get-started/rate-limits) page. To upgrade, follow the steps in the [Upgrade your organization's plan](/v1.3/docs/advanced/organizations/administrators-guide#upgrade-your-organizations-plan) section.

## How do I switch between a personal account and an organization account?

Your current account is displayed in the top-right corner of the screen. Personal accounts are marked with your initials, while organization accounts will show the initials of the respective organization.

To switch between your accounts:


  
    From any page of the [Playground](https://playground.twelvelabs.io), select your profile icon in the top-right corner.
  

  
    Select **Switch organization accounts** in the dropdown menu.
  

  
    Choose the account you want to use. A checkmark will appear next to your active account.
  


![](file:2732cc90-2bf9-4e03-8122-0dbebb189eec)

## Can I view my personal indexes in an organization account?

Indexes created in your personal account remain private and cannot be accessed through your organization account. Similarly, organization indexes cannot be accessed through your personal account. You'll need to switch between accounts to access different sets of resources.

## What happens when a user is removed from an organization?

When a user is removed from an organization, the platform revokes their API key for the organization, and they will no longer have access to the organization's resources.

## Will the S3 integrations be shared with other team members?

Yes. All S3 integrations created within an organization account are automatically shared with all members of that organization. This enables teams to collaborate using shared storage resources.

## How can I view the billing and usage for my organization?

See the [Manage your organization](/docs/get-started/organizations/administrators-guide#manage-your-organization) section for details on how you can view your billing and usage. Note that organization billing is only accessible to administrators.

## Is billing consolidated for organization accounts?

Yes. When you use an organization account, you receive a single consolidated bill for all team members instead of separate bills for each user.

## Can I create private indexes within an organization account?

Currently, we are working on this capability.

## Which rate limits apply to organization accounts?

When using an organization account, rate limits apply in aggregate to all users in the organization. For details on the rate limits for each plan, see the [Rate limits](/docs/get-started/rate-limits) page.


# Fine-tuning


  TwelveLabs currently provides fine-tuning for selected customers only. To enroll in a fine-tuning project, contact us at [sales@twelvelabs.io](mailto:sales@twelvelabs.io)


Fine-tuning is adapting a base model to a specific task or domain by training it on a smaller, domain-focused dataset. This efficiently tailors a base model to your unique requirements, improving its accuracy and performance.

Fine-tuning a base model provides the following key benefits:

* **Increased ROI**: Fine-tuning eliminates the need to train dedicated models from scratch, saving time and resources.
* **Faster time to value**: TwelveLabs' automated fine-tuning pipeline allows you to train and deploy a fine-tuned model within days instead of weeks or months.
* **Improved accuracy**: You can focus on areas where a base model falls short or incorporate your specific taxonomies, resulting in more accurate results.

Note the following about fine-tuning a base model:

* Fine-tuning is currently only available for the Marengo 2.7 model or newer versions. See the [Marengo](/v1.3/docs/concepts/models/marengo)  page for details about its capabilities.
* Small or background objects and actions might not work well with fine-tuning.
* Long-term or time-based actions, such as identifying a 5-minute run in a video, may not be suitable for fine-tuning.
* Generalization is not guaranteed, as a model fine-tuned on specific objects or actions might not distinguish similar objects or actions in different contexts. For example, a model fine-tuned to identify "writing" and "drawing" actions on paper might not accurately distinguish between these actions when performed on a digital tablet, as the input method and the appearance of the strokes may differ from the training examples.

As a best practice, test the desired taxonomies on the base model before starting a fine-tuning project. This testing helps identify performance gaps and define the project scope.


# Fine-tuning workflow

The fine-tuning process is comprised of the following steps:

1. **Prepare and upload training data**: You prepare the taxonomies the model must learn and securely share the training dataset using private links.
2. **Train the new fine-tuned model**: TwelveLabs’s automated pipeline trains the model using your dataset.
3. **Evaluate the fine-tuned model**: You evaluate the fine-tuned model quantitatively and qualitatively.
   1. **Quantitative evaluation**: Assess the model's performance using metrics such as mean average precision (mAP). TwelveLabs will provide the metrics for the private beta release.\
      **Qualitative evaluation**: The fine-tuned model will be available in the Playground or accessible programmatically through the API for qualitative assessment.
4. **Deploy the fine-tuned model**: TwelveLabs deploys the fine-tuned model to your environment.\
   Based on the evaluation results, you can retrain the model by repeating the cycle from the second step to improve its performance.


# Data preparation

Fine-tuning a base model requires both a training dataset and a validation dataset. The training dataset is used to teach the model the desired concepts and actions, while the validation dataset is used to assess the model's performance and generalization ability on unseen data.

Typically, the amount of data required for fine-tuning a base model depends on the following factors:

* **Data quality**: Higher-quality data with tighter and noise-free annotations may require fewer samples for effective fine-tuning.
* **Task complexity**: Complex concepts or actions may require more data to capture the full range of variations.

TwelveLabs recommends you provide at least ten samples for the training dataset, with an 80:20 split between the training and validation sets. The validation set should test the decision boundary well, containing diverse positive and hard negative videos and matching the distribution of practical usage.

To ensure successful fine-tuning, the following data requirements must be met:

* The training data consists of raw videos rather than clipped or edited videos as input data.
* The training data must be in a CSV file. Each line in the CSV file represents a single annotation, with the following fields separated by commas:
  * ``: The publicly accessible URL of the raw video. Note that YouTube URLs are not supported.
  * ``: The start time of the relevant segment, expressed in seconds from the beginning of the video.
  * ``: The end time of the relevant segment, expressed in seconds from the beginning of the video..
  * ``: The label or description of the concept or action occurring in the specified segment.


# Best practices

To ensure successful fine-tuning, consider the best practices in the sections below.

## Provide diverse positive and negative examples

The training algorithm uses deep learning to create a data-driven decision boundary. To establish an effective boundary in a data-driven manner, provide diverse examples:

* **Positive examples**: Instances that belong to the target taxonomy.
* **Negative examples**: Instances that don't belong to the target taxonomy but share similar visual characteristics.

By providing both types of examples, you help the base model learn to distinguish between the desired taxonomy and visually similar instances. This approach improves the precision and generalization capabilities of the model in real-world scenarios.

For example, if you want to fine-tune a base model to recognize the "timeout" gesture in American football footage, you should provide the following:

* **Positive examples**:
  * Players or coaches calling a timeout in various American football games.
  * Timeout gestures are performed by different individuals to capture variations in motion.
  * Timeout gestures from different camera angles and distances to improve the robustness of the model.
* **Negative examples**:
  * Unrelated actions that resemble the timeout gesture, such as clapping or waving

## Match data distribution to your use case

When fine-tuning a base model, the training data should be representative of the real-world scenarios in which the model will be used, including factors such as lighting conditions, camera angles, and object variations.

Align the data distribution with your practical use case to improve the accuracy and reliability of the model in the target environment.

For example, if you're fine-tuning a base model to detect product defects, consider two approaches to creating a training dataset:

* **Limited dataset**: This dataset contains only close-up images of defective products under ideal lighting conditions. As a result:
  * The model learns to identify defects based on specific, controlled conditions.
  * The model may struggle with real-world applications that involve varying distances and lighting.
* **Comprehensive dataset**:  This dataset includes images of products at various distances from the camera and under different lighting conditions. As a result:
  * The model performs better in real-world environments.
  * The model can detect defects across a range of practical scenarios.

If your training data doesn't account for real-world variations, the performance of the model may decrease when deployed in practical settings.

## Provide visually similar examples

Visual similarity occurs when the embedding vectors of a taxonomy cluster well in the embedding space. By ensuring visual similarity in your taxonomy examples, you can create more stable and effective fine-tuned models. This approach helps minimize distortions to the base model and improves the overall performance of your fine-tuned model in real-world applications.

The impact of visual dissimilarity:

* When examples of a taxonomy are visually dissimilar, bringing the embedding vectors together requires more distortion of the base model during fine-tuning.
* This approach might work for in-domain samples, but larger distortions can negatively affect the general baseline model.
* These distortions may lead to instabilities in the model's performance and cause the model to "forget" previously learned information.

To assess the visual similarity of taxonomy examples, you can use the following method:

1. Search for the same moments using the base model with your original query.
2. Perform another search using paraphrased queries that describe the visual content.
3. Compare the results of both searches.

For example,  when fine-tuning a model to detect "hurdles," you might perform the following searches:

* **Original search**: "hurdles"
* **Paraphrased search**: "man jumps over another man."

If both searches yield similar results, the examples of "hurdles" are likely visually similar and well-suited for fine-tuning.

## Tightness of the taxonomy

The quality and specificity of your training dataset significantly impact the performance of the fine-tuned model. When preparing your dataset, focus on two key aspects of taxonomy tightness: spatial and temporal.

* **Spatial tightness**: Refers to the precision and specificity of visual content within the training data. To ensure spatial tightness, follow these best practices:
  * Annotate raw videos with precise start and end timestamps for each target taxonomy occurrence.
  * Avoid including extraneous actions or objects in annotated segments.
  * Minimize noise and irrelevant information within the training data.\
    For example, annotating the "sawing" action tightly encompasses the sawing motion itself. Do not include broader scenes where sawing happens in the background or alongside other actions. By focusing on spatial tightness, the model learns to recognize and classify the target action accurately without influence from irrelevant background elements.

* **Temporal tightness**: Refers to the accuracy and precision of time-based annotations within the training dataset. To ensure temporal tightness, follow these best practices:
  * Provide tight temporal bounds for labeled actions.
  * Ensure the model associates the correct temporal context with each action.\
    For example, annotating the "spike" action in American football tightly encompasses the spiking motion. Avoid including extended scenes of post-spike celebrations. By maintaining temporal tightness, the model accurately recognizes the target action without erroneously associating it with related but distinct events.

Maintaining both spatial and temporal tightness in your dataset helps create a more accurate and reliable fine-tuned model. Focusing on these aspects ensures that your model learns to recognize and classify actions with precision and accuracy.

# Practical examples

This section illustrates critical concepts in fine-tuning, such as taxonomy tightness, data diversity, and temporal tightness. Each concept is essential for effective model training and performance.

## Temporal tightness

The videos below illustrate the importance of temporal tightness in training data. They demonstrate how the precise timing of action labeling affects model training. Accurately isolating the specific moment of an action is crucial for the model to learn the correct association between the action and its visual cues.

This video shows an example of tight temporal bounds for a "spike" action in American football. The ground truth segment accurately isolates the specific moment of the spike:

![](https://media.giphy.com/media/i6HJ5XAf7JgRuIn5aX/giphy.gif)

This video illustrates loose temporal bounds for a "spike" action. It includes the spike motion followed by unrelated hand-waving, which can confuse the model if incorrectly labeled:

![](https://media.giphy.com/media/iAW45o4SkXmfF5XeOS/giphy.gif)




## Spatial tightness

This video illustrates the importance of spatial tightness in training data. It represents a suboptimal training sample for identifying paramedic activities. While paramedics are present in the scene, they appear as small figures in the background next to an ambulance, making it difficult for the model to associate this video segment with paramedic-related categories. For effective training, the target action should be the primary focus of the sample.

![](https://media.giphy.com/media/xUA7aNa4VCrVXbekxi/giphy.gif)

## Diverse examples

The videos below illustrate the importance of diverse positive samples in training data. Insufficient diversity in training data can lead to poor performance. When training samples lack the complexity of real-world scenarios, the model may fail to generalize effectively.

The video below shows an example of insufficient diversity in training data. In it, a cyclist falls from their bike on an empty road. This isolated scenario, while clear, doesn't represent the full range of real-world conditions.

![](https://media.giphy.com/media/ANpaEQfZSyHmuVYcvL/giphy.gif)

The video below demonstrates a real-world scenario. It shows a cyclist falling during a professional cycling competition with other racers nearby, better representing the environmental complexity the model needs to handle.

![](https://media.giphy.com/media/cpm33TSSauyv6/giphy.gif)


# Dataset examples

A good fine-tuning dataset improves the performance of the model in specific domains. This section provides examples of effective datasets across various fields or industries. Each example includes the following elements:

* **Domain**: The specific field or industry.
* **Dataset content**: Types of data to include.
* **Annotation guidance**: How to label or annotate the data.
* **Specific examples**: Detailed instances within the domain.

# Healthcare

**Domain**: Different surgeries and medical procedures.\
**Dataset content**: Include diverse videos showing different surgeries and procedures.\
**Annotation guidance**: Identify key steps, label medical tools, and define basic terms.\
**Specific examples**:

* **Procedure**: Cardiac catheterization.
  * **Key steps**: Patient preparation, catheter insertion, contrast dye injection, image acquisition, etc.
  * **Annotation**: Mark procedural phases, label catheter types, and identify anatomical landmarks.
* **Procedure**: Annual health check-up.
  * **Key steps:** Check vital signs, examine body systems, discuss health history, etc.
  * **Annotation**: Mark examination steps, label medical tools, and identify patient-doctor interactions.

# Sports

**Domain**: Various sporting events and activities\
**Dataset content**: Include videos of different sports, focusing on specific actions, plays, and rules.\
**Annotation guidance:** Label techniques, mark key events, and annotate rule applications.\
**Specific examples**:

* **American football**: Stiff arm.
  * **Definition**: Defensive maneuver to fend off a tackler.\
    **Annotation**: Label the technique, ensuring spatial and temporal tightness.
* **Ice hockey**: Slapshot
  * **Definition**: A powerful shot where the player swings their stick in a wide arc before striking the puck.
  * **Annotation**: Label the technique, ensuring spatial and temporal tightness.
* **Basketball**: Pick-and-roll
  * **Definition**: A play involving a player setting a screen (pick) for a teammate and then moving toward the basket (roll).
  * **Annotation**: Label the technique, ensuring spatial and temporal tightness.

# Media & Entertainment (M\&E)

**Domain**: Television shows, movies, and live performances.\
**Dataset content**: Include a variety of videos, such as TV episodes, movie scenes, music videos, and live concert recordings.\
**Annotation Guidance**: Identify key scenes, label dialogue segments, and identify special effects.\
**Specific Examples**:

* **Scenario**: Golden Buzzer Moment from talent shows like "America's Got Talent"
  * **Key Elements**: Performance build-up, emotional peak, judges' and host's reactions, special effects.
  * **Annotation**: Label the exact moment the golden buzzer is pressed, the visual effects (confetti, lighting), and the ensuing reactions.
* **Scenario**: Dramatic TV series scene "Big Reveal Moment"
  * **Key elements**: The moment when a major plot twist or hidden truth is revealed, drastically changing the narrative or characters' understanding.
  * **Annotation**: Label the suspenseful dialogue and visual cues leading up to the reveal, and mark the exact moment the critical information is disclosed


# Webhooks

A webhook allows your application to receive real-time HTTP notifications for events. Currently, this functionality is available for the following events:

* A video indexing task has finished indexing and is ready to be searched.
* A video indexing task has failed.


  Webhooks are supported for the Search API but are unavailable for the Embed API.


# Use Webhooks to receive notifications

To receive event notifications for events in your application, use the steps in this section:

1. **Create an HTTP endpoint in your application**. The platform sends notifications to your endpoint as POST requests with a JSON payload. For details about the schema, see the [Response schema](/v1.3/docs/advanced/webhooks/response-schema) section.
2. **Register your webhook**. Once you've created an HTTP endpoint in your application, you must go to the Webhooks page and register your webhook. For instructions, see the [Register a webhook](/v1.3/docs/advanced/webhooks/manage#register-a-webhook) section.
3. **Handle requests**. Ensure that your application meets all the [requirements for processing notifications](/v1.3/docs/advanced/webhooks/requirements-for-processing-notifications).


# Manage webhooks

This topic explains how you can use the [Webhooks](https://playground.twelvelabs.io/dashboard/integrations/webhooks) section of the Dashboard to manage your webhooks and retrieve your secret keys.\
Note that, for each webhook, the platform tracks the status of your endpoint and displays it in the **Status** column. The platform uses the following statuses:

* **Not Executed**: A notification has not yet been sent to your endpoint.
* **Success**: Your endpoint responded with a `2xx` code to the latest notification.
* **Failed**: Your endpoint failed to respond with a `2xx` code to the latest notification. If you see this status, review the information in the [Requirements for processing notifications](/v1.3/docs/advanced/webhooks/requirements-for-processing-notifications) section.

# Register a webhook

Before you can receive notifications, you must first register your endpoint by following the steps below:

1. Go to the Webhooks page.
2. Select the **Add Endpoint** button.
3. In the **Add webhook** endpoint modal, enter the URL of your endpoint and a brief description.
4. When you've finished, select the **Add** button. Note that the status will show as `Not Executed` until the API sends the first request to your endpoint and receives a `2xx` response.

# Remove a webhook

When a webhook is no longer needed, you can remove it.

1. Go to the Webhooks page.
2. Find the webhook that you want to remove. Then, select the trash icon located at the far right end of the row.

# Enable or disable a webhook

This section explains how you can temporarily suspend and then reactivate notifications by disabling or enabling a webhook

1. Go to the Webhooks page.
2. Find the webhook that you want to enable or disable. Then, use the toggle under **Enabled** to enable or disable the webhook.

# Retrieve your secret key

TwelveLabs signs the notifications it sends to your endpoint by including a signature in each notification. Before you can verify signatures, you must retrieve your secret key.

1. Go to the Webhooks page.
2. Find your webhook and select the **Reveal Secret** button.
3. Select the copy icon located at the right of the secret key.


# Requirements for processing notifications

To receive notifications, ensure that the URL of your endpoint is publicly accessible. Once your application receives notifications, it must perform at least the following steps:

# 1. Validate the integrity of an event

TwelveLabs includes a unique header named `TL-Signature` in each `POST` request. The header is composed of the following parts:

* `t` : A Unix timestamp representing the time when the platform has sent the notification
* `v1` : A signature generated uniquely for each webhook event, using `HMAC` with `SHA-256`

To validate the signature and verify that the event was sent by TwelveLabs:

1. The platform uses a secret key to sign each `POST` request. You must retrieve it from the Webhooks page by following the steps in the [Retrieve your secret key](/v1.3/docs/advanced/webhooks/manage#retrieve-your-secret-key) section and then store it in your application.

2. Extract the `t` and `v1` fields. Split the `TL-Signature` header, using the comma (`,`) character as a separator.

3. Generate a signed payload. Concatenate the timestamp retrieved from the header and the raw request body, using the dot ( `.`) character as a separator.

   Note that you must use the raw request body. Do not parse the request body or else the validation will fail.\
   Example:

   ```go Go
   timestampFromHeader = "1659342128"
   body = '{"name":"test"}'
   signedPayload = timestampFromHeader + "." + body //1659342128.{"name":"test"}
   ```

4. Create a `HmacSha256` signature using the secret key you've retrieved from the [Webhooks](https://playground.twelvelabs.io/dashboard/integrations/webhooks) page for the `HMAC` key and the signed payload as payload.

   The following example shows how you can generate a signature in Go:

   ```go Go
   func generateHmacSha256Signature(secret, payload string) string {
      h := hmac.New(sha256.New, []byte(secret))
      h.Write([]byte(payload))
      return hex.EncodeToString(h.Sum(nil))
   }
   ```

5. Compare the signature computed in the previous step with the signature provided in the header. If the signatures match, it means that TwelveLabs is the sender of the notification.

6. *(Optional)* As an additional security measure, you can also compare the timestamp received in the `t` field to the current time. TwelveLabs suggest you consider valid all the requests for which the time difference is less than five minutes.

# 2. Respond with a 2xx status code

Your webhook must return a `2xx` status code for each notification it receives from the platform, indicating successful delivery of the notification. The platform will treat a different response code as a failure, and the status will show as `Failed`.


# Response schema

# Header

The header contains a field named `TL-Signature`. This allows you to verify that the event was sent by TwelveLabs. The `TL-Signature` field is composed of two parts:

* `t` : A Unix timestamp representing the time when the platform has sent the notification.
* `v1` : A signature generated uniquely for each webhook event, using `HMAC` with `SHA-256`. For instructions on how you can verify the signature, see the [Validate the integrity of an event](/v1.3/docs/advanced/webhooks/requirements-for-processing-notifications#1-validate-the-integrity-of-an-event) section.

# Response schema

Responses are in JSON format and contain the following fields:

* `id`: A string representing the unique identifier of the notification.
* `created_at`: A string indicating the date and time, in the RFC 3339 format, that the platform has sent the notification.
* `type`: A string indicating the type of event. It can take one of the following values:
  * `index.task.ready`: Specifies that your video is ready to be searched
  * `index.task.failed`: Specifies that the video indexing task has failed.
* `data`: An object that contains the following information:
  * `id`: A string representing the unique identifier of the video indexing task.
  * `metadata`: An object containing metadata about the video such as its duration.
  * `status`: A string representing the status of the video indexing task. For details about the possible statuses, see the [Task object](/v1.3/api-reference/tasks/the-task-object) page.
  * `models`: A list of objects containing the [video understanding models](/v1.3/docs/concepts/models) and the associated [model options](/v1.3/docs/concepts/modalities#model-options) used to index this video.
  * `tags`: An array of strings representing the tags associated with this video indexing task.

# Example

```
POST /user-webhook-endpoint HTTP/1.1
TL-Signature: t=1659342128,v1=0f596565898448fe00d22c52fcaddffb1d1054da3b1d47268b99b6041c79aa29

{
  "id": "whe_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
  "created_at": "2022-08-01T17:22:18.372553+09:00",
  "type": "index.task.ready",
  "data": {
    "id": "xxxxxxxxxxxxxxxxxxxxxxx",
    "metadata": { "duration": 30 },
    "status": "ready",
    "models": [
      {
        "name": "marengo2.7", "options": ["visual", "audio"]
      },
      {
        "name": "pegasus1.2", "options": ["visual", "audio"]
      }      

    ],
    "tags": []
  }
}

```


# Metadata

Metadata includes technical and contextual information about each video uploaded to the platform. By default, all the videos have the following metadata associated with them:

* `duration`: The duration of the video, expressed in seconds
* `filename`: The filename
* `fps`: The number of frames per second
* `height`: The height of the video
* `size`: The size of the video file, expressed in KB
* `video_title`: The title of the video (defaults to the file name)
* `width`: The width of the video

Custom metadata allows you to add more data to your videos, providing more detailed, specialized, or context-specific information.

The values you provide must be of the following types: `string`, `integer`, `float` or `boolean`. If you want to store other types of data such as objects or arrays, you must convert your data into string values.

# Provide custom metadata

Once the platform has finished indexing your videos, you can provide custom metadata by invoking the `update` method of the `index.video` object with the following parameters:

* `index_id`:  A string representing the unique identifier of the index containing the video for which you want to provide custom metadata.
* `id`:  A string representing the unique identifier of the video
* `user_metadata`: A dictionary containing your custom metadata.  In this example, the `metadata` dictionary has four keys: `views`, `downloads`, `language` and `country`. The `views` and `downloads` keys are integers, and the`creation_date` and `country` keys  are strings.


  ```python Python
  client.index.video.update(
      index_id="",
      id="",
      user_metadata={
          "views": 12000,
          "downloads": 40000,
          "language": "en-us",
          "country": "USA"
      }
  )
  ```

  ```javascript Node.js
  client.index.video.update(
    "",
    "", 
    {
      userMetadata: {
        views: 12000,
        downloads: 40000,
        language: "en-us",
        country: "USA",
      },
    },
  );
  ```


# Filter on custom metadata

Once you've added custom metadata to your videos, you can use it to filter your search results.

The example code below filters on a custom field named `views` of type `integer`. The platform will return only the results found in the videos for which the value of the `views` field equals `120000`.


  ```python Python maxLines=8
  search_results = client.search.query(
    index_id="",
    query_text= "",
    options=["audio"],
    filter = {
      "views": 120000
    }
  )
  ```

  ```javascript Node.js maxLines=8
  let searchResults = await client.search.query({
    indexId: "",
    queryText: "",
    options: ["audio"],
    filter: {
      views: 120000,
    },
  });
  ```


For more details on filtering search results, see the [Filtering](/v1.3/docs/guides/search/filtering) page.


# Cloud-to-cloud integrations


  Cloud-to-cloud integrations require a paid plan. If you're on the Free plan, you can find information on upgrading your plan  in the  [Upgrade your plan](/v1.3/docs/get-started/manage-your-plan#upgrade-your-plan) section.


Cloud-to-cloud integrations allow you to upload multiple videos in a single API call. This feature is currently supported for the `us-west-2` region of AWS S3. If your data is in other regions or with other cloud providers, contact us at [sales@twelvelabs.io](mailto:sales@twelvelabs.io).

Note the following about cloud-to-cloud integrations:

* The import process is asynchronous. Your videos will be uploaded and indexed after you initiate an import.
* You can perform downstream tasks on your videos only after the platform has finished uploading and indexing them.
* Only one import job can run at a time. To start a new import, wait for the current job to complete. Use the [`GET`](/v1.3//api-reference/tasks/cloud-to-cloud-integrations/get-logs) method of the `/tasks/transfers/import/{integration-id}/logs` endpoint to retrieve a list of your import jobs, including their creation time, completion time, and processing status for each video file.

The steps for importing videos are as follows:


  

  


# Set up an integration

Setting up an integration grants TwelveLabs access to read files in your S3 bucket. The platform assigns a unique identifier to each integration. Before you can use this feature, you must set up an integration by following the steps in this section.

## Prerequisites

Ensure the following prerequisites are met:

* Your bucket must be in the `us-west-2` region.
* You have the following information on hand:
  * Your AWS account ID. For instructions, see the Finding Your AWS Account ID section of the AWS documentation.
  * The name of your bucket.

## Procedure


  
    Go to the Integrations page.
  

  
    Select  **Add Integration** > **AWS S3**:
    ![](file:8adfd082-377c-4e5b-b493-b853f89456ba)
  

  
    In the **Integrate** AWS S3 modal, enter your AWS ID and bucket name. Then, select the **Next** button:
    ![](file:7bce15c0-048a-4dbf-9604-aec8418cc600)
  


### Create an AWS policy


  
    Open the IAM Dashboard  page in a new window or tab.
  

  
    From the sidebar, choose **Policies**. Then, select the **Create Policy** button:
    ![](file:2858c120-0424-4a7c-8b30-467e9004082d)
  

  
    Select the **JSON** tab:
    ![](file:134dcba2-4845-4bfa-8220-8346e7e485fd)
  

  
    Move to the **TwelveLabs Dashboard** page, and copy the JSON snippet under **Step 1 : Create Policy**:
    ![](file:8b77458a-0744-4265-b26d-247f0a246ad6)
  

  
    Move to the **IAM Dashboard** page, and replace the content of the **JSON** tab with the snippet you've copied in the previous step. Then, select the **Next** button:
    ![](file:69a4f73d-f785-472f-bad0-e11ccbeb02fc)
  

  
    On the **Review Policy** page, set the name of the policy to "TwelvelabsIntegrationPolicy" and enter a brief description. Then, select the **Create Policy** button:
    ![](file:103a463f-bdfa-41c3-a562-dcb2b1e29dc3)

    
      Do not change the name of the policy. If you rename the policy, the integration will not work.
    
  

  
    On the **Policies** page, make sure that the system displays your new policy:
    ![](file:ba9343ea-32a2-4100-8b0e-027b90be0ad9)
    If the system does not display your policy, review the steps in this section, making sure all the information you entered is correct.
  


### Create an AWS role


  
    From the sidebar, choose **Roles**. Then, select the **Create Role** button:
    ![](file:9b1b145d-d514-4c57-99ee-ddbca8cc8ca8)
  

  
    Under **Trusted Entity Type**, select **Custom Trust Policy**:
    ![](file:8ff46ef4-a492-4c89-85bb-a1f73ac27b9e)
  

  
    Move to the **TwelveLabs Integrations** page, and copy the JSON snippet under **Step 2 : Create Role**:
    ![](file:e02330cf-e88a-4320-b2e6-f4e199e74fcd)
  

  
    4. Move to the **IAM Dashboard** page, and replace the content of the **Custom trust policy** box with the snippet you've copied in the previous step. Then, select the **Next** button:
       ![](file:804c0fd5-1b3d-454d-9560-1f00d1d360cb)
  

  
    On the **Add Permissions** page, select the policy you've created in the previous section. Then, select the **Next** button at the bottom-right corner of the page:
    ![](file:59d5be95-6bf2-4d2a-b8bf-6df6ebf12db8)
  

  
    On the **Name, Review, and Create** page, set the name of the role to "TwelvelabsIntegrationRole" and enter a brief description. Then, select the **Create Role** button:
    ![](file:a32aecb3-861a-4ba4-9926-589192bf4d6f)

    
      Do not change the name of the role. If you rename the role, the integration will not work.
    
  

  
    On the **Roles** page, make sure that the system displays your new role:
    ![](file:92a05f49-4f57-4a78-9dac-1a02017eef58)

    If the system does not display your role, review the steps in this section, making sure all the information you entered is correct.
  


### Verify your integration configuration


  
    Move to the **TwelveLabs Integrations** page. Under **Step 3 : Check Verification**, select the **Verify My Account** button:
    ![](file:75747042-987e-4cc5-8e7f-73fade03ccbe)

    If everything went well, you should see a message saying "Verification successful!":
    ![](file:665c36a0-485e-4b15-9f9e-1732c4c202e5)
  

  
    Select the **Next** button.
  

  
    On the **Integrations** page, make sure that the status of your new integration shows as **Active**.  Note that each integration has a unique identifier that you must provide to import videos.
    ![](file:4fec06b5-d493-438b-b239-a584058329ae)

    When the status of your integration shows as **Active**, the platform can access the videos in your S3 bucket. If the status does not show as **Active**,  review the steps in this section, making sure all the information you entered is correct.
  


# Import videos

You can import videos in two ways: 

* **Programmatically**: Follow the steps in this guide.
* **Using the Playground**: Go to the [Integrations](https://playground.twelvelabs.io/dashboard/integrations)  page, choose your integration, and select the **Import Videos** button.

## Prerequisites

Before you import videos, ensure the following prerequisites are met:

* You’ve already [created and index](/v1.3/docs/concepts/indexes#create-an-index).
* You have an AWS S3 bucket containing the videos you want to import.
* You've already set up an integration. If you didn't set up an integration yet, follow the steps in the [Set up an integration](#set-up-an-integration)  section.
* Retrieve the unique identifier of your integration from the [Integrations](https://playground.twelvelabs.io/dashboard/integrations) page:
  ![](file:47379106-b403-43d1-8fc6-241a5b6f3aa9)
* The videos you wish to use must meet the following requirements:
  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.

  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.

  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the FFmpeg Formats Documentation page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  * **Duration**: Must be between 4 seconds and 2 hours (7,200s).

  * **File size**:  Must not exceed 2 GB.\
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

## Procedure


  
    To initiate an import, invoke the `import_video` method, passing the  unique identifier of your integration and the unique identifier of the target index as parameters:

    
      ```python Python
      from twelvelabs import TwelveLabs

      client = TwelveLabs(api_key="")

      res = client.task.transfers.import_videos(
      "",
      "",
      )
      for video in res.videos:
      print(f"video: {video.video_id} {video.filename}")
      if res.failed_files:
      for failed_file in res.failed_files:
          print(f"failed_file: {failed_file.filename} {failed_file.error_message}")
      ```

      ```javascript Node.js
      const client = new TwelveLabs({ apiKey:"" });

      const res = await client.task.transfers.importVideos({ "", "" });
      res.videos.forEach((v) => {
      console.log(`video: ${v.videoId} ${v.filename}`);
      });
      res.failedFiles?.forEach((f) => {
      console.log(`failed file: ${f.filename} ${f.errorMessage}`);
      });
      ```
    

    Note the following about the response:

    * It consists of two lists:
      * `videos`: Videos that will be imported.
      * `failed_files`: Videos that failed to import, typically due to unmet [prerequisites](#prerequisites-1).
    * The platform returns this information immediately after initiating the import process, before the actual upload and indexing begin.
      For details about each field in the request and response, see the [Import videos](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/create) page.
  

  
    *(Optional)*  Monitor the status of your  import by invoking the `import_status` method, passing the  unique identifier of your integration and the unique identifier of the target index as parameters:

    
      ```python Python
      status = client.task.transfers.import_status("", "")
      for ready in status.ready:
      print(f"ready: {ready.video_id} {ready.filename} {ready.created_at}")
      for failed in status.failed:
      print(f"failed: {failed.filename} {failed.error_message}")
      ```

      ```javascript Node.js
      const status = await client.task.transfers.importStatus("", "");
      status.ready.forEach((v) => {
      console.log(`ready: ${v.videoId} ${v.filename} ${v.createdAt}`);
      });
      status.failed.forEach((f) => {
      console.log(`failed: ${f.filename} ${f.errorMessage}`);
      });
      ```
    

    Note the following about the response:

    * It consists of lists of videos grouped by their status. For details about each status, see the [Task object](/v1.3/api-reference/tasks/the-task-object) page.
    * Each video entry includes:
      * `video_id`: The unique identifier of the video. This identifier serves a dual purpose:
      * It identifies the video itself.
      * It identifies the associated video indexing task.
      * `filename`: The original filename of the video.
      * `created_at`: The date and time when the video was added to the import process.

    For details about each field in the request and response, see the [Retrieve import status](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/get-status) page.
  


## Duplicate videos

By default, the platform checks for duplicate files using hashes within the target index and will not upload the same video to the same index twice. However, the same video can exist in multiple indexes.

To bypass duplicate checking entirely and import duplicate videos into the same index, set the value of the `incremental_import` parameter to `false`, as shown in the example code below:


  ```python Python
  res = client.task.transfers.import_videos(
     integration_id="",
     index_id="",
     incremental_import=False  # Allows duplicate videos in the same index
  )
  ```

  ```javascript Node.js
  const resp = await client.task.transfers.importVideos({
      integrationId: "",
      indexId: "",
      incrementalImport: false  // Allows duplicate videos in the same index
  });
  ```


## Troubleshooting

To view a history of the import operations for a specific integration, invoke the `import_log` method, passing the unique identifier of your integration as a parameter:


  ```python Python
  logs = client.task.transfers.import_logs("")
  for log in logs:
      print(
          f"index_id={log.index_id} index_name={log.index_name} created_at={log.created_at} ended_at={log.ended_at} video_status={log.video_status}"
      )
      if log.failed_files:
          for failed_file in log.failed_files:
              print(
                  f"failed_file: {failed_file.filename} {failed_file.error_message}"
              )
  ```

  ```javascript Node.js
  const logs = await client.task.transfers.importLogs("");
  logs.forEach((l) => {
    console.log(
      `indexId: ${l.indexId} indexName: ${l.indexName} createdAt: ${l.createdAt} endedAt: ${l.endedAt} videoStatus: ${l.videoStatus}`,
    );
    l.failedFiles?.forEach((f) => {
      console.log(`failed file: ${f.filename} ${f.errorMessage}`);
    });
  });
  ```


The response consists of a chronological list of import operations for the specified integration. The list is sorted by creation date, with the oldest imports first. Each item in the list contains:

* The number of videos in each status
* Detailed error information for failed uploads, including filenames and error messages.

For details about each field in the request and response, see the [Retrieve import logs](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/get-logs) page.


# Platform overview

The TwelveLabs Video Understanding Platform equips developers with the following key capabilities:

* **Deep semantic search**: Find the exact moment you need within your videos using natural language queries instead of tags or metadata.
* **Dynamic video-to-text generation:** Capture the essence of your videos into concise summaries or custom reports. Additionally, you can provide a prompt detailing the content and desired output format, such as a police report, to tailor the results to your needs.
* **Intuitive integration**: Embed a state-of-the-art multimodal foundation model for video understanding into your application in just a few API calls.
* **Rapid result retrieval**: Receive your results within seconds.
* **Scalability**: Our cloud-native distributed infrastructure seamlessly processes thousands of concurrent requests.

# TwelveLabs’ Advantages

The table below provides a basic comparison between TwelveLabs Video Understanding Platform and other video AI solutions:

![](file:b1749867-f9d0-4832-be0b-32a9a55a2555)

* **Simplified API integration:** Perform a rich set of video understanding tasks with just a few API calls. This allows you to focus on building your application rather than aggregating data from separate image and speech APIs or managing multiple data sources.
* **Natural language use**: Tap into the model's capabilities using everyday language to write queries or prompts. This method is more effective, intuitive, flexible, and accurate than using solely rules, tags, or keywords.
* **Image-to-video search**: Perform searches using images as queries and find videos semantically similar to the provided images. This addresses the challenges you face when the existing reverse image search tools yield inconsistent results or when describing the desired results using text is challenging.
* **Multimodal approach**:  The platform adopts a video-first, multimodal approach, surpassing traditional unimodal models that depend exclusively on text or images, providing a comprehensive understanding of your videos.
* **One-time video indexing for multiple tasks**: Index your videos once and create contextual video embeddings that encapsulate semantics for scaling and repurposing, allowing you to search your videos swiftly.
* **Flexible deployment**: The platform can adapt to varied business needs, with deployment options spanning on-premise, hybrid, or cloud-based environments.
* **Fine-tuning capabilities**: Though our state-of-the-art foundation model for video understanding already yields highly accurate results, we can provide fine-tuning capabilities to help you get more out of the models and achieve better results with only a few examples.

For details on fine-tuning the models or different deployment options, please contact us at [sales@twelvelabs.io](mailto:sales@twelvelabs.io).

# Architecture

The following diagram illustrates the architecture of the TwelveLabs Video Understanding Platform and how different parts interact:

![](file:b04beca7-bfa1-4aa0-a9bb-9ec414547da1)

## Indexes

An index is a basic unit for organizing and storing video data consisting of video embeddings and metadata. Indexes facilitate information retrieval and processing.

## Video understanding models

A video understanding model consists of a family of deep neural networks built on top of our multimodal foundation model for video understanding, offering search and summarization capabilities. For each index, you must configure the models you want to enable. See the [Video understanding models](/v1.3/docs/concepts/models) page for more details about the available models and their capabilities.

## Model options

The model options define the types of information that a specific model will process. Currently, the platform provides the following model options: visual and audio. For more details, see the [Model options](/v1.3/docs/concepts/modalities#model-options) page.

## Query/Prompt Processing Engine

This component processes the following user inputs and returns the corresponding results to your application:

* Search queries
* Prompts for analyzing videos and generating text based on their content


# Playground

The Playground is a sandbox environment specifically designed to provide you with a hands-on experience in exploring and testing the various features of the TwelveLabs Video Understanding Platform. This intuitive web application is a powerful tool for product managers, developers, and AI enthusiasts to familiarize themselves with the platform's capabilities and experiment with its functionalities.

# The Overview page

When you open the Playground, by default, you see the Overview page. From here, you can follow the basic steps for using the Playground described in the [Get started with the Playground](#get-started-with-the-playground) section below or try out the predefined set of [examples](/docs/resources/playground/examples) to search and generate text based on your videos.

![](file:fede6dbb-57a0-4ac8-afe4-8cd7a6de001d)

1. **Upload videos pane**: Allows you to upload videos quickly.
2. **Endpoints pane**: Provides quick access to the [Search](/docs/resources/playground/search), [Analyze](/docs/resources/playground/analyze-videos), and [Embed](/docs/resources/playground/visualize-embeddings) pages.
3. **Sample indexes pane**:  Allows you to experiment without uploading and indexing your own videos.
4. **Indexes pane**:  Allows you to create a new index and shows the most recently used indexes. For each index displayed, you can upload videos and directly access the [Search](/docs/resources/playground/search), [Analyze](/docs/resources/playground/analyze-videos), and [Embed](/docs/resources/playground/visualize-embeddings)  pages.
5. **Invite button**: Invite your team members to share resources with them. For instructions on setting up and managing this feature, see the [Organizations](/v1.3/docs/advanced/organizations) section..
6. **User profile menu**: Manage your account settings, such as API key, integrations, and billing. You can also log out of the Playground from this menu.

# Get started with the Playground

{/*
Before you start using the Playground, ensure that the following prerequisites are met:

- To use the platform, you need an API key:
     
         
             If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
         
         
             Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
         
         
             Select the **Copy** icon next to your key.
         
     
- You're familiar with the concepts that are described on the [Platform overview](/docs/resources/platform-overview) page.
- If you're a member of an organization, Select **Switch Organization accounts** and choose the account you wish to use.
*/}


  If you're a member of an organization, ensure you select the account you wish to use. For details, see the [Switch between you accounts](/docs/advanced/organizations/frequently-asked-questions#how-do-i-switch-between-a-personal-account-and-an-organization-account) section.


Consider the following basic steps for getting started with the Playground:

1. [Create an index](/docs/resources/playground/manage-indexes#create-an-index)
2. [Upload videos](/docs/resources/playground/upload-and-manage-videos#upload-a-video)
3. Perform one of the following tasks:

* [Search](/docs/resources/playground/search)
* [Analyze videos](/docs/resources/playground/analyze-videos)
* [Visualize embeddings](/docs/resources/playground/visualize-embeddings)


# Manage indexes

An index is a basic unit for organizing and storing your video data (video embeddings and metadata). Indexes facilitate information retrieval and processing.

You can use indexes to group related videos. For example, if you want to upload multiple videos from a car race, you can create a single index and upload all the videos to it.

You can manage your indexes from the Indexes page.

# User interface

The following image shows the user interface and the key elements:



1. **Create an Index**: Opens a modal to create a new index.

2. **Index information**: Displays the following information about each index:

   * A thumbnail of the first video.
   * The three dots button, allowing you to rename or remove this index.
   * The number of videos in this index.
   * *(Optional)* When videos are being indexed, the Playground displays the progress of the indexing process.
   * Index name.
   * The [video understanding model](/docs/concepts/models) that is enabled for this index:

     * **Marengo**, for searching and creating embeddings, is represented by the following icon:
       

     * **Pegasus**, for generating text from videos, is represented by the following icon:
       

3. **Quick actions**: Provides quick access to commonly used actions you can perform on an index.

The following sections describe the most common actions you can perform on the Indexes page.

# Create an index



1. From the Indexes page, select the **Create an Index** tile.

2. Enter a descriptive name for your index.

3. Choose a [video understanding model](/docs/concepts/models):
   * Use Marengo to search and create embeddings.
   * Use Pegasus to analyze videos and generate text based on their content.

4. Choose the [model options](/docs/concepts/modalities#model-options) you want to enable. The model options determine how videos in the index are processed. Note that once set, these settings cannot be changed.

5. When you've finished, select the **Next** button.

6. *(Optional)* Follow the instructions to upload videos to your index.

# Rename an index

1. From the Indexes page, find the index that you want to rename, and select the three dots button located at the top-right corner of the tile.
2. Select **Rename Index**.
3. Enter the desired name.
4. When you've finished, select the **Save** button.

# Delete an index


  * Deleting an index removes all the information about it.
  * This action cannot be undone.


1. From the Indexes page, find the index that you want to delete, and select the three dots button located at the top-right corner of the tile.
2. Select **Delete  Index**.
3. Review the information displayed in the modal, and select the **Delete Index** button.


# Upload and manage videos

When you select an index on the Indexes page, the index details page is displayed. From here, you can upload and manage videos.

# User interface

The following image shows the user interface and the key elements:



1. **Index information**: Displays the following information about each index:
   1. Index name and its unique identifier
   2. The [video understanding models](/docs/concepts/models) and [model options](/docs/concepts/modalities#model-options) that are enabled for this index.
2. **Quick actions**: Provides quick access to commonly used actions that you can perform on an index.
3. **Sort by**: Use this dropdown to sort the list of videos based on the date they were uploaded, duration, name, or resolution.
4. **Multiselect**: Use this button to select multiple videos and then delete them.

The following sections describe the most common actions you can perform on the index details page.

# Upload a video


  * Before uploading a video, ensure it meets [the requirements](/api-reference/tasks/create#video-requirements).
  * The ability to upload videos from YouTube is no longer supported.
  * You can upload files from direct URLs using the API. For details see the [Create a video indexing task](/api-reference/tasks/create) page.


There are two ways in which you can upload videos.

**From the  Overview page**:

1. From the **Select index** drop-down, choose the index to which you want to upload your video.

2. Select the videos you wish to upload.

3. *(Optional)* Review the list of videos to be uploaded. To remove a video, select the `X` icon located at the right.

4. Select the **Upload** button.

5. *(Optional)* Monitor the progress of the indexing process. After uploading the video, the platform must index it before you can perform any downstream tasks. Wait until you see the video with a thumbnail and duration.

**From the Indexes page**:

1. Choose the index to which you want to upload your video.

2. On the index details page, select the **Upload Video** button.

3. Select the videos you wish to upload.

4. *(Optional)* Review the list of videos to be uploaded. To remove a video, select the `X` icon located at the right.

5. Select the **Upload** button.

6. *(Optional)* Monitor the progress of the indexing process. After uploading the video, the platform must index it before you can perform any downstream tasks. Wait until you see the video with a thumbnail and duration.

# Delete videos


  This action cannot be undone.


There are two ways in which you can delete videos.

**Delete a single video**:

1. From the Indexes page, select the index that contains the video you want to delete.
2. On the index details page, select the video that you want to delete.
3. In the modal, select the trash icon located at the bottom right corner of the modal.

**Delete multiple videos**:

1. From the Indexes page, select the index that contains the video you want to delete.
2. From the index details page, select the **Multiselect** button.
3. Choose the videos that you want to delete.
4. Select the **Delete** button.




# Cloud-to-cloud integrations

You can also upload multiple videos in a single API call. This feature is currently supported for the `us-west-2 region` of AWS S3. For details, see the [Cloud-to-cloud integrations](/docs/advanced/cloud-to-cloud-integrations) page.


# Search

The Playground allows you to search using natural language queries. A natural language query refers to the ability to enter search requests using everyday language, instead of using specific keywords. For example, you can enter a query like "Interviews with scientists discussing climate change." The platform analyzes the query, identifies the key elements, and searches for video clips that match it. Then, the Playground shows the relevant moments from your videos that match the query you entered, making the search process intuitive and user-friendly.

Follow the steps in this guide to find specific moments in your videos using natural language queries.

1. From the Indexes page, find the index containing the videos to be searched and choose **Select action** > **Search**:

   ![](file:37d038aa-73e4-4189-8f0b-b10270a93e68)

2. Under **Select search options**, use the checkboxes to choose the [search options](/docs/concepts/modalities#search-options):

   ![](file:804cf1b8-944f-4828-b951-582fc0d857e2)

3. You can search using either images or text.

   * **Image search**: Select  **Search by Image** and either upload an image or provide a publicly accessible URL:

     ![](file:d34022f4-30c1-4224-9c0a-85cc47bfcdb3)

   * **Text search**: Enter a natural language query in the search box and press the **Enter** key:

     ![](file:f8d0a3b1-1b0a-47e6-8b3d-399cd3503858)

4. *(Optional)* The Playground allows you to group your search results by video or view them in a flat list, with the default setting presenting ungrouped results sorted by confidence level in descending order.

   Select the **View by clip** button to see results as a flat list:

   ![](file:9053f8db-9bfc-48a7-95a5-d5182860a5a0)

   Select the **View by video** button to organize results into groups representing individual videos:

   ![](file:04c9a298-80e5-42b6-a44a-c7e69a40de99)

5. *(Optional)* To customize the search results to your specific needs, ensure that the **Advanced parameters** section is expanded and set the following parameters:

   * **Filter by minimum confidence level**: This drop-down customizes the search results according to your desired trade-off between precision and recall.
   * **Adjust confidence level**: This slider configures the strictness of the thresholds for assigning the high, medium, or low confidence levels to search results. For example, if you move the slider to the left (less strict), the thresholds become more relaxed. This means that more search results will be classified as having high, medium, or low confidence levels. You can use this setting to include a broader range of potentially relevant video clips, even if some results might be less precise.
   * **Show confidence score**: When you turn this toggle on, the Playground will also display the confidence score of each matching video fragment as a numeric value.

     ![](file:b60fdef7-8887-4b6c-a1e1-9217eec37662)

6. *(Optional)* Use the **View code** button to view the code snippet that the platform used to perform this request. You can copy and paste it into your application.

   ![](file:8951376e-034f-4baf-a219-a1450ae2cad2)


# Analyze videos

The platform analyzes videos to generate text based on their content using a multimodal approach. This method analyzes the visuals, sounds, spoken words, and relationships between them. As a result, it provides a comprehensive understanding of your videos, capturing nuances that might be overlooked when using an unimodal interpretation.

The platform generates the following types of text:

* **Topics and hashtags**: Represent a swift breakdown of the essence of a video.
* **Summaries**: Encapsulate the key points of a video, presenting the most important information clearly and concisely.
* **Highlights**: List the key events in order. Unlike chapters, they spotlight primary topics.
* **Chapters**:  A chapter in a video typically focuses on a particular topic or theme. The platform chronologically lists all the chapters in your video for a thorough content breakdown.
* **Open-ended text (your own prompt)**: Custom outputs based on your prompts, including, but not limited to, tables of content, action items, memos, reports, marketing copy, and comprehensive analyses.


  - This feature is available only for the indexes that have the Pegasus video understanding engine enabled. See the [Create an index](/docs/resources/playground/manage-indexes#create-an-index) section for details.
  - Your prompts can be instructive or descriptive, or you can also phrase them as questions. For guidance on creating effective prompts, see the [Prompt engineering](/docs/guides/analyze-videos/prompt-engineering) page.
  - The maximum length of a prompt is 2,000 tokens.


# Procedure

Follow the steps in this guide to generate text based on the content of your videos:

1. From the sidebar, select **Analyze**.

{/*   ![](/docs/assets/1.3/playground-generate-select-generate.png)*/}

2. Choose the **Select a video** button:

   ![](file:708dd47a-45df-4ee0-af25-8bd68d74b082)

3. Choose an index and the video for which you want to generate text:

   ![](file:42f8b49c-97e6-418a-8e28-94af0e3ce5b5)

## For predefined templates

1. Select one of the prompt options below the video to generate the corresponding type of text you wish to generate. For summaries, chapters, and highlights, you can provide a custom prompt that guides the model on how to generate the output.

   ![](file:7c33a4c2-199b-4ca0-9cb0-9b9726126f69)

   The Playground displays the generated text in the output panel. For chapters and highlights, you can select the timecodes to play the exact moments in the video player.

   ![](file:f7930802-bd81-444d-972e-6efdb91c0850)
2. *(Optional)* You can select the **View code** button to view the code snippet the platform used to perform this request. You can copy and paste it into your application.

   ![](file:98a3642e-e670-41ae-ba9d-cbea6f281b5b)




## For open-ended text

1. In the **Input prompt** section, provide clear instructions for the desired output:

   ![](file:3b82289c-b315-4e62-bf5c-bf8763255bf3)

   Prompt examples:

   * Generate a company-wide memo based on the announcements made in the video.
   * Identify key visual elements, scene changes, and events in the video.
   * Generate three taglines for an email marketing campaign based on the video.

2. *(Optional)* Temperature is a configurable parameter that controls the randomness of the text output generated by the model. A higher value generates more creative text, while a lower value results in more deterministic text output. Use the **Temperature** slider to tailor the behavior of the model to your requirements.

   ![](file:a6ec6647-cfd0-42af-9897-006a45d8b284)

3. When you've finished, press `Command+Enter` or select the button with an upward-pointing arrow icon:

   ![](file:b98c6298-2e0f-4b27-8d54-3ad5312e328b)

   The Playground displays the generated text in the output panel:

   ![](file:4836f618-9578-4f7e-9ac5-9e820c2a62e9)

4. *(Optional)* You can select the **View code** button to view the code snippet the platform used to perform this request. You can copy and paste it into your application.

   ![](file:7548feab-5d16-4c4b-a245-0fa0bbc9c059)


# Visualize embeddings

Use the [Embed](https://playground.twelvelabs.io/indexes/65eff5ce6dc02a0c6004a05a/embed) page to analyze semantic relationships between your video content and text queries. The visualization displays up to 500 clips from your selected index as interactive data points in a two-dimensional space. Dots that cluster together indicate content with similar characteristics. Each dot in the visualization has a distinct color:

* **Light green**: Represents video clips.
* **Dark green**: Indicates the currently selected video clip.
* **Orange**: Represents your text query

When you enter a text query, its position relative to video clips indicates semantic similarity - clips positioned closer to your query are more semantically similar to the text you entered.

# User interface

The following image shows the user interface and the key elements you can use to interact with the visualization:

![](file:0b56ad55-7c05-4b59-a6bc-ed10ba4b3b64)

1. **Choose index**: Use this dropdown to select the index you want to visualize.  For best performance, choose an index with diverse videos.
2. **Visualization area**: Displays dots representing video clips and the embedding for your text query.
3. **Create a text embedding**: Use this text field to enter your text query. Note that your text cannot exceed 77 tokens. The token count is displayed at the bottom.
4. **View controls**: Use these buttons to adjust the visualization:
   1. Zoom in
   2. Zoom out
   3. Fit: Reset the view to show all points
5. **Legend**: Shows what the different colors represent.
6. **Selected embedding**: Shows detailed information about the selected embedding. including a video player.
7. **View code**: Displays the code the platform used to create the selected embedding. You can copy and paste it into your application.

# Typical workflow

Follow the steps below to explore semantic relationships in your video content:

1. Select an index to visualize its video clips.
2. Enter a text query to find relevant video segments.
3. Explore relationships by:
   * Observing clusters of similar content
   * Adjusting the view to focus on specific areas using the buttons in the **View Controls** area or your mouse.
   * Selecting dots to preview video segments
4. View the code to implement similar functionality in your application.


# Examples

The Examples page enables you to experiment with a set of examples included in the Playground, eliminating the need to upload and index your own videos. Note that all the examples are glossary:zero-shot results.

![](file:30808301-2bd2-4591-b5e3-2dcfc293e3d8)

1. **Examples pane**: Shows the examples as a list of tiles. For each example, the Playground displays the following information:
   * A search query, classification category, or prompt for generating text based on a video.
   * A brief explanation of how you can use this search query or classification category
   * A list of relevant use cases
2. **Endpoint selector**:  By default, the Playground displays all the examples. Use this dropdown to filter based on an endpoint.


# TwelveLabs SDKs

{/* 
The 1.3 version of the API includes significant improvements and introduces breaking changes. If you are using v1.2, refer to the [Migration guide](/v1.3/docs/resources/migration-guide) page for a detailed list of changes, migration instructions, and code examples.
 */}

TwelveLabs provides the following client SDKs that enable you to integrate and utilize the platform within your application:


  

  



# Frequently asked questions

Navigate to the section that best addresses your query. If you don't find an answer to your question, please [contact us](mailto:support@twelvelabs.io) .

* [General questions](#general-questions)
* [Embed API](#embed-api)
* [Analyze](#analyze-api)

# General questions

This section answers frequently asked general questions.

## How does your model handle temporal dimension within videos?

We utilize a technique known as Positional Encoding, which is employed within the Transformers architecture to convey information regarding the position of a sequence of tokens within the input data. In this case, the tokens refer to the key scenes within the video. This technique facilitates the integration of sequential information into our model while simultaneously preserving the parallel processing capability of self-attention within the Transformer architecture.

## What is the maximum size of videos that can be stored in one index?

The Developer plan can accommodate up to 10,000 hours of video (whether in a single index or a combination of all indexes). For larger volumes, our enterprise plan would be best suited. Please contact us for more information at [sales@twelvelabs.io](mailto:sales@twelvelabs.io).

## How long does it take to index a video?

Indexing is typically completed in 30-40% of the duration of the video. However, indexing duration also depends on the number of concurrent indexing tasks, and delays can occur if too many indexing tasks are being processed simultaneously. If you're on the Free plan, for faster indexing, consider upgrading to the Developer plan, which supports more concurrent tasks. We also offer a dedicated cloud deployment option for enterprise customers. Please contact us at [sales@twelvelabs.io](mailto:sales@twelvelabs.io) to discuss this option.

## Can your model recognize natural sounds in videos?

Yes, the model analyzes visual and audio information and learns the correlation between certain visual objects or situations with sounds frequently appearing together.

## Can your models recognize text from other languages?

Yes, the models support multiple languages. See the [Supported languages](/v1.3/docs/concepts/models#supported-languages) page for details.

## How does your visual language model compare to other LLMs?

The platform utilizes a multimodal approach for video understanding. Instead of relying on textual input like traditional LLMs, the platform interprets visuals, sounds, and spoken words to deliver comprehensive and accurate results.

## Can I use TwelveLabs with my own LLM or with LangChain?

You can optionally integrate our video-to-text model (Pegasus) with your LLMs. We also provide an open-source project demonstrating the integration with LangChain. Find out more at [twelvelabs-io/tl-jockey](https://github.com/twelvelabs-io/tl-jockey).

## How can I change my login method?

To change your login method (for example, from username/password to SSO or vice versa), contact our support team at [support@twelvelabs.io](mailto:support@twelvelabs.io) to delete your current account, then create a new one with your preferred login method.

## Does my invoice include a detailed cost breakdown?

If you're on the Developer plan, TwelveLabs provides invoices that include a detailed cost breakdown. You can view your invoice using one of the following methods:

* **Email**: Open your invoice sent via email, and select the **View invoice and payment details** button.
* **Playground**: Go to the [Billing & plan](https://playground.twelvelabs.io/dashboard/billing) page, log in to your account, scroll to the **Billing History** section, and select the PDF for your invoice.

If you're on the Enterprise plan, TwelveLabs provides invoices without detailed cost breakdowns.

# Embed API

This section answers frequently asked questions related to the Embed API.

## When should I use the Embed API versus the built-in search?

The Embed API and built-in search service offer different functionalities for working with visual content.

**Embed API**

* Generate visual embeddings for:
  * RAG workflows
  * Hybrid search
  * Classification
  * Clustering
* Use the embeddings as input for your custom models
* Create flexible, domain-specific solutions

**Built-in search service**

* Perform semantic searches across multiple modalities:
  * Visual content
  * Conversation (human speech)
  * Text-in-video (OCR)
  * Logo
* Utilize production-ready, out-of-the-box functionality
* Ideal for projects not requiring additional customization

# Analyze API

This section answers frequently asked questions related to the Analyze API.

## What LLM does the Analyze API use?

The Analyze API employs our foundational Visual Language Model (VLM), which integrates a language encoder to extract multimodal data from videos and a decoder to generate concise text representations.

## To use the Analyze API, do I need to reindex my videos if I already indexed them with Marengo?

Yes, you must reindex videos using the Pegasus engine. See the [Analyze videos](/v1.3/docs/guides/analyze-videos) and Pricing pages for details.


# Sample applications

Discover the capabilities of the TwelveLabs Video Understanding Platform by experimenting with our fully functional sample applications:


  
    This application uses the semantic search capabilities of the platform to identify the most suitable influencers (organic brand fans) to reach out to.
  

  
    This application simplifies the cross-platform video promotion workflow by generating unique posts for each social media platform.
  

  
    This application uses image queries to find color shades in videos.
  

  
    This application evaluates job interview performances using the ability of the Pegasus video understanding engine to generate text based on video content.
  

  
    This application uses the Marengo video understanding engine to classify sports footage based on specific classes.
  

  
    This application processes security footage, dash camera videos, and CCTV recordings to identify and timestamp key security events such as unauthorized access attempts or suspicious behavior patterns.
  

  
    This application automatically creates multiple-choice questions from your video content, enabling educators and content creators to transform passive video viewing into interactive learning experiences.
  

  
    This application allows you to search for specific video content using text or image queries. You can refine your visual searches in real-time by cropping any section of your query image.
  

  
    This application automatically analyzes video content to create chapters and highlights, streamlining the video production workflow for content creators.
  

  
    This application transcribes and translates video content in multiple languages, offering adjustable proficiency levels.
  

  
    This multimodal RAG application offers personalized fashion recommendations based on both text and image queries. It utilizes the Embed API for analyzing video content, Milvus for vector searches, and GPT-3.5 for natural language processing.
  

  
    This application analyzes source footage, summarizes content, and recommends ads based on the footage’s context and emotional tone. It also supports embedding-based searches and suggests optimal ad placements, letting you preview how the footage and ads fit together.
  

  
    This application provides tailored video recommendations based on your profile and preferences, plus embedding-based searches for more accurate results.
  

  
    This application combines TwelveLabs' video embedding capabilities with Qdrant's vector similarity search functionality. This integration enables semantic understanding of video content, allowing you to discover relevant videos based on meaning rather than simple keyword matching.
  



# Partner integrations

This section features some of the most notable partner integrations that expand the functionality and efficiency of the TwelveLabs Video Understanding Platform.


  
    Adobe Premiere Pro plugin
  

  
    Semantic video search engine
  

  
    Multimodal RAG: Chat with videos
  

  
    Mastering Multimodal AI: Advanced video understanding
  

  
    Building advanced video understanding applications
  

  
    Advanced video search
  

  
    Build a powerful video summarization tool
  

  
    Semantic video search
  

  
    Unleashing video intelligence
  

  
    Multimodal RAG: Chat with videos
  

  
    Building a semantic video search workflow
  

  
    Multimodal video understanding
  

  
    Multivector video retrieval
  

  
    Search your videos semantically
  

  
    Leveraging RAG for improved video processing times
  


# Explore integration opportunities with TwelveLabs

If you're considering integrating your application with our platform or have already done so, we're here to support you. For assistance with integration, to feature your integration, or to explore further collaboration, please feel free to contact us.


# Adobe Premiere Pro Plugin

The TwelveLabs plugin for Adobe Premiere Pro integrates AI-driven video understanding into your editing workflow. The plugin allows you to search videos using natural language queries and automatically create chapters and highlights. This helps you save time by precisely locating the segments you need.

The plugin features a sidebar on the left with multiple tabs and a central panel:

* **Project**: Shows the current project and allows you to select or switch indexes, plus buttons to refresh or add new indexes.
* **Ingest**: Handles the uploading and indexing of your videos.
* **Search**: Allows natural language searches within your videos.
* **Segment**: Enables the creation of chapters or highlights to segment videos.
* **Settings**: Offers configuration options, such as logging in and out and resetting preferences.

# Typical workflow

To work with the TwelveLabs plugin, complete the tasks below in order:


  
    Ensure you have the required software, API key, and video files.
  

  
    Prepare your environment, install the plugin, and connect it to your TwelveLabs account.
  

  
    Upload your videos to the TwelveLabs Video Understanding Platform.
  

  
    Search for specific moments and work with auto-generated chapters and highlights.
  


# Prerequisites

Ensure you have the following:

* **Adobe Premiere Pro CC 2024-2025**: This version must be installed on your system.
* **TwelveLabs API key**: Retrieve your key from the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
* **Proxy files**: Create lower-resolution copies of your videos in Premiere Pro. For instructions, see the [Ingest and Proxy Workflow in Adobe Premiere Pro](https://helpx.adobe.com/premiere-pro/using/ingest-proxy-workflow.html) page of the Adobe documentation.
* **Video requirements**: Your proxy files must meet the requirements on the [Create a video indexing task](/api-reference/tasks/create#video-requirements) page.

# Set up the plugin

To set up the plugin, install it, connect your TwelveLabs account, and assign an index to your project.

## Install the plugin

Follow the steps below to add the plugin to Premiere Pro and access it from the application’s menu.


  
    If you do not already have the ZXP Installer, download it from the [ZXP/UXP Installer](https://aescripts.com/learn/zxp-installer/) page.
  

  
    Download the [TwelveLabs plugin](https://drive.google.com/uc?export=download\&id=1XxH3R7J_TeICyaDzTjaZqooUr4v3T7DJ).
  

  
    Launch the ZXP Installer application on your computer.
  

  
    Drag and drop the downloaded `.zxp` file onto the ZXP Installer window.
  

  
    Open Adobe Premiere Pro.
  

  
    In the Premiere Pro menu, go to **Window** > **Extensions** > **TwelveLabs**. This will open the plugin panel.
  


## Connect your TwelveLabs account

Connect the plugin to your TwelveLabs account:


  
    Open the TwelveLabs panel in Premiere Pro by selecting **Window** > **Extensions** > **TwelveLabs**.
  

  
    Enter your API key.
  

  
    Select the **Login** button.
  



  To switch API keys, go to the **Settings** tab, select the **Logout** button, and enter a new API key.


## Assign an index to your project

Assigning an index to a Premiere Pro project determines where videos are uploaded using the plugin. However, you can search and generate chapters or highlights across other indexes.


  
    In the TwelveLabs panel, select the **Project** tab.
  

  
    From the **TwelveLabs index** dropdown menu, choose an existing index or select the **+** icon to create a new one.
  



  * You can change the assigned index anytime by selecting a different option from the **TwelveLabs Index** dropdown in the **Project** tab.
  * The index ID is stored in your project metadata and will be retained in future versions of this project.


# Ingest videos

Ingesting videos will upload your proxy files to the TwelveLabs Video Understanding Platform for processing. After processing, you can search the videos or create chapters and highlights.


  
    Select a bin in your Premiere Pro project panel.
  

  
    Select the **Ingest** tab in the TwelveLabs panel.
  

  
    Select the **Add Selected Bins** button in the TwelveLabs panel. Note that clips are skipped if they don't have proxies or are already queued.
  

  
    *(Optional)* Monitor upload progress in the **Queue** section.
  



  By default, the plugin monitors your bins for new media. To disable this, toggle off the **Auto Scan** option at the bottom of the panel. When multiple editors work on the same project, configure only one machine with automatic scanning to prevent conflicts.


# Use the plugin

Follow the steps in the sections below to search through your video content and generate chapters or highlights.

## Search for specific moments

Searching enables you to locate specific video segments by describing content in natural language. Note that you can only search videos uploaded using the plugin. Videos not uploaded via the plugin will show a "Clips not in project" banner and will be greyed out.


  
    Select the **Search** tab.
  

  
    Choose an index from the dropdown menu.
  

  
    Enter your query in the text field and press **Enter**.
  

  
    Review the results displayed as a grid with thumbnails.
  

  
    For each clip in the search results, you can:

    * Select the **Play** button to open the clip with in/out points in the source monitor.
    * Select the **Timeline** button to add the clip with in/out points to the active timeline.
  

  
    *(Optional)* To import multiple clips simultaneously, select the checkboxes next to each clip and select the **Import Into Timeline** button to add all selected clips to the active timeline.
  


## Generate chapters or highlights

Generating chapters or highlights divides videos into meaningful segments, allowing you to quickly locate and work with specific sections without manually reviewing entire videos. Note that you can only generate chapters and highlights for videos uploaded using the plugin. Videos not uploaded through the plugin will display a "Clip not in project" banner and will be greyed out.


  
    Select the **Segments** tab.
  

  
    Choose an index from the dropdown menu.
  

  
    Choose the video for which you wish to generate chapters or highlights.
  

  
    Choose either the **Chapters** or **Highlights** tab.
  

  
    Review the results. For each segment, you can:

    * Select the **Play** button to open that segment with in/out points in the source monitor.
    * Select the **Timeline** button to add that segment with in/out points to the active timeline.
  

  
    *(Optional)* To customize the segments, edit the text in the **Prompt** field and select the **Refresh** button.
  



  To add multiple segments at once, select the checkboxes next to each segment and select the **Import into Timeline** button.



# Voxel51 - Semantic video search plugin

![](file:cf4012a3-14eb-46c2-94d2-5f6f6cac4623)

**Summary**: The Semantic Video Search plugin integrates Voxel FiftyOne,  an open-source tool for building and enhancing machine learning datasets, with the TwelveLabs Video Understanding Platform, enabling you to perform semantic searches across multiple modalities.

**Description**: The plugin allows you to accurately identify movements, actions, objects, people, sounds, on-screen text, and speech. For example, this feature is helpful in scenarios where you need to quickly locate and analyze specific scenes based on actions or spoken words, significantly improving your efficiency in categorizing and analyzing video data.

**Step-by-step guide**: Our blog post, Search Your Videos Semantically with TwelveLabs and FiftyOne Plugin, walks you through the steps required to create this plugin from scratch.

**GitHub**: Semantic Video Search

# Integration with TwelveLabs

The integration with the TwelveLabs Video Understanding Platform is comprised of three distinct steps:

* [Create an index](#create-an-index)
* [Upload videos](#upload-videos)
* [Perform semantic searches](#perform-semantic-searches)

## Create an index

The plugin invokes the [`POST`](/api-reference/indexes/create) method of the `/indexes` endpoint to create an index and enable the Marengo video understanding engine with the engine options that the user has selected:

```python Python
INDEX_NAME = ctx.params.get("index_name")

INDEXES_URL = f"{API_URL}/indexes"

headers = {
    "x-api-key": API_KEY
}

so = []

if ctx.params.get("visual"):
    so.append("visual")
if ctx.params.get("logo"):
    so.append("logo")
if ctx.params.get("text_in_video"):
    so.append("text_in_video")
if ctx.params.get("conversation"):
    so.append("conversation")

data = {
"engine_id": "marengo2.7",
"index_options": so,
"index_name": INDEX_NAME,
}

response = requests.post(INDEXES_URL, headers=headers, json=data)
```

## Upload videos

The plugin invokes the [`POST`](/api-reference/tasks/create) method of the `/tasks` endpoint. Then, it monitors the indexing process using the [`GET`](/api-reference/tasks/retrieve) method of the `/tasks/{task_id}` endpoint:

```python Python
TASKS_URL = f"{API_URL}/tasks"

videos = target_view
for sample in videos:
    if sample.metadata.duration < 4:
        continue
    else:
        file_name = sample.filepath.split("/")[-1] 
        file_path = sample.filepath 
        file_stream = open(file_path,"rb")
    
        headers = {
            "x-api-key": API_KEY
        }
    
        data = {
            "index_id": INDEX_ID, 
            "language": "en"
        }
    
        file_param=[
            ("video_file", (file_name, file_stream, "application/octet-stream")),]
    
        response = requests.post(TASKS_URL, headers=headers, data=data, files=file_param)
        TASK_ID = response.json().get("_id")
        print (f"Status code: {response.status_code}")
        pprint (response.json())
    
        TASK_STATUS_URL = f"{API_URL}/tasks/{TASK_ID}"
        while True:
            response = requests.get(TASK_STATUS_URL, headers=headers)
            STATUS = response.json().get("status")
            if STATUS == "ready":
                break
            time.sleep(10)
        
        VIDEO_ID = response.json().get('video_id')
        sample["TwelveLabs " + INDEX_NAME] = VIDEO_ID
        sample.save()
```

## Perform semantic searches

The plugin invokes the [`POST`](/api-reference/any-to-video-search/make-search-request) method of the `/search` endpoint to search across the sources of information that the user has selected:

```python Python
SEARCH_URL = f"{API_URL}/search"

headers = {
"x-api-key": API_KEY
}

so = []

if ctx.params.get("visual"):
    so.append("visual")
if ctx.params.get("logo"):
    so.append("logo")
if ctx.params.get("text_in_video"):
    so.append("text_in_video")
if ctx.params.get("conversation"):
    so.append("conversation")

data = {
"query": prompt,
"index_id": INDEX_ID,
"search_options": so,
}

response = requests.post(SEARCH_URL, headers=headers, json=data)
video_ids = [entry['video_id'] for entry in response.json()['data']]
print(response.json())
samples = []
view1 = target_view.select_by("TwelveLabs " + INDEX_NAME, video_ids,ordered=True)
start = [entry['start'] for entry in response.json()['data']]
end = [entry['end'] for entry in response.json()['data']]
if "results" in ctx.dataset.get_field_schema().keys():
    ctx.dataset.delete_sample_field("results")

i=0
for sample in view1:
    support = [int(start[i]*sample.metadata.frame_rate)+1 ,int(end[i]*sample.metadata.frame_rate)+1]
    sample["results"] = fo.TemporalDetection(label=prompt, support=tuple(support))
    sample.save()

view2 = view1.to_clips("results")
ctx.trigger("set_view", {"view": view2._serialize()})

return {}
```

# Next steps

After reading this page, you have several options:

* **Use the plugin as-is**:  Inspect the source code to better understand the platform's features and start using the plugin immediately.
* **Customize and enhance the plugin**: Feel free to modify the code to meet your specific requirements.
* **Explore further**: Try the [applications built by the community](/docs/resources/from-the-community) or our [sample applications](/docs/resources/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# MindsDB - The TwelveLabs handler

![](file:f44f4e5f-1445-4e86-bf62-804af811af05)

**Summary**: The  TwelveLabs handler for MindsDB allows you to search and summarize video content directly within MindsDB, streamlining the integration of these features into your applications.

**Description**:  This guide outlines how you can use the handler and how the handler interfaces with the TwelveLabs Video Understanding Platform to combine TwelveLabs' state-of-the-art foundation models for video understanding with MindsDB's platform for building customized AI solutions.

**Step-by-step guide**: Our blog post, [Build a Powerful Video Summarization Tool with Twelve Lans, MindsDB, and Slack](https://www.twelvelabs.io/blog/twelve-labs-and-mindsdb), walks you through the steps required to configure the TwelveLabs integration in MindsDB, deploy the TwelveLabs model for summarization within MindsDB, and automate the whole flow through a Slack bot that will periodically post the video summarizations as announcements.

**GitHub**:  TwelveLabs Handler.

# Use the handler

This section assumes the following:

* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  
* You have a running MindsDB database inside of a Docker container. If not, see the [Docker for MindsDB](https://docs.mindsdb.com/setup/self-hosted/docker)  section of the MindsDB documentation

Typically, the steps for using the handler are as follows:

1. Install the required dependencies inside the Docker container:
   ```shell
   pip install mindsdb[twelve_labs] 
   ```

2. Open the [MindsDB SQL Editor](https://docs.mindsdb.com/mindsdb_sql/connect/mindsdb_editor).

3. **Create an ML engine**. Use the `CREATE ML_ENGINE` statement, replacing the placeholders surrounded by `<>` with your values:




1. ```sql
   CREATE ML_ENGINE  
   from twelve_labs
   USING
       twelve_labs_api_key = ''
   ```
   The example below creates an ML engine named `twelve_labs_engine`:
   ```sql
   CREATE ML_ENGINE twelve_labs_engine 
   from twelve_labs
   USING
       twelve_labs_api_key = 'tlk_111' 
   ```

2. **Create a model**. Use the `CREATE_MODEL` statement to create a model. The `PREDICT` clause specifies the name of the column that will contain the results of the task. The `USING` clause specifies the parameters for the model. The parameters depend on the task you want to perform. The available tasks are `search` and `summarization`, and the parameters for each task are described in the [Creating Models](https://github.com/mindsdb/mindsdb/tree/staging/mindsdb/integrations/handlers/twelve_labs_handler#creating-models) section of the handler's GitHub Readme file.\
   The example below creates a model for the `search` task:
   ```sql
   CREATE MODEL mindsdb.twelve_labs_search
   PREDICT search_results
   USING
     engine = 'twelve_labs_engine',
     task = 'search',
     engine_id = 'marengo2.7',
     index_name = 'index_1',
     index_options = ['visual', 'conversation', 'text_in_video', 'logo'],
     video_urls = ['https://.../video_1.mp4', 'https://.../video_2.mp4'],
     search_options = ['visual', 'conversation', 'text_in_video', 'logo'],
     search_query_column = 'query'; 
   ```
   The example below creates a model for the `summarization` task:
   ```sql
   CREATE MODEL mindsdb.twelve_labs_summarization
   PREDICT summarization_results
   USING
     engine = 'twelve_labs_engine',
     task = 'summarization',
     engine_id = 'pegasus1',
     index_name = 'index_1',
     index_options = ['visual', 'conversation'],
     video_urls = ['https://.../video_1.mp4', 'https://.../video_2.mp4'],
     summarization_type = 'summary'; 
   ```

3. *(Optional)* **Check the status of the video indexing process**. The TwelveLabs Video Understanding Platform requires some time to index videos. You can search or summarize your videos only after the indexing process is complete. Use the `DESCRIBE` statement to check the status of the indexing process, replacing the placeholder surrounded by `<>` with your the name your model:

   ```sql
   DESCRIBE mindsdb.;
   ```

   The example below checks the status of a model named `twelve_labs_summarization`:

   ```
   DESCRIBE mindsdb.twelve_labs_summarization;
   ```

   You should see the status as `complete` in the `STATUS` column. In case of an error, check the `ERROR` column, which contains detailed information about the error.

4. **Retrieve the identifiers of the indexed videos**. Perform this step if you want to summarize a video. To retrieve the identifiers, use the `DESCRIBE` statement on the `indexed_videos` table of your model, replacing the placeholder surrounded by `<>` with the name of your model:

   ```
   DESCRIBE mindsdb..indexed_videos;
   ```

   The example below retrieves the identifiers of the videos uploaded to a model named `twelve_labs_summarization`:

   ```
   DESCRIBE mindsdb.twelve_labs_summarization.indexed_videos;
   ```

5. **Make predictions**. Use the `SELECT` statement to make predictions using the model created in the previous step. The `WHERE` clause specifies the condition for the prediction. The condition depends on the task you want to perform. The available tasks are `search` and `summarization`, and the conditions for each task are described in the [Making Predictions](https://github.com/mindsdb/mindsdb/tree/staging/mindsdb/integrations/handlers/twelve_labs_handler#making-predictions) section of the handler's GitHub Readme file:\
   The example below performs a search request. Ensure you replace the placeholder surrounded by `<>` with your query

   **Search**:\
   In the SQL query below, ensure you replace the placeholders surrounded by `<>` with your values:

   ```sql
   SELECT *
   FROM mindsdb.
   WHERE query = '';
   ```

   The example below makes predictions for the search task using a model named `twelve_labs_search`:

   ```sql
   SELECT *
   FROM mindsdb.twelve_labs_search
   WHERE query = 'Soccer player scoring a goal';
   ```

   **Summarize**:

   In the SQL query below, ensure you replace the placeholders surrounded by `<>` with your values:

   ```sql
   SELECT *
   FROM mindsdb.
   WHERE video_id = '';
   ```

   The example below makes predictions for the summarization task using a model named `twelve_labs_summarization`:

   ```sql
   SELECT *
   FROM mindsdb.twelve_labs_summarization
   WHERE video_id = '660bfa6766995fbd9fd662ee';
   ```




# Integration with TwelveLabs

For brevity, the sections below outline the key components for integrating MindsDB with the TwelveLabs Video Understanding Platform:

* Initialize a client
* Create indexes
* Upload videos
* Perform downstream tasks such as search or classification.

For all the components, refer to the TwelveLabs Handler page on GitHub.

## Initialize a client

The constructor sets up a new `TwelveLabsAPIClient` object that establishes a connection to the TwelveLabs Video Understanding Platform:

```python
def __init__(self, api_key: str, base_url: str = None):
    """
    The initializer for the TwelveLabsAPIClient.

    Parameters
    ----------
    api_key : str
        The TwelveLabs API key.
    base_url : str, Optional
        The base URL for the TwelveLabs API. Defaults to the base URL in the TwelveLabs handler settings.
    """

    self.api_key = api_key
    self.headers = {
        'Content-Type': 'application/json',
        'x-api-key': self.api_key
    }
    self.base_url = base_url if base_url else twelve_labs_handler_config.BASE_URL
```

## Create indexes

To create indexes, the `create_index` method  invokes the [`POST`](/api-reference/indexes/create)  method of the `/indexes` endpoint:

```python
def create_index(self, index_name: str, index_options: List[str], engine_id: Optional[str] = None, addons: Optional[List[str]] = None) -> str:
    """
    Create an index.

    Parameters
    ----------
    index_name : str
        Name of the index to be created.

    index_options : List[str]
        List of that specifies how the platform will process the videos uploaded to this index.

    engine_id : str, Optional
        ID of the engine. If not provided, the default engine is used.

    addons : List[str], Optional
        List of addons that should be enabled for the index.

    Returns
    -------
    str
        ID of the created index.
    """

    # TODO: change index_options to engine_options?
    # TODO: support multiple engines per index?
    body = {
        "index_name": index_name,
        "engines": [{
            "engine_name": engine_id if engine_id else twelve_labs_handler_config.DEFAULT_ENGINE_ID,
            "engine_options": index_options
        }],
        "addons": addons,
    }

    result = self._submit_request(
        method="POST",
        endpoint="/indexes",
        data=body,
    )

    logger.info(f"Index {index_name} successfully created.")
    return result['_id']
```

## Upload videos

To upload videos to the TwelveLabs Video Understanding Platform and index them, the handler invokes the [POST](/api-reference/tasks/create)  method of the `/tasks` endpoint:

```python
def create_video_indexing_tasks(self, index_id: str, video_urls: List[str] = None, video_files: List[str] = None) -> List[str]:
    """
    Create video indexing tasks.

    Parameters
    ----------
    index_id : str
        ID of the index.

    video_urls : List[str], Optional
        List of video urls to be indexed. Either video_urls or video_files should be provided. This validation is handled by TwelveLabsHandlerModel.

    video_files : List[str], Optional
        List of video files to be indexed. Either video_urls or video_files should be provided. This validation is handled by TwelveLabsHandlerModel.

    Returns
    -------
    List[str]
        List of task IDs created.
    """

    task_ids = []

    if video_urls:
        logger.info("video_urls has been set, therefore, it will be given precedence.")
        logger.info("Creating video indexing tasks for video urls.")

        for video_url in video_urls:
            task_ids.append(
                self._create_video_indexing_task(
                    index_id=index_id,
                    video_url=video_url
                )
            )

    elif video_files:
        logger.info("video_urls has not been set, therefore, video_files will be used.")
        logger.info("Creating video indexing tasks for video files.")
        for video_file in video_files:
            task_ids.append(
                self._create_video_indexing_task(
                    index_id=index_id,
                    video_file=video_file
                )
            )

    return task_ids

def _create_video_indexing_task(self, index_id: str, video_url: str = None, video_file: str = None) -> str:
    """
    Create a video indexing task.

    Parameters
    ----------
    index_id : str
        ID of the index.

    video_url : str, Optional
        URL of the video to be indexed. Either video_url or video_file should be provided. This validation is handled by TwelveLabsHandlerModel.

    video_file : str, Optional
        Path to the video file to be indexed. Either video_url or video_file should be provided. This validation is handled by TwelveLabsHandlerModel.

    Returns
    -------
    str
        ID of the created task.
    """

    body = {
        "index_id": index_id,
    }

    file_to_close = None
    if video_url:
        body['video_url'] = video_url

    elif video_file:
        import mimetypes
        # WE need the file open for the duration of the request. Maybe simplify it with context manager later, but needs _create_video_indexing_task re-written
        file_to_close = open(video_file, 'rb')
        mime_type, _ = mimetypes.guess_type(video_file)
        body['video_file'] = (file_to_close.name, file_to_close, mime_type)

    result = self._submit_multi_part_request(
        method="POST",
        endpoint="/tasks",
        data=body,
    )

    if file_to_close:
        file_to_close.close()

    task_id = result['_id']
    logger.info(f"Created video indexing task {task_id} for {video_url if video_url else video_file} successfully.")

    # update the video title
    video_reference = video_url if video_url else video_file
    task = self._get_video_indexing_task(task_id=task_id)
    self._update_video_metadata(
        index_id=index_id,
        video_id=task['video_id'],
        metadata={
            "video_reference": video_reference
        }
    )

    return task_id
```

Once the video has been uploaded to the platform, the handler monitors the indexing process using the [GET](/api-reference/tasks/retrieve)  method of the `/tasks/{task_id}` endpoint:

```python
def poll_for_video_indexing_tasks(self, task_ids: List[str]) -> None:
    """
    Poll for video indexing tasks to complete.

    Parameters
    ----------
    task_ids : List[str]
        List of task IDs to be polled.

    Returns
    -------
    None
    """

    for task_id in task_ids:
        logger.info(f"Polling status of video indexing task {task_id}.")
        is_task_running = True

        while is_task_running:
            task = self._get_video_indexing_task(task_id=task_id)
            status = task['status']
            logger.info(f"Task {task_id} is in the {status} state.")

            wait_durtion = task['process']['remain_seconds'] if 'process' in task else twelve_labs_handler_config.DEFAULT_WAIT_DURATION

            if status in ('pending', 'indexing', 'validating'):
                logger.info(f"Task {task_id} will be polled again in {wait_durtion} seconds.")
                time.sleep(wait_durtion)

            elif status == 'ready':
                logger.info(f"Task {task_id} completed successfully.")
                is_task_running = False

            else:
                logger.error(f"Task {task_id} failed with status {task['status']}.")
                # TODO: update Exception to be more specific
                raise Exception(f"Task {task_id} failed with status {task['status']}.")

    logger.info("All videos indexed successffully.")
```

## Perform downstream tasks

The handler supports the following downstream tasks - search and summarize videos. See the sections below for details.

### Search videos

To perform search requests, the handler  invokes the [`POST`](/api-reference/any-to-video-search/make-search-request)  method of the `/search` endpoint:

```python
def search_index(self, index_id: str, query: str, search_options: List[str]) -> Dict:
    """
    Search an index.

    Parameters
    ----------
    index_id : str
        ID of the index.

    query : str
        Query to be searched.

    search_options : List[str]
        List of search options to be used.

    Returns
    -------
    Dict
        Search results.
    """

    body = {
        "index_id": index_id,
        "query": query,
        "search_options": search_options
    }

    data = []
    result = self._submit_request(
        method="POST",
        endpoint="/search",
        data=body,
    )
    data.extend(result['data'])

    while 'next_page_token' in result['page_info']:
        result = self._submit_request(
            method="GET",
            endpoint=f"/search/{result['page_info']['next_page_token']}"
        )
        data.extend(result['data'])

    logger.info(f"Search for index {index_id} completed successfully.")
    return data
```

### Summarize videos

To summarize videos, the handler invokes the [`POST`](/api-reference/generate-text-from-video/summarize)  method of the `summarize` endpoint:

```python
    def summarize_videos(self, video_ids: List[str], summarization_type: str, prompt: str) -> Dict:
        """
        Summarize videos.

        Parameters
        ----------
        video_ids : List[str]
            List of video IDs.

        summarization_type : str
            Type of the summary to be generated. Supported types are 'summary', 'chapter' and 'highlight'.

        prompt: str
            Prompt to be used for the Summarize task

        Returns
        -------
        Dict
            Summary of the videos.
        """

        results = []
        results = [self.summarize_video(video_id, summarization_type, prompt) for video_id in video_ids]

        logger.info(f"Summarized videos {video_ids} successfully.")
        return results

    def summarize_video(self, video_id: str, summarization_type: str, prompt: str) -> Dict:
        """
        Summarize a video.

        Parameters
        ----------
        video_id : str
            ID of the video.

        summarization_type : str
            Type of the summary to be generated. Supported types are 'summary', 'chapter' and 'highlight'.

        prompt: str
            Prompt to be used for the Summarize task

        Returns
        -------
        Dict
            Summary of the video.
        """
        body = {
            "video_id": video_id,
            "type": summarization_type,
            "prompt": prompt
        }

        result = self._submit_request(
            method="POST",
            endpoint="/summarize",
            data=body,
        )

        logger.info(f"Video {video_id} summarized successfully.")
        return result
```




# Next steps

After reading this page, you have several options:

* **Use the handler**: Inspect the TwelveLabs Handler page on GitHub to better understand its features and start using it in your applications.
* **Explore further**: Try the [applications built by the community](/docs/examples/from-the-community) or our [sample applications](/docs/examples/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# Backblaze B2 - Media management application

**Summary**: The Media Asset Management example application demonstrates how you can add video understanding capabilities to a typical media asset management application using TwelveLabs for video understanding and Backblaze B2 for cloud storage.

**Description**: The application allows you to upload videos and perform deep semantic searches across multiple modalities such as visual, conversation, text-in-video, and logo. The matching video segments in the response are grouped by video, and you can view details about each segment.

**Step-by-step guide**: The  b2-twelvelabs-example GitHub repository provides detailed instructions on setting up the example application on your computer and using it.

**GitHub**: Backblaze B2 + TwelveLabs Media Asset Management Example

![](file:bd4ee401-1e54-4ccf-a900-d313061741e5)

# Integration with TwelveLabs

The integration with TwelveLabs video enhances the functionalities of a typical media management application by adding video understanding capabilities. The process is segmented into two main steps:

* Upload and index videos
* Search videos

# Upload and index videos

A Huey task automates the process of uploading and indexing videos. For each video, the application invokes the `create` method of the `task` object with the following parameters and values:

* `index_id`: A string representing unique identifier of the index to which the video will be updated.
* `url`: A string representing the URL of the video to be uploaded.
* `disable_video_stream`: A boolean indicating that the platform shouldn't store the video for streaming.

```python Python
@huey.db_task()
def do_video_indexing(video_tasks):
    print(f'Creating tasks: {video_tasks}')

    # Create a task for each video we want to index
    for video_task in video_tasks:
        task = TWELVE_LABS_CLIENT.task.create(
            TWELVE_LABS_INDEX_ID,
            url=default_storage.url(video_task['video']),
            disable_video_stream=True
        )
        print(f'Created task: {task}')
        video_task['task_id'] = task.id

    print(f'Created {len(video_tasks)} tasks')

```

Then, the application monitors the status of the upload process by invoking the `retrieve` method of the `task` object with the unique identifier of a task as a parameter:

```python Python
    print(f'Polling TwelveLabs for {video_tasks}')

    # Do a single database query for all the videos we're interested in
    video_ids = [video_task['id'] for video_task in video_tasks]
    videos = Video.objects.filter(id__in=video_ids)

    while True:
        done = True
        videos_to_save = []

        # Retrieve status for each task we created
        for video_task in video_tasks:
            # What's our current state for this video?
            video = videos.get(video__exact=video_task['video'])

            # Do we still need to retrieve status for this task?
            if video.status != 'Ready':
                task = TWELVE_LABS_CLIENT.task.retrieve(video_task['task_id'])
                if task.status != 'ready':
                    # We'll need to go round the loop again
                    done = False

                # Do we need to write a new status to the DB?
                if video.status.lower() != task.status:
                    # We store the status in the DB in title case, so it's ready to render on the page
                    new_status = task.status.title()
                    print(f'Updating status for {video_task["video"]} from {video.status} to {new_status}')
                    video.status = new_status
                    if task.status == 'ready':
                        video.video_id = task.video_id
                        get_all_video_data(video)
                    videos_to_save.append(video)

        if len(videos_to_save) > 0:
            Video.objects.bulk_update(videos_to_save, ['status', 'video_id', THUMBNAILS_PATH, TRANSCRIPTS_PATH, TEXT_PATH, LOGOS_PATH])

        if done:
            break

        sleep(TWELVE_LABS_POLL_INTERVAL)

    print(f'Done polling {video_tasks}')
```

## Search videos

![](file:ab6721a6-8cab-44a4-9101-111ed9e74c7f)




The application invokes the `query` method of the `search` object with the following parameters:

* `index_id`: A string representing the unique identifier of the index containing the videos to be searched.
* `query`: A string representing the query the user has provided.
* `options`: An array of strings representing the sources of information the TwelveLabs video understanding platform should consider when performing the search.
* `group_by`: A string specifying that the matching video clips in the response must be grouped by video.
* `threshold`: A string specifying the sstrictness of the thresholds for assigning the high, medium, or low confidence levels to search results. See the [Filter on the level of confidence](/docs/guides/search/filtering#filtering-on-the-level-of-confidence) section for details.

```python Python
def get_queryset(self):
    """
    Search TwelveLabs for videos matching the query
    """
    query = self.request.GET.get("query", None)

    result = TWELVE_LABS_CLIENT.search.query(
        TWELVE_LABS_INDEX_ID,
        query,
        ["visual", "conversation", "text_in_video", "logo"],
        group_by="video",
        threshold="medium"
    )

    # Search results may be in multiple pages, so we need to loop until we're done retrieving them
    search_data = result.data
    print(f"First page's data: {search_data}")

    search_results = []
    while True:
        # Do a database query to get the videos for each page of results
        video_ids = [group.id for group in search_data]
        videos = Video.objects.filter(video_id__in=video_ids)
        for group in search_data:
            try:
                search_results.append(SearchResult(video=videos.get(video_id__exact=group.id),
                                                   clip_count=len(group.clips),
                                                   clips=group.clips.model_dump_json()))
            except self.model.DoesNotExist:
                # There is a video in TwelveLabs, but no corresponding row in the database.
                # Just report it and carry on.
                print(f'Can\'t find match for video_id {group.id}')

        # Is there another page?
        try:
            search_data = next(result)
            print(f"Next page's data: {search_data}")
        except StopIteration:
            print("There is no next page in search result")
            break

    return search_results
```

# Next steps

After reading this page, you have several options:

* **Customize and use the example application**: Explore the Media Asset Management example application on GitHub to understand its features and implementation. You can make changes to the application and add more functionalities to suit your specific use case.
* **Explore further**: Try the [applications built by the community](/docs/resources/from-the-community) or our [sample applications](/docs/resources/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# MongoDB - Semantic video search

![](file:76e6c209-01cb-4a34-bc6d-c7e1ad27dfc8)

**Summary**: This integration combines TwelveLabs' [Embed API](/docs/guides/create-embeddings) with MongoDB [Atlas Vector Search](https://www.mongodb.com/products/platform/atlas-vector-search) to create an efficient semantic video search solution. It captures rich video content as multimodal embeddings, enabling precise and relevant search results.

**Description**: The process of performing a semantic video search using TwelveLabs and MongoDB Atlas involves two main steps:

1. Create embeddings for your video content and query.
2. Use the embeddings to perform a vector search in MongoDB Atlas.

**Step-by-step guide**: Our blog post, [Building Semantic Video Search with TwelveLabs Embed API and MongoDB Atlas](https://www.twelvelabs.io/blog/twelve-labs-and-mongodb), guides you through the process of creating a video search application, from setup to performing vector searches.

**Colab Notebook**: [TwelveLabs-EmbedAPI-MongoDB-Atlas](https://colab.research.google.com/drive/11f6LZVRtq2DpGuo6eWk4mbgjyNHfv-EY?usp=sharing)

# Integration with TwelveLabs

This section describes how you can use the [TwelveLabs Python SDK](https://github.com/twelvelabs-io/twelvelabs-python) to create embeddings for semantic video search. The integration involves creating two types of embeddings:

* Video embeddings from your video content
* Text embeddings from queries.

These embeddings form the basis for vector search operations.

## Video embeddings

The code below creates a video embedding task that handles the uploading and processing of a video. It periodically checks the status of the task and retrieves the embeddings upon completion:

```python Python
# Create a video embedding task for the uploaded video
task = tl_client.embed.task.create(
    engine_name="Marengo-retrieval-2.7",
    video_url="your-video-url"
)
print(
    f"Created task: id={task.id} engine_name={task.engine_name} status={task.status}"
)

# Monitor the status of the video embedding task
def on_task_update(task: EmbeddingsTask):
    print(f"  Status={task.status}")

status = task.wait_for_done(
    sleep_interval=2,
    callback=on_task_update
)
print(f"Embedding done: {status}")

# Retrieve the video embeddings
task_result = tl_client.embed.task.retrieve(task.id)
```

For more details, see the [Create video embeddings](/docs/guides/create-embeddings/video) page.

## Text embeddings

The code below creates a text embedding for the query provided in the `text` parameter:

```python Python
# Create a text embedding task for the text
embedding = tl_client.embed.create(
  engine_name="Marengo-retrieval-2.7",
  text="your-text"
)

print("Created a text embedding")
print(f" Engine: {embedding.engine_name}")
print(f" Embedding: {embedding.text_embedding.float}")
```

For more details, see the [Create text embeddings](/docs/guides/create-embeddings/text) page.

# Next steps

After reading this page, you have the following options:

* **Customize and use the example**: Use the [TwelveLabs-EmbedAPI-MongoDB-Atlas](https://colab.research.google.com/drive/11f6LZVRtq2DpGuo6eWk4mbgjyNHfv-EY?usp=sharing) notebook  to understand how the integration works. You can make changes and add more functionalities to suit your specific use case.
* **Explore further**: Try the [applications built by the community](/docs/resources/from-the-community) or our [sample applications](/docs/resources/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# Milvus - Advanced video search

![](file:436a8a02-7e05-4286-832b-592ed1b0c59c)

**Summary**:  This integration creates an efficient semantic video search solution by combining TwelveLabs' [Embed API](/docs/guides/create-embeddings), which generates multimodal embeddings from video content, with Milvus,  an open-source vector database that provides efficient storage and retrieval for your embeddings. This integration enables you to incorporate video content analysis capabilities into your applications.

Key use cases include:

* Content-based video retrieval
* Recommendation systems
* Search engines that understand the nuances of video data.

**Description**: The process of performing a semantic video search using the Embed API and Milvus involves two main steps:

1. Create multimodal embeddings for your video content using the Embed API.
2. Use the embeddings created in the previous step to perform similarity searches in Milvus.

**Step-by-step guide**: Our blog post, [Advanced Video Search: Leveraging TwelveLabs and Milvus for Semantic Search](https://www.twelvelabs.io/blog/twelve-labs-and-milvus), guides you through the process of creating a video search application, from setup to performing searches.

**Colab Notebook**: [TwelveLabs-EmbedAPI-Milvus](https://colab.research.google.com/drive/1nTOAxo82E71_d9XhSfFuVmkMPv1Fj4u9?usp=sharing)

# Integration with TwelveLabs

This section describes how you can use the [TwelveLabs Python SDK](https://github.com/twelvelabs-io/twelvelabs-python) to create video embeddings for semantic video search. The `generate_embedding` function returns a list of dictionaries, each containing an embedding vector and the associated metadata:

```python Python
def generate_embedding(video_url):
		"""
    Generate embeddings for a given video URL using the TwelveLabs API.

    This function creates an embedding task for the specified video URL using
    the Marengo-retrieval-2.7 engine. It monitors the task progress and waits
    for completion. Once done, it retrieves the task result and extracts the
    embeddings along with their associated metadata.

    Args:
        video_url (str): The URL of the video to generate embeddings for.

    Returns:
        tuple: A tuple containing two elements:
            1. list: A list of dictionaries, where each dictionary contains:
                - 'embedding': The embedding vector as a list of floats.
                - 'start_offset_sec': The start time of the segment in seconds.
                - 'end_offset_sec': The end time of the segment in seconds.
                - 'embedding_scope': The scope of the embedding (e.g., 'shot', 'scene').
            2. EmbeddingsTaskResult: The complete task result object from TwelveLabs API.

    Raises:
        Any exceptions raised by the TwelveLabs API during task creation,
        execution, or retrieval.
    """

    # Create an embedding task
    task = twelvelabs_client.embed.task.create(
        engine_name="Marengo-retrieval-2.7",
        video_url=video_url
    )
    print(f"Created task: id={task.id} engine_name={task.engine_name} status={task.status}")

    # Define a callback function to monitor task progress
    def on_task_update(task: EmbeddingsTask):
        print(f"  Status={task.status}")

    # Wait for the task to complete
    status = task.wait_for_done(
        sleep_interval=2,
        callback=on_task_update
    )
    print(f"Embedding done: {status}")

    # Retrieve the task result
    task_result = twelvelabs_client.embed.task.retrieve(task.id)

    # Extract and return the embeddings
    embeddings = []
    for v in task_result.video_embeddings:
        embeddings.append({
            'embedding': v.embedding.float,
            'start_offset_sec': v.start_offset_sec,
            'end_offset_sec': v.end_offset_sec,
            'embedding_scope': v.embedding_scope
        })
    
    return embeddings, task_result
```

For more details on how to create and customize video embeddings, see the [Create video embeddings](https://docs.twelvelabs.io/docs/create-video-embeddings) page.

# Next steps

After reading this page, you have the following options:

* **Customize and use the example**: Use the [TwelveLabs-EmbedAPI-Milvus](https://colab.research.google.com/drive/1nTOAxo82E71_d9XhSfFuVmkMPv1Fj4u9?usp=sharing) notebook to understand how the integration works. You can make changes and add more functionalities to suit your specific use case. Some notable examples include:
  * **Combine text and video queries for hybrid search**: Use the Embed API to [create text embeddings](/docs/guides/create-embeddings/text) for your queries. Then, you can perform weighted searches in Milvus using both text and video embeddings.
  * **Search within specific parts of videos**: Break long videos into smaller segments. Create an embedding for each segment to find specific moments in videos. To adjust the timing and length of your embeddings, see [Customize your embeddings](/docs/guides/create-embeddings/video#customize-your-embeddings).
  * **Analyze video content**: Use embeddings to group similar video segments. This helps you detect trends or find unusual content in large video collections.
* **Explore further**: Try the [applications built by the community](https://docs.twelvelabs.io/docs/from-the-community) or our [sample applications](https://docs.twelvelabs.io/docs/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# LanceDB - Building advanced video understanding applications

![](file:b961d670-a153-4980-bc70-2bf8372a3e04)

**Summary**: This integration allows you to create advanced video understanding and retrieval applications. It combines two key components:

* [**TwelveLabs' Embed API**](/docs/guides/create-embeddings): Generates multimodal embeddings for video content and text.
* [**LanceDB**](https://lancedb.com): A serverless vector database that stores, indexes, and queries high-dimensional vectors at scale.

Key use cases include:

* Semantic video search engines
* Content-based recommendation systems
* Anomaly detection in video streams.

**Description**: The process of performing a semantic video search using Twleve Labs and LanceDB involves two main steps:

1. Use the Embed API to create multimodal embeddings for video content and text queries.
2. Use the embeddings to perform similarity searches in LanceDB.

**Step-by-step guide**: Our blog post, [Building Advanced Video Understanding ApplicationsL Integrating TwelveLabs Embed API with LanceDB for Multimodal AI](https://www.twelvelabs.io/blog/twelve-labs-and-lancedb), guides you through the process of creating a video search application, from setup to generating video embeddings and querying them efficiently.\
**Colab Notebook**: [TwelveLabs-EmbedAPI-LanceDB](https://colab.research.google.com/drive/1ujc-_QkOgJjszOHq5EXqU767QWJGCAZm?usp=sharing)

# Integration with TwelveLabs

This section describes how you can use the [TwelveLabs Python SDK](https://github.com/twelvelabs-io/twelvelabs-python) to create embeddings for videos and text queries.

## Video embeddings

The `generate_embedding` function takes the URL of a video as a parameter and returns a list of dictionaries, each containing an embedding vector and the associated metadata:

```javascript Node.js
from twelvelabs.models.embed import EmbeddingsTask

def generate_embedding(video_url: str) -> tuple[List[Dict[str, Any]], Any]:
    """Generate embeddings for a given video URL."""
    task = twelvelabs_client.embed.task.create(
        engine_name="Marengo-retrieval-2.7",
        video_url=video_url
    )
    
    def on_task_update(task: EmbeddingsTask):
        print(f"  Status={task.status}")

    task.wait_for_done(sleep_interval=2, callback=on_task_update)
    task_result = twelvelabs_client.embed.task.retrieve(task.id)

    embeddings = [{
        'embedding': v.embedding.float,
        'start_offset_sec': v.start_offset_sec,
        'end_offset_sec': v.end_offset_sec,
        'embedding_scope': v.embedding_scope
    } for v in task_result.video_embeddings]
    
    return embeddings, task_result
```

For more details, see the [Create video embeddings](/docs/guides/create-embeddings/video) page.

## Text embeddings

The `get_text_embedding` function generates embeddings for text queries:

```javascript Node.js
def get_text_embedding(text_query: str) -> List[float]:
    """Generate a text embedding for a given text query."""
    return twelvelabs_client.embed.text(text_query).embedding.float

```

For more details, see the [Create text embeddings](/docs/guides/create-embeddings/text) page.

# Next steps

After reading this page, you have the following options:

* **Customize and use the example**: Use the [TwelveLabs-EmbedAPI-LanceDB](https://colab.research.google.com/drive/1ujc-_QkOgJjszOHq5EXqU767QWJGCAZm?usp=sharing) notebook to understand how the integration works. You can make changes and add more functionalities to suit your specific use case. Some notable examples include:
  * Explore advanced features in LanceDB like [hybrid search combining vector and metadata filtering](https://lancedb.github.io/lancedb/hybrid_search/hybrid_search/).
  * Implement a continuous user feedback loop to improve your search and recommendation results.
* **Explore further**: Try the [applications built by the community](https://docs.twelvelabs.io/docs/from-the-community) or our [sample applications](https://docs.twelvelabs.io/docs/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# ApertureDB - Semantic video search engine

![](file:6e4c7aff-ec85-4bc0-a596-8dc568338c59)

**Summary**: This integration combines TwelveLabs' [Embed API](/docs/guides/create-embeddings)  with ApertureDB to create an efficient semantic video search solution. It captures rich video content as multimodal embeddings, enabling precise and relevant search results.

**Description**: The process of performing a semantic video search using TwelveLabs and ApertureDB involves two main steps:

1. Create embeddings for your video content and query
2. Use the embeddings to perform a vector search in ApertureDB

**Step-by-step guide**: Our blog post, [Semantic Video Search Engine with TwelveLabs and ApertureDB](https://www.twelvelabs.io/blog/twelve-labs-and-aperturedb), guides you through the process of creating a video search application, from setup to performing vector searches.

**Colab Notebook**: [TwelveLabs-EmbedAPI-ApertureDB](https://colab.research.google.com/drive/19SETd2qpGPLQmuyzBok5GUxXqHLkQ7NF?usp=sharing)

# Integration with TwelveLabs

This section describes how you can use the TwelveLabs Python SDK to create embeddings for semantic video search. The integration involves creating two types of embeddings:

* Video embeddings from your video content
* Text embeddings from queries

These embeddings form the basis for vector search operations.

## Video embeddings

The code below creates a video embedding task that handles the processing of a video. It periodically checks the status of the task and retrieves the embeddings upon completion:

```python Python
from twelvelabs import TwelveLabs
from twelvelabs.models.embed import EmbeddingsTask

# Initialize the TwelveLabs client
twelvelabs_client = TwelveLabs(api_key=TL_API_KEY)

def generate_embedding(video_url):
    # Create an embedding task
    task = twelvelabs_client.embed.task.create(
        engine_name="Marengo-retrieval-2.7",
        video_url=video_url
    )
    print(f"Created task: id={task.id} engine_name={task.engine_name} status={task.status}")

    # Define a callback function to monitor task progress
    def on_task_update(task: EmbeddingsTask):
        print(f"  Status={task.status}")

    # Wait for the task to complete
    status = task.wait_for_done(
        sleep_interval=2,
        callback=on_task_update
    )
    print(f"Embedding done: {status}")

    # Retrieve the task result
    task_result = twelvelabs_client.embed.task.retrieve(task.id)

    # Extract and return the embeddings
    embeddings = []
    for v in task_result.video_embeddings:
        embeddings.append({
            'embedding': v.embedding.float,
            'start_offset_sec': v.start_offset_sec,
            'end_offset_sec': v.end_offset_sec,
            'embedding_scope': v.embedding_scope
        })

    return embeddings, task_result

# Example usage
video_url = "https://storage.googleapis.com/ad-demos-datasets/videos/Ecommerce%20v2.5.mp4"

# Generate embeddings for the video
embeddings, task_result = generate_embedding(video_url)

print(f"Generated {len(embeddings)} embeddings for the video")
for i, emb in enumerate(embeddings):
    print(f"Embedding {i+1}:")
    print(f"  Scope: {emb['embedding_scope']}")
    print(f"  Time range: {emb['start_offset_sec']} - {emb['end_offset_sec']} seconds")
    print(f"  Embedding vector (first 5 values): {emb['embedding'][:5]}")
    print()
```

For details on creating text embeddings, see the [Create video embeddings](/docs/guides/create-embeddings/video) page.

## Text embeddings

The code below creates a text embedding for the query provided in the `text` parameter:

```python Python
# Generate a text embedding for our search query
text_embedding = twelvelabs_client.embed.create(
  engine_name="Marengo-retrieval-2.7",
  text="Show me the part which has lot of outfits being displayed",
  text_truncate="none"
)

print("Created a text embedding")
print(f" Engine: {text_embedding.engine_name}")
print(f" Embedding: {text_embedding.text_embedding.float[:5]}...")  # Display first 5 values

```

For details on creating text embeddings, see the [Create text embeddings](/docs/guides/create-embeddings/text) page.

# Next steps

After reading this page, you have the following options:

* **Customize and use the example**: Use the [TwelveLabs-EmbedAPI-ApertureDB](https://colab.research.google.com/drive/19SETd2qpGPLQmuyzBok5GUxXqHLkQ7NF?usp=sharing) notebook to understand how the integration works. You can make changes and add functionalities to suit your specific use case. Below are a few examples:
  * **Explore data modeling**: Experiment with different video segmentation strategies to optimize embedding generation.
  * **Implement advanced search**: Try multimodal queries combining text, image, and audio inputs.
  * **Scale your system**: Test performance with larger video datasets and optimize for high-volume queries.
* **Explore further**: Try the [applications built by the community](/docs/resources/from-the-community) or our [sample applications](/docs/resources/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# Pinecone - Multimodal RAG

![](file:eb8db3cf-3b48-460c-b822-f5264804ce19)

**Summary**: This integration combines TwelveLabs' Embed and Analyze APIs with Pinecone's hosted vector database to build RAG-based video Q\&A applications. It transforms video content into rich embeddings that can be stored, indexed, and queried to extract text answers from unstructured video databases.

**Description**: The process of performing video-based question answering using TwelveLabs and Pinecone involves the following steps:

* Generate rich, contextual embeddings from your video content using the Embed API
* Store and index these embeddings in Pinecone's vector database
* Perform semantic searches to find relevant video segments
* Generate natural language responses using the Analyze API

This integration also showcases the difference in developer experience between using the Analyze API to generate text responses and a leading open-source model, [LLaVA-NeXT-Video](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf), allowing you to compare approaches and select the most suitable solution for your needs.

**Step-by-step guide**: Our blog post, [Multimodal RAG: Chat with Videos Using TwelveLabs and Pinecone](https://www.twelvelabs.io/blog/twelve-labs-and-pinecone), guides you through the process of creating a RAG-based video Q\&A application.

**Colab Notebook**: [TwelveLabs\_Pinecone\_Chat\_with\_video](https://colab.research.google.com/drive/10t5t6tfafJgD6y2Ghfn63RSd701hKE7U).

# Integration with TwelveLabs

This section describes how the application uses the [TwelveLabs Python SDK](https://github.com/twelvelabs-io/twelvelabs-python) with Pinecone to create a video Q\&A application. The integration is comprised of the following main steps:

* Video embedding generation using the Embed API
* Vector database storage and indexing
* Similarity search for relevant video segments
* Natural language response generation using the Analyze API

## Video Embeddings

The `generate_embedding` function generates embeddings for a video file:

```python Python
def generate_embedding(video_file, engine="Marengo-retrieval-2.7"):
    """
    Generate embeddings for a video file using TwelveLabs API.
    
    Args:
        video_file (str): Path to the video file
        engine (str): Embedding engine name
        
    Returns:
        tuple: Embeddings and metadata
    """
    # Create an embedding task
    task = twelvelabs_client.embed.task.create(
        engine_name=engine,
        video_file=video_file
    )
    print(f"Created task: id={task.id} engine_name={task.engine_name} status={task.status}")
    
    # Monitor task progress
    def on_task_update(task: EmbeddingsTask):
        print(f"  Status={task.status}")
    
    status = task.wait_for_done(
        sleep_interval=2,
        callback=on_task_update
    )
    print(f"Embedding done: {status}")
    
    # Retrieve results
    task_result = twelvelabs_client.embed.task.retrieve(task.id)
    
    # Extract embeddings and metadata
    embeddings = task_result.float
    time_ranges = task_result.time_ranges
    scope = task_result.scope
    
    return embeddings, time_ranges, scope
```

For details on creating video embeddings, see the [Create video embeddings](/docs/guides/create-embeddings/video) page.

The `ingest_data` function stores embeddings in Pinecone:

```python Python
def ingest_data(video_file, index_name="twelve-labs"):
    """
    Generate embeddings and store them in Pinecone.
    
    Args:
        video_file (str): Path to the video file
        index_name (str): Name of the Pinecone index
    """
    # Generate embeddings
    embeddings, time_ranges, scope = generate_embedding(video_file)
    
    # Connect to Pinecone index
    index = pc.Index(index_name)
    
    # Prepare vectors for upsert
    vectors = []
    for i, embedding in enumerate(embeddings):
        vectors.append({
            "id": f"{video_file}_{i}",
            "values": embedding,
            "metadata": {
                "video_file": video_file,
                "time_range": time_ranges[i],
                "scope": scope
            }
        })
    
    # Upsert vectors to Pinecone
    index.upsert(vectors=vectors)
    print(f"Successfully ingested {len(vectors)} embeddings into Pinecone")
```

## Video search

The `search_video_segments` function creates text embeddings and performs similarity searches to find relevant video segments using the embeddings that have already been stored in Pinecone:

```python Python
def search_video_segments(question, index_name="twelve-labs", top_k=5):
    """
    Search for relevant video segments based on a question.
    
    Args:
        question (str): Question text
        index_name (str): Name of the Pinecone index
        top_k (int): Number of results to retrieve
        
    Returns:
        list: Relevant video segments and their metadata
    """
    # Generate text embedding for the question
    question_embedding = twelvelabs_client.embed.create(
        engine_name="Marengo-retrieval-2.7",
        text=question
    ).text_embedding.float
    
    # Query Pinecone
    index = pc.Index(index_name)
    query_results = index.query(
        vector=question_embedding,
        top_k=top_k,
        include_metadata=True
    )
    
    # Process and return results
    results = []
    for match in query_results.matches:
        results.append({
            "score": match.score,
            "video_file": match.metadata["video_file"],
            "time_range": match.metadata["time_range"],
            "scope": match.metadata["scope"]
        })
    
    return results
```

For details on creating text embeddings, see the [Create text embeddings](/docs/guides/create-embeddings/text) page.

## Natural language responses

After retrieving relevant video segments, the application uses the  Analyze API to create natural language responses:

```python Python
def generate_response(question, video_segments):
    """
    Generate a natural language response using Pegasus.
    
    Args:
        question (str): The user's question
        video_segments (list): Relevant video segments from search
        
    Returns:
        str: Generated response based on video content
    """
    # Prepare context from video segments
    context = []
    for segment in video_segments:
        # Get the video clip based on time range
        video_file = segment["video_file"]
        start_time, end_time = segment["time_range"]
        
        # You can extract the clip or use the metadata directly
        context.append({
            "content": f"Video segment from {video_file}, {start_time}s to {end_time}s",
            "score": segment["score"]
        })
    
    # Generate response using TwelveLabs Analyze API
    response = twelvelabs_client.generate.create(
        engine_name="Pegasus-1.0",
        prompt=question,
        contexts=context,
        max_tokens=250
    )
    
    return response.generated_text
```

For details on analyzing videos and generating open-ended texts from their content see the [Open-ended analysis](/docs/guides/analyze-videos/open-ended-analysis) page.

## Create a complete Q\&A function

The application creates a complete Q\&A function by combining search and response generation:

```python Python
def video_qa(question, index_name="twelve-labs"):
    """
    Complete video Q&A pipeline.
    
    Args:
        question (str): User's question
        index_name (str): Pinecone index name
        
    Returns:
        dict: Response with answer and supporting video segments
    """
    # Find relevant video segments
    video_segments = search_video_segments(question, index_name)
    
    # Generate response using Pegasus
    answer = generate_response(question, video_segments)
    
    return {
        "question": question,
        "answer": answer,
        "supporting_segments": video_segments
    }
```

# Next steps

After reading this page, you have the following options:

* **Customize and use the example**: Use the [TwelveLabs\_Pinecone\_Chat\_with\_video](https://colab.research.google.com/drive/10t5t6tfafJgD6y2Ghfn63RSd701hKE7U) notebook to understand how the integration works. You can make changes and add functionalities to suit your specific use case. Below are a few examples:
  * Training a linear adapter on top of the embeddings to better fit your data.
  * Re-ranking videos using Pegasus when clips from different videos are returned.
  * Adding textual summary data for each video to the Pinecone entries to create a hybrid search system, enhancing accuracy using [Pinecone's Metadata capabilities](https://docs.pinecone.io/guides/data/filter-with-metadata#insert-metadata-into-an-index).
* **Explore further**: Try the [applications built by the community](/docs/resources/from-the-community) or our [sample applications](/docs/resources/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# Databricks - Advanced video understanding

![](file:fffd8a56-50ab-4f43-ac1b-50b2ed31d789)

**Summary**: This integration combines TwelveLabs' [Embed API](/docs/guides/create-embeddings) with Databricks Mosaic AI Vector Search to create advanced video understanding applications. It transforms video content into multimodal embeddings that capture the relationships between visual expressions, body language, spoken words, and overall context, enabling powerful similarity search and recommendation systems.

**Description**: Integrating TwelveLabs with Databricks Mosaic AI addresses key challenges in video AI, such as efficient processing of large-scale video datasets and accurate multimodal content representation. The process involves these main steps:

1. Generate multimodal embeddings from video content using TwelveLabs' Embed API
2. Store these embeddings along with video metadata in a Delta Table
3. Configure Mosaic AI Vector Search with a Delta Sync Index to access the embeddings
4. Generate text embeddings for search queries
5. Perform similarity searches between text queries and video content
6. Build a video recommendation system that suggests videos similar to a given video based on embedding similarities

**Step-by-step guide**: Our blog post, [Mastering Multimodal AI: Advanced Video Understanding with TwelveLabs + Databricks Mosaic AI](https://www.twelvelabs.io/blog/twelve-labs-and-databricks-mosaic-ai), guides you through setting up the environment, generating embeddings, and implementing the similarity search and recommendation functionalities.

# Integration with TwelveLabs

This section describes how you can use the TwelveLabs Python SDK to create embeddings. The integration involves creating two types of embeddings:

* Video embeddings from your video content
* Text embeddings from queries

## Video embeddings

The `get_video_embeddings` function creates a Pandas UDF to generate multimodal embeddings using TwelveLabs Embed API:

```Python Python
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import ArrayType, FloatType
from twelvelabs.models.embed import EmbeddingsTask
import pandas as pd

@pandas_udf(ArrayType(FloatType()))
def get_video_embeddings(urls: pd.Series) -> pd.Series:
    def generate_embedding(video_url):
        twelvelabs_client = TwelveLabs(api_key=TWELVE_LABS_API_KEY)
        task = twelvelabs_client.embed.task.create(
            engine_name="Marengo-retrieval-2.7",
            video_url=video_url
        )
        task.wait_for_done()
        task_result = twelvelabs_client.embed.task.retrieve(task.id)
        embeddings = []
        for v in task_result.video_embeddings:
            embeddings.append({
                'embedding': v.embedding.float,
                'start_offset_sec': v.start_offset_sec,
                'end_offset_sec': v.end_offset_sec,
                'embedding_scope': v.embedding_scope
            })
        return embeddings

    def process_url(url):
        embeddings = generate_embedding(url)
        return embeddings[0]['embedding'] if embeddings else None

    return urls.apply(process_url)
```

For details on creating video embeddings, see the [Create video embeddings](/docs/guides/create-embeddings/video) page.

## Text embeddings

The `get_text_embedding` function generates text embeddings:

```Python Python
def get_text_embedding(text_query):
    # TwelveLabs Embed API supports text-to-embedding
    text_embedding = twelvelabs_client.embed.create(
      engine_name="Marengo-retrieval-2.7",
      text=text_query,
      text_truncate="start"
    )

    return text_embedding.text_embedding.float
```

For details on creating video embeddings, see the [Create text embeddings](/docs/guides/create-embeddings/text) page.

## Similarity search

The `similarity_search` function generates an embedding for a text query, and uses the Mosaic AI Vector Search index to find similar videos:

```Python Python
def similarity_search(query_text, num_results=5):
    # Initialize the Vector Search client and get the query embedding
    mosaic_client = VectorSearchClient()
    query_embedding = get_text_embedding(query_text)

    print(f"Query embedding generated: {len(query_embedding)} dimensions")

    # Perform the similarity search
    results = index.similarity_search(
        query_vector=query_embedding,
        num_results=num_results,
        columns=["id", "url", "title"]
    )
    return results
```

## Video recommendation

The `get_video_recommendations` takes a video ID and the number of recommendations to return as parameters and performs a similarity search to find the most similar videos.

```Python Python
def get_video_recommendations(video_id, num_recommendations=5):
    # Initialize the Vector Search client
    mosaic_client = VectorSearchClient()

    # First, retrieve the embedding for the given video_id
    source_df = spark.table("videos_source_embeddings")
    video_embedding = source_df.filter(f"id = {video_id}").select("embedding").first()

    if not video_embedding:
        print(f"No video found with id: {video_id}")
        return []

    # Perform similarity search using the video's embedding
    try:
        results = index.similarity_search(
            query_vector=video_embedding["embedding"],
            num_results=num_recommendations + 1,  # +1 to account for the input video
            columns=["id", "url", "title"]
        )
        
        # Parse the results
        recommendations = parse_search_results(results)
        
        # Remove the input video from recommendations if present
        recommendations = [r for r in recommendations if r.get('id') != video_id]
        
        return recommendations[:num_recommendations]
    except Exception as e:
        print(f"Error during recommendation: {e}")
        return []

# Helper function to display recommendations
def display_recommendations(recommendations):
    if recommendations:
        print(f"Top {len(recommendations)} recommended videos:")
        for i, video in enumerate(recommendations, 1):
            print(f"{i}. Title: {video.get('title', 'N/A')}")
            print(f"   URL: {video.get('url', 'N/A')}")
            print(f"   Similarity Score: {video.get('score', 'N/A')}")
            print()
    else:
        print("No recommendations found.")

# Example usage
video_id = 1  # Assuming this is a valid video ID in your dataset
recommendations = get_video_recommendations(video_id)
display_recommendations(recommendations)
```

# Next steps

After reading this page, you have the following options:

* **Customize and use the example**: After implementing the basic integration, consider these improvements:
  * **Update and synchronize the index**: Implement efficient incremental updates and scheduled synchronization jobs using Delta Lake features.
  * **Optimize performance and scaling**: Leverage distributed processing, intelligent caching, and index partitioning for larger video libraries
  * **Monitoring and analytics**: Track key performance metrics, implement feedback loops, and correlate capabilities with business metrics
* **Explore further**: Try the [applications built by the community](/docs/resources/from-the-community) or our [sample applications](/docs/resources/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# Qdrant - Building a semantic video search workflow

![](file:9e94ba7e-e5f4-4d35-a036-b44aeb88fd4b)

**Summary**: This integration combines TwelveLabs' [Embed API](/docs/guides/create-embeddings) with [Qdrant Vector Search](https://qdrant.tech) to create an efficient semantic video search solution. It generates multimodal embeddings for video content, allowing for precise and relevant search results across various modalities.

**Description**: The process of performing semantic video searches using TwelveLabs and Qdrant involves the following main steps:

1. Generate multimodal embeddings for your video content.
2. Store these embeddings in Qdrant.
3. Create embeddings for your search queries. You can provide either text, audio, or images as queries.
4. Use these query embeddings to perform vector searches in Qdrant.

**Step-by-step guide**: Our blog post, [Building a Semantic Video Search Workflow with TwelveLabs and Qdrant](https://www.twelvelabs.io/blog/twelve-labs-and-qdrant), guides you through the process of building a semantic video search workflow. This workflow is ideal for applications like video indexing, content recommendation systems, and contextual search engines.

**Colab Notebook**: [TwelveLabs-EmbedAPI-Qdrant](https://colab.research.google.com/drive/1_fYca9j1Tx52WkSRSt2IOwi_H4XKMmLS?usp=sharing)

# Integration with TwelveLabs

This section explains how to utilize the [TwelveLabs Python SDK](https://github.com/twelvelabs-io/twelvelabs-python) to create embeddings for semantic video search. The integration involves generating the following types of embeddings:

* Video embeddings derived from your video content
* Text, video, and audio embeddings for the queries

## Video embeddings

The following code generates an embedding for a video. It creates a video embedding task that processes the video and periodically checks the task's status to retrieve the embeddings upon completion.

```python Python
# Step 1: Create an embedding task
task = twelvelabs_client.embed.task.create(
    model_name="Marengo-retrieval-2.7",  # Specify the model
    video_url="https://sample-videos.com/video321/mp4/720/big_buck_bunny_720p_2mb.mp4"  # Video URL
)

# Step 2: Wait for the task to complete
task.wait_for_done(sleep_interval=3)  # Check every 3 seconds

# Step 3: Retrieve the embeddings
task_result = twelvelabs_client.embed.task.retrieve(task.id)

# Display the embedding results
print("Embedding Vector (First 10 Dimensions):", task_result.embeddings[:10])
print("Embedding Dimensionality:", len(task_result.embeddings))
```

For details on creating text embeddings, see the [Create video embeddings](/docs/guides/create-embeddings/video) page.

## Text embeddings

The code below generates a text embedding and identifies the video segments that match your text semantically.

```python Python
# Generate text embedding
text_segment = twelvelabs_client.embed.create(
    model_name="Marengo-retrieval-2.7",
    text="A white rabbit",  # Input query
).text_embedding.segments[0]

# Perform semantic search in Qdrant
text_results = qdrant_client.query_points(
    collection_name=collection_name,
    query=text_segment.embeddings_float,  # Use the embedding vector
)

print("Text Query Results:", text_results)
```

For details on creating text embeddings, see the [Create text embeddings](/docs/guides/create-embeddings/text) page.

## Audio embeddings

The code below generates an audio embedding and finds the video segments that match the semantic content of your audio clip.

```python Python
# Generate audio embedding
audio_segment = twelvelabs_client.embed.create(
    model_name="Marengo-retrieval-2.7",
    audio_url="https://codeskulptor-demos.commondatastorage.googleapis.com/descent/background%20music.mp3",  # Audio file URL
).audio_embedding.segments[0]

# Perform semantic search in Qdrant
audio_results = qdrant_client.query_points(
    collection_name=collection_name,
    query=audio_segment.embeddings_float,  # Use the embedding vector
)

print("Audio Query Results:", audio_results)
```

For details on creating text embeddings, see the [Create audio embeddings](/docs/guides/create-embeddings/audio) page.

## Image embeddings

The code below generates an image embedding and identifies video segments that are semantically similar to the image.

```python Python
# Generate image embedding
image_segment = twelvelabs_client.embed.create(
    model_name="Marengo-retrieval-2.7",
    image_url="https://gratisography.com/wp-content/uploads/2024/01/gratisography-cyber-kitty-1170x780.jpg",  # Image URL
).image_embedding.segments[0]

# Perform semantic search in Qdrant
image_results = qdrant_client.query_points(
    collection_name=collection_name,
    query=image_segment.embeddings_float,  # Use the embedding vector
)

print("Image Query Results:", image_results)
```

For details on creating image embeddings, see the [Create image embeddings](/docs/guides/create-embeddings/image) page.

# Next steps

After reading this page, you have the following options:

* **Customize and use the example**: Use the [TwelveLabs-EmbedAPI-Qdrant](https://colab.research.google.com/drive/1_fYca9j1Tx52WkSRSt2IOwi_H4XKMmLS?usp=sharing) notebook to understand how the integration works. You can make changes and add functionalities to suit your specific use case.
* **Explore further**: Try the [applications built by the community](/docs/resources/from-the-community) or our [sample applications](/docs/resources/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# Vespa - Multivector video retrieval

![](file:0bb7b4e5-e0de-4776-8743-146f095770c4)

**Summary**: This integration combines TwelveLabs' Analyze and Embed APIs with [Vespa](https://vespa.ai/) to create an efficient solution for semantic video search. It captures rich video content as multimodal embeddings and utilizes Vespa's robust indexing and hybrid ranking capabilities to deliver precise and relevant search results.

**Description**: The process of performing a semantic video search using TwelveLabs and Vespa involves three main steps:

1. Create summaries and keywords using the [Analyze](/docs/guides/analyze-videos) API.
2. Create multimodal embeddings for your video content using the [Embed API](/docs/guides/create-embeddings).
3. Deploy a Vespa application to index these embeddings.
4. Use the embeddings to perform vector searches with hybrid ranking.
5. Review the results.

**Step-by-step guide**: Our blog post, [Multivector Video Retrieval with TwelveLabs and Vespa](https://www.twelvelabs.io/blog/twelve-labs-and-vespa), guides you through the process of building a semantic video search solution.

**Colab Notebook**: [video\_search\_twelvelabs\_cloud](https://drive.google.com/file/d/1CKX476xAF_JMr16Xg5XeDW1fAsYsLtfe/view?usp=sharing)

# Integration with TwelveLabs

This section shows how to use the Generate and Embed APIs for creating video embeddings and metadata, which facilitate the efficient retrieval of relevant video segments.

## Generate summaries and keywords

The code below uploads videos to an index and monitors the processing status:

```python Python
def on_task_update(task: EmbeddingsTask):
    print(f"  Status={task.status}")

for video_url in VIDEO_URLs:
    task = client.task.create(index_id=index.id, url=video_url, language="en")
    status = task.wait_for_done(sleep_interval=10, callback=on_task_update)
    if task.status != "ready":
        raise RuntimeError(f"Indexing failed with status {task.status}")

```

Once the videos are processed, you can generate rich metadata using the `/summarize` and `/analyze` endpoints. This code creates summaries and lists of keywords for each video to enhance search capabilities:

```python Python
summaries = []
keywords_array = []
titles = [
    "Mr. Bean the Animated Series Holiday for Teddy",
    "Twas the night before Christmas",
    "Hide and Seek with Giant Jenny",
]

videos = client.index.video.list(index_id)
for video in videos:
    # Generate summary
    res = client.generate.summarize(
        video_id=video.id,
        type="summary",
        prompt="Generate an abstract of the video serving as metadata on the video, up to five sentences."
    )
    summaries.append(res.summary)
    
    # Generate keywords
    keywords = client.generate.text(
        video_id=video.id,
        prompt="Based on this video, I want to generate five keywords for SEO. Provide just the keywords as a comma delimited list."
    )
    keywords_array.append(keywords.data)
```

## Create video embeddings

The code below creates multimodal embeddings for each video. These embeddings capture the temporal and contextual nuances of the video content:

```python Python
task_ids = []

for url in VIDEO_URLs:
    task = client.embed.task.create(model_name="Marengo-retrieval-2.7", video_url=url)
    task_ids.append(str(task.id))
    status = task.wait_for_done(sleep_interval=10, callback=on_task_update)
    if task.status != "ready":
        raise RuntimeError(f"Embedding failed with status {task.status}")

tasks = []
for task_id in task_ids:
    task = client.embed.task.retrieve(task_id)
    tasks.append(task)
```

See the [Create video embeddings](/docs/guides/create-embeddings/video) section for details.

## Create text emeddings

The code below generates an embedding for your text query:

```python Python
client = TwelveLabs(api_key=TL_API_KEY)
user_query = "Santa Claus on his sleigh"

# Generate embedding for the query
res = client.embed.create(
    model_name="Marengo-retrieval-2.7",
    text=user_query,
)

print("Created a text embedding")
print(f" Model: {res.model_name}")
if res.text_embedding is not None and res.text_embedding.segments is not None:
    q_embedding = res.text_embedding.segments[0].embeddings_float
    print(f" Embedding Dimension: {len(q_embedding)}")
    print(f" Sample 5 values from array: {q_embedding[:5]}")
```

See the [Create text embeddings](/docs/guides/create-embeddings/text) section for details.

## Perform hybrid searches

The code below uses Vespa's approximate nearest neighbor (ANN) search capabilities to combine lexical search (BM25) with vector similarity ranking. The query retrieves the top hit based on hybrid ranking:

```python Python
with app.syncio(connections=1) as session:
    response: VespaQueryResponse = session.query(
        yql="select * from videos where userQuery() OR ({targetHits:100}nearestNeighbor(embeddings,q))",
        query=user_query,
        ranking="hybrid",
        hits=1,
        body={"input.query(q)": q_embedding},
    )
    assert response.is_successful()

# Print the top hit
for hit in response.hits:
    print(json.dumps(hit, indent=4))

# Get full response JSON
response.get_json()
```

# Next steps

After reading this page, you have the following options:

* **Customize and use the example**: Use the [video\_search\_twelvelabs\_cloud](https://drive.google.com/file/d/1CKX476xAF_JMr16Xg5XeDW1fAsYsLtfe/view?usp=sharing) notebook to understand how the integration works. You can make changes and add functionalities to suit your specific use case.
* **Explore further**: Try the [applications built by the community](/docs/resources/from-the-community) or our [sample applications](/docs/resources/sample-applications) to get more insights into the TwelveLabs Video Understanding Platform's diverse capabilities and learn more about integrating the platform into your applications.


# Weaviate - Leveraging RAG for Improved Video Processing Times

![](file:421f364a-2051-4dee-bcbc-2eec6fd666bc)

**Summary**: This integration combines Twelve Labs' advanced video understanding capabilities with Weaviate's vector database to create an efficient Retrieval-Augmented Generation (RAG) system for video content. It enables precise retrieval of relevant video segments, significantly reducing processing time and computational resources while maintaining high-quality analysis for applications such as content moderation, sports analysis, and educational content.

**Description**: Video processing, particularly for long-form content, is resource-intensive and time-consuming. This integration addresses these challenges using a RAG-based approach, which processes only the most relevant video segments rather than entire videos. This approach reduces computational load and enhances scalability, enabling effective analysis of longer videos and larger datasets while delivering precise results.

**Step-by-step guide**:
For detailed instructions on implementing this integration, refer to our blog post, [Leveraging RAG for Improved Video Processing Times with TwelveLabs and Weaviate](https://www.twelvelabs.io/blog/twelve-labs-and-weaviate), which walks you through the process of building an efficient RAG system for video content using Twelve Labs and Weaviate.

**Colab notebook**: [Google Drive Folder containing the Colab Notebook and video data](https://drive.google.com/drive/folders/1BAfHkqjVD-z___FpnC5bjE4SrS49z_Vu?usp=sharing).


# Chroma - Multimodal RAG: Chat with Videos

![](file:ce81b57d-b829-43d0-a712-b2d0d17f7cc3)

**Summary**: This integration combines TwelveLabs' [Embed API](/docs/guides/create-embeddings) with [Chroma’s Vector Database](https://www.trychroma.com/) to enable Retrieval-Augmented Generation (RAG) based Q\&A on videos.

**Description**: The integration leverages TwelveLabs' contextual embeddings and Chroma’s efficient vector storage to power a chat application that extracts precise text answers from unstructured video content.

**Step-by-step guide**: Our blog post, [Multimodal RAG: Chat with Videos Using TwelveLabs and Chroma](https://www.twelvelabs.io/blog/twelve-labs-and-chroma), provides a detailed walkthrough of this integration process.

**Colab notebook**: [TwelveLabs\_Chroma\_Chat\_with\_video\_Colab](https://drive.google.com/file/d/1xbW2Ycen4uFaIX-8P8SM8qldgSoBwPVy/view?usp=sharing)


# Snowflake - Multimodal Video Understanding

![](file:f3d99fae-2ce7-4fe1-bad8-130f4b83966e)

**Summary**: This integration combines TwelveLabs' [Embed API](/docs/guides/create-embeddings) with [Snowflake Cortex](https://docs.snowflake.com/en/user-guide/snowflake-cortex/overview) to create advanced video search and summarization workflows.

**Description**: This integration enables you to generate multimodal video embeddings using TwelveLabs' Embed API and store them in Snowflake tables with the `VECTOR` datatype for efficient similarity searches. By utilizing Snowflake Cortex Complete for summarization and other AI capabilities, you can build powerful applications such as semantic video search, content recommendations, and more.

**Step-by-step guide**: Our blog post, [Integrating TwelveLabs Embed API with Snowflake Cortex for Multimodal Video Understanding](https://www.twelvelabs.io/blog/twelve-labs-and-snowflake-cortex), provides a detailed walkthrough of the integration process.


# Oracle - Unleashing Video Intelligence

![](file:7794dc47-4314-4e3c-bcdb-7c89e51bcc99)

**Summary**: This integration combines TwelveLabs' [Embed API](/docs/guides/create-embeddings) with Oracle Database 23ai's AI [Vector Search](https://www.oracle.com/database/ai-vector-search/) to enable advanced video understanding and search capabilities.

**Description**: The integration provides a unified architecture that combines vector embeddings and relational data storage, streamlining operations by eliminating synchronization needs, reducing maintenance costs, and enhancing query performance with lower latency. Utilizing Oracle’s enterprise-grade reliability and scalability, this solution supports semantic video search, cross-modal queries, and advanced analytics, enabling you to efficiently uncover insights from video content while maintaining robust performance as data scales.

**Step-by-step guide**: Our blog post, [Unleashing Video Intelligence: A Tutorial for Integrating TwelveLabs Multimodal Embed API with Oracle Database 23ai](https://www.twelvelabs.io/blog/twelve-labs-and-oracle-23ai), provides a detailed walkthrough of the integration process.

**GitHub repo**: [Twelve Labs Oracle DB Integration](https://github.com/twelvelabs-io/twelvelabs-developer-experience/tree/main/integrations/Oracle)


# From the community

This section features a range of applications built by developers using the TwelveLabs Video Understanding Platform. These applications illustrate the platform's versatility in diverse industries:

* [E-learning](/docs/resources/from-the-community/e-learning)
* [Social and public goods](/docs/resources/from-the-community/social-and-public-goods)
* [Interactive content](/docs/resources/from-the-community/interactive-content)
* [Sports](/docs/resources/from-the-community/sports)
* [Media and entertainment](/docs/resources/from-the-community/media-and-entertainment)

# Feature your application

Have you built something extraordinary with our video understanding platform? We are always looking for groundbreaking applications that push the boundaries of what's possible. If you have an application that deserves a spot in the limelight, don't hesitate to reach out. Contact us to get your application featured.


# E-learning

The example projects on this page demonstrate using the TwelveLabs Video Understanding Platform to enhance educational experiences. These projects address complex topics in education and pave the way for the future of AI-powered e-learning solutions.

# AI'm Right

**Summary**: AI'm Right is an educational application developed by [Muhammad Adil Fayya](https://www.linkedin.com/in/muhammad-adil-fayyaz/) during [SBHacks’25](https://www.linkedin.com/company/sb-hacks/?lipi=urn%3Ali%3Apage%3Ad_flagship3_detail_base%3BMQWTGKmXR32dcnvQoUkWRA%3D%3D) at the University of California, Santa Barbara. It leverages artificial intelligence to streamline studying through quiz generation, lecture summarization, and video content search capabilities.

**Description**: The application integrates multiple AI services through a Flask backend that manages the processing operations and a Streamlit frontend that provides the user interface.:

* TwelveLabs for video understanding
* Claude for content processing,
* Aryn for additional functionality

These services are integrated through a Flask backend that manages the processing operations and a Streamlit frontend that provides the user interface.

**GitHub repo**: [darshanrao/RAG-Enhanced-Learning-Assistant](https://github.com/darshanrao/RAG-Enhanced-Learning-Assistant)

## Integration with TwelveLabs

AI'm Right integrates with the Labs Video Understanding Platform for searching and retrieving video content, enabling you to locate specific topics or concepts within your recorded lectures without manually scanning through entire videos.

The `query`  function uses the TwelveLabs Python SDK to perform one or more search queries. For each query, it retrieves paginated search results that include video IDs, confidence scores, and timestamp information. The function can optionally save these results to a JSON file and returns them as a nested list of dictionaries.

```python Python
def query(self, query_text_list, file_name=None):
   all_results = []  # List of lists of dictionaries


   for query_text in query_text_list:
       search_results = self.client.search.query(
           index_id=os.getenv("INDEX_ID"),
           query_text=query_text,
           # query={"text": query_text},
          
           # query = {
           #     "$not": {
           #         "origin": {
           #             "text": query_text
           #         },
           #         "sub": {
           #             "$or": [
           #                 {
           #                     "text": "News Anchor"
           #                 },
           #                 {
           #                     "text": "Podcast"
           #                 }
           #             ]
           #         }
           #     }
           # },


           options=["visual","audio"]
       )
      
       query_result = []  # List of dictionaries for this query
       while True:
          
           try:
               # Retrieve each page of search results
               page_data = next(search_results)
               for clip in page_data:  # Limiting to 3 results per page
                   query_result.append({
                       "video_id": clip.video_id,
                       "score": clip.score,
                       "start": clip.start,
                       "end": clip.end,
                       "confidence": clip.confidence,
                       # "metadata": clip.metadata,
                   })
           except StopIteration:
               break
       all_results.append(query_result)  # Append this query's results as a list


   if file_name:
       self.save_json(all_results, file_name)


   return all_results  # List of lists of dictionaries

```

The `get_video_info` function  retrieves detailed information about a specific video using its ID. It returns the complete video metadata, including the HLS streaming URL.

```python Python
def get_video_info(video_id):
   api_key= os.getenv("TWELVE_LABS_KEY")


   index_id=os.getenv("INDEX_ID")
   url = f"https://api.twelvelabs.io/v1.3/indexes/{index_id}/videos/{video_id}"
   headers = {
       "x-api-key": api_key,
       "Content-Type": "application/json",
   }
  
   response = requests.get(url, headers=headers)


   if response.status_code == 200:
       video_info = response.json()  # Parse response as JSON
       return video_info
   elif response.status_code == 400:
       print("The request has failed. Check your parameters.")
       return None
   else:
       print(f"Error: Status code {response.status_code}")
       return None

```

# NeuroLearn

**Summary**:  NeuroLearn is an AI-based learning platform that integrates neurofeedback to personalize education by creating.

**Description**: The application analyzes brainwave data to select and emphasize parts of lectures that match the student's educational objectives. This approach keeps students engaged and improves their learning results. Developed by Akhil Dhavala, Ayush Khandelwal, Jackson Mowatt Gok, and Jacky Wong, NeuroLearn won 1st Place at the [TEDAI Multimodal Hackathon (23 Labs)](https://app.twelvelabs.io/blog/introducing-the-multimodal-ai-23labs-hackathon) in San Francisco.

**GitHub repo**: [NeuroLearn](https://github.com/ayushkhd/neurolearn)

## Integration with TwelveLabs

NeuroLearn uses the TwelveLabs Video Understanding Platform to create custom snippets from lectures tailored to each student's knowledge, background, and interests.

The code below indexes videos from YouTube and for each video generates a summary and highlights:

```python Python
    def __init__(self):
        self.api_url = os.getenv("TWELVE_LABS_BASE_URL")
        self.headers = {"x-api-key": os.getenv("TWELVE_LABS_API_KEY")}

    def index_youtube_video(self, index_id: str, youtube_url: str):
        task_url = f"{self.api_url}/tasks/external-provider"
        data = {
            "index_id": index_id,
            "url": youtube_url,
        }
        response = requests.post(task_url, headers=self.headers, json=data)
        # you can get the video ID from video_id
        return response.json()

    def highlight_video(self, body: HighlightVideoBody) -> HighlightVideoResponse:
        url = f"{self.api_url}/summarize"
        data = body.dict()
        response = requests.post(url, headers=self.headers, json=data)
        return HighlightVideoResponse(**response.json())

    def summarize(self, body: HighlightVideoBody) -> HighlightVideoResponse:
        url = f"{self.api_url}/summarize"
        data = body.dict()
        response = requests.post(url, headers=self.headers, json=data)
        return response.json()
```

# 42Labs - Personalized Podcast Builder

**Summary**: The 42 Labs Personalized Podcast Builder transforms how you learn on the go by synthesizing high-quality content from diverse sources like TED talks, podcasts, and articles into customized podcasts.

**Description**: The application analyzes your preferences, including topics of interest and proficiency levels, to curate content in various languages. You can interactively refine your learning experience by selecting subtopics and providing feedback, enhancing the application's ability to offer relevant material. The application not only overcomes language and learning barriers but also holds the potential to evolve into a tool for creating personalized educational videos. The application was developed by Shivani Poddar, David Salib, Varun Theja, and Everett Knag.

**GitHub**:

* **Frontend**: TEDAI\_2023.
* **Backend**: 42labs-answer-to-everything.

## Integration with TwelveLabs

The application uses the `/classify/bulk` endpoint to identify videos relevant to a specified topic:

```python
def classify_videos(index_id, sub_topic, api_key):
    CLASSIFY_BULK_URL = f"{url}/classify/bulk"

    data =  {
    "options": ["conversation", "text_in_video"],
    "index_id": index_id,
    "classes": [{"name": sub_topic,
        "prompts": [
            sub_topic,
        ]}]
    }
    headers = {
        "accept": "application/json",
        "Content-Type": "application/json",
        "x-api-key": api_key
    }

    response = requests.post(CLASSIFY_BULK_URL, headers=headers, json=data)
    print (f'Status code: {response.status_code}')
    print(response.json())
    return response.json()["data"]
```

For each relevant video, the application invokes the `/summarize` endpoint to summarize videos as lists  of chapters:

```python
def summarize_video(video_id, api_key):
    payload = {
        "type": "chapter",
        "video_id": video_id,
    }
    headers = {
        "accept": "application/json",
        "x-api-key": api_key,
        "Content-Type": "application/json"
    }

    url = "https://api.twelvelabs.io/v1.2/summarize"
    response = requests.post(url, json=payload, headers=headers)

    print(response.text)
    chapter_summaries = [x["chapter_summary"] for x in response.json()["chapters"]]
    return chapter_summaries
```


# Social and public goods

The example projects on this page utilize the TwelveLabs Video Understanding Platform to create social and public goods. These projects demonstrate how multimodal AI can drive positive changes, exemplifying its transformative power.

# Chowtown

**Summary**: Chowtown connects users with authentic Chinese restaurants and educates them about traditional Chinese ingredients.

**Description**: Chowtown provides an intuitive platform that features detailed descriptions of restaurants, menus, and a recommendation system tailored to user preferences. The application also encourages exploration through a user-friendly interface built with Next.js, Shadcn, and MagicUI. The backend utilizes Groq and ChatGPT-4 for generative content, while TwelveLabs offers video understanding capabilities to analyze and categorize video content.

The application was developed by Andrew Lau, Jiaqiao Situ, lilalin808 Lin, and Kayliann Sin, and it has won the Best Use of Twelve Labs Jockey award at the [Chinatown Hacks](https://chinatown-hacks.devpost.com/?_gl=1*1768z6t*_ga_0YHJK3Y10M*MTc0MjgwMTY5Ni4xLjAuMTc0MjgwMTc1NS4wLjAuMA..) hackathon.

**GitHub repo**: [andrewlau624/chinatown-hacks-restaurant-project](https://github.com/andrewlau624/chinatown-hacks-restaurant-project)

**Demo**: [Chowtown](https://chowtown.vercel.app).

## Integration with TwelveLabs

Chowtown utilizes the TwelveLabs Video Understanding Platform to to search based on user queries and retrieve video playback links. The following code snippets illustrate key integration points.

### Search video content

The `returnSearchData` function uses the `client.search.query` method to search video content based on user queries. It processes both visual and audio elements, returning relevant video segments that match the search criteria.

```JavaScript Node.js
export async function returnSearchData(data: string) {
    const result = await client.search.query({
        indexId: process.env.TWELVELABS_INDEX_ID!,
        queryText: data,
        options: ["visual", "audio"]
    });

    return result.data
}
```

### Retrieve video links

The `returnVideoData` function retrieves HLS (HTTP Live Streaming) links using the `client.index.video.retrieve` method. These URLs are used within the application for video playback,  providing users with direct access to the video clips that match their search criteria.

```JavaScript Node.js
export async function returnVideoData(data: any) {
    const videoLinks = await client.index.video.retrieve(
        process.env.TWELVELABS_INDEX_ID!,
        data.content[0].videoId,
        { embed: true }
    );

    return videoLinks.hls
}
```

# Israel Palestine Video Understanding

**Summary**: The "Israel Palestine Video Understanding" application addresses misinformation and promotes empathy regarding the Israel-Palestine conflict.

**Description**: The application aggregates and summarizes content from YouTube and Reddit, presenting diverse viewpoints on the issue. These summaries, covering a range of opinions, are then visualized using an algorithm similar to T-SNE , offering a comprehensive understanding of the conflict's various perspectives. The application was developed by Sasha Sheng.

**GitHub repo**: Israel Palestine Video Understanding

## Integration with TwelveLabs

This application invokes the `/summarize` endpoint to create summaries for videos based on their content, specifically focusing on their stance regarding the Israel-Palestine conflict and the level of violence depicted:

```python
def generate_summary(videoID, videoID_to_filename):
    SUMMARIZE_URL = f"{API_URL}/summarize"
    headers = {
        "x-api-key": API_KEY
    }

    data = {
      "video_id": videoID,
      "type": "summary",
      "prompt": "Summarize if this video is pro-israel or pro-palestine or else and how violent it is."
    }

    response = requests.post(SUMMARIZE_URL, headers=headers, json=data)
    print(f"{videoID}: status code - {response.status_code}")

    summary_data = response.json()
    print(summary_data)

    with open(filename, 'a') as f:
        writer = csv.writer(f, delimiter='\t')
        writer.writerow([videoID, videoID_to_filename[videoID][0], videoID_to_filename[videoID][1], summary_data.get('summary')])
```

# Accelerate SF Notifications

**Summary**: The "Accelerate SF Notifications" application simplifies public hearings for residents and special interest groups, particularly those focused on San Francisco housing developments.

**Description**: The application addresses the challenge of keeping up with numerous and lengthy public hearings, where the critical issue is identifying relevant discussions without watching entire meetings. The application was developed by Rahul Pal, Lloyd Chang, and Haonan Chen.

Key features include:

* **Data scraping**: Extract information from public agendas, live-streamed hearings, and sources like San Francisco Gov TV.
* **Issue tracking**: Utilize algorithms to pinpoint and extract discussions about housing projects and specific issues within hearings.
* Automated notifications: Implement a system that sends real-time alerts.

**GitHub repo**: Accelerate SF Notifications

## Integration with TwelveLabs

The application uses the `/summarize` endpoint to perform the following main functions: summarize videos and generate lists of chapters.

* A summary encapsulates the key points of a video clearly. The code below shows how the application generates summaries:

  ```python

  data = {
      "video_id": "6545f931195730422cc38329",
      "type": "summary"
  }

  # Send request
  response = requests.post(f"{BASE_URL}/summarize", json=data, headers={"x-api-key": api_key})

  ```

* A list of chapters provides a chronological breakdown of all the parts in a video. The following code shows how the application generates lists of chapters:

  ```python
  data = {
      "video_id": "6545f931195730422cc38329",
      "type": "chapter"
  }

  # Send request
  response = requests.post(f"{BASE_URL}/summarize", json=data, headers={"x-api-key": api_key})
  ```

The `/gist` endpoint generates swift breakdowns of the essence of your videos in the form of titles, topics, and hashtags. The following code shows how the application invokes this endpoint:

```python
data = {
    "video_id": "6545f931195730422cc38329",
    "types": [
        "title",
        "hashtag",
        "topic"
    ]
}

# Send request
response = requests.post(f"{BASE_URL}/gist", json=data, headers={"x-api-key": api_key})
```

# Deep Green

**Summary**: The "Dep Green" application uses the TwelveLabs Video Understanding Platform to accurately detect and map ocean trash using aerial and satellite imagery.

**Description**: The application offers a solution to the problem of plastic pollution in the oceans. It detects different types of ocean trash with over 90% accuracy and can scan over 500 hours of video daily. Trash is timestamped and geographically pinpointed, allowing easy data analysis and export. The application was developed by Shalini Ananda and Hans Walker.

**GitHub**: Deep Green

## Integration with TwelveLabs

The `search_trash` function searches for videos containing specific types of trash, returning a list of such videos with key information about each:

```python
def search_trash(query, API_KEY):

      data = {
            "query": query,
            "index_id": INDEX_ID,
            "search_options": ["visual"]
        }

      response = requests.post(f"{API_URL}/search", headers={"x-api-key": API_KEY}, json=data)

      response = response.json()


      results = []

      # Getting thumbnail and relevant data
      for i in range(len(response['data'])):
            score = response['data'][i]['score']
            video_id = response['data'][i]['video_id']
            video_location = Get_Video_Metadata(response['data'][i]['video_id'],API_KEY)['Location Type']
            thumbnail_url = response['data'][i]['thumbnail_url']
            results.append({"score": score,"video_location": video_location, "video_id": video_id, "thumbnail_url": thumbnail_url})

        return results
```

The `search_video_single` function finds specific content within a single video:

```python
def search_video_single(video_id, query, API_KEY):


    headers = {
    "accept": "application/json",
    "x-api-key": API_KEY,
    "Content-Type": "application/json"}
    
    data = {
    "query": query,
    "search_options": ["visual", "conversation", "text_in_video", "logo"],
    "threshold": "high",
    "filter": { "id": [video_id] },
    "index_id": INDEX_ID }


    response = requests.post(f"{API_URL}/search", headers=headers, json=data)

    results = []

    for i in range(len(response.json()['data'])):
        score = response.json()['data'][i]['score']
        video_id = response.json()['data'][i]['video_id']
        results.append({"score": score, 'start_time':response.json()['data'][i]['start'], 
                        'end_time':response.json()['data'][i]['end']})
    
```

The `classify_latest_video` function classifies videos into specific environmental categories:

```python
def classify_latest_video(id, file_name, API_KEY):
    classify_url = f"{API_URL}/classify"
    file_name = file_name.split('.')[0]

    video_list = get_video_list(API_KEY)
    time_initiated = time.time()
    video_uploaded=True
    video_index = 0
    for i, next_video in enumerate(video_list):
        if(next_video['metadata']['filename']==file_name):
            video_uploaded=False
            video_index = i
            break
    while(video_uploaded):
        time.sleep(60)
        video_list = get_video_list(API_KEY)
        for i, next_video in enumerate(video_list):
            print(next_video['metadata']['filename'],"   ",file_name)
            if(next_video['metadata']['filename']==file_name):
                video_uploaded=False
                video_index = i
                break
    
    id = video_list[video_index]["_id"]

    meta_url = f"{API_URL}/indexes/{INDEX_ID}/videos/{id}"

    print("\n\nStarting Metadata",time.time()-time_initiated,"\n\n", file=sys.stderr)
    payload = {
        "page_limit": 10,
        "include_clips": False,
        "threshold": {
            "min_video_score": 15,
            "min_clip_score": 15,
            "min_duration_ratio": 0.5
        },
        "show_detailed_score": False,
        "options": ["conversation"],
        "conversation_option": "semantic",
        "classes": [
            {
                "prompts": ["This video is taken in an urban enviorment", "This means a dense environment", "Lots of people, cars and buildings"],
                "options": ["visual"],
                "conversation_option": "semantic",
                "name": "Urban"
            },
            {
                "prompts": ["This video is taken in a suburban enviorment", "There should be buildings, roads", "Everything should be a lot more spread out", "The majority of the space should be developed"],
                "options": ["visual"],
                "conversation_option": "semantic",
                "name": "Suburban"
            },
            {
                "prompts": ["This video was taken in a rural enviorment", "There shouldn't be a ton of human development", "Buildings should be extremly spread out", "Should mostly be nature", "Very few humans around"],
                "options": ["visual"],
                "conversation_option": "semantic",
                "name": "Rural"
            }
        ],
        "video_ids": [id]
    }
    headers = {
        "accept": "application/json",
        "x-api-key": API_KEY,
        "Content-Type": "application/json"
    }

    response = requests.post(classify_url, json=payload, headers=headers)
    response = response.json()
    
    print(response, file=sys.stderr)
    video_class = response['data'][0]['classes'][0]['name']

    payload = { "metadata": { "Location Type": video_class } }
    headers = {
        "accept": "application/json",
        "x-api-key": API_KEY,
        "Content-Type": "application/json"
    }

    response = requests.put(meta_url, json=payload, headers=headers)
```

# RememberMe - Dementia Assistant

**Summary**: The project addresses the critical challenge of assisting individuals with dementia in retaining their independence and enhancing their quality of life.

**Description**: The application is a comprehensive digital support system with a home screen displaying the current date, important reminders, and action buttons. It also has a chatbot that users can use to ask questions about their lives. The application collects data such as video, audio, and personal notes, and it utilizes the TwelveLabs Video Understanding Platform to convert multimedia information into text for the chatbot database's organizational and storage purposes. The objective is to provide a seamless and intuitive platform that enables users to recall important details about their lives, manage daily tasks, and maintain connections with people and places that matter to them. The application was developed by Tatiane Wu Li, Pedro Goncalves de Paiva, Aleksei (Alex) Korablev, and Na Le.

**GitHub**: RememberMe.

**Presentation**: RememberMe.

## Integration with TwelveLabs

The `submit_video_for_processing` function uploads a video to the platform by invoking the `POST` method of the `/tasks/external-provider` endpoint. Upon receiving the response, the function processes it to determine the outcome. If the upload is successful, the function returns the unique identifier of the submitted task. In case of an error, the function returns an error message that details the specific reason for the failure. This helps developers identify and resolve any issues with the video upload process.

```python
import requests
from pprint import pprint

# Constants
API_URL = "https://api.twelvelabs.io/v1.2"
API_KEY = ""
INDEX_ID = ""  # Replace with your actual index ID obtained from creating an index

# Function to submit a video URL for processing by an external provider
def submit_video_for_processing(video_url):
    """Submit a video URL to an external processing service and return the task ID."""
    TASKS_URL = f"{API_URL}/tasks/external-provider"
    headers = {"x-api-key": API_KEY}
    data = {"index_id": INDEX_ID, "url": video_url}
    response = requests.post(TASKS_URL, headers=headers, json=data)
    if response.status_code == 201:
        task_id = response.json().get("_id")
        print(f"Task submitted successfully. Task ID: {task_id}")
        return task_id
    else:
        print(f"Failed to submit task: {response.status_code}")
        pprint(response.json())
        return None

# Example usage
video_url = "https://www.youtube.com/watch?v=TLwhqmf4Td4&ab_channel=RGSACHIN"
task_id = submit_video_for_processing(video_url)
```

The `get_video_summary` function takes the unique identifier of a video as a parameter and invokes the `POST` method of the `/generate` endpoint to summarize it. If successful, it returns the generated summary; otherwise, it prints an error message and returns `None`.

```python
def get_video_summary(video_id):
    GENERATE_URL = f"{API_URL}/generate"  # Define the URL to generate the summary
    data = {"video_id": video_id, "prompt": "Make a summary"}  # Set up the data payload
    response = requests.post(GENERATE_URL, headers=headers, json=data)  # Make the POST request
    if response.status_code == 200:
        summary = response.json().get('data')  # Get the summary data from the response
        print("Video summary generated successfully.")
        return summary  # Return the summary
    else:
        print(f"Failed to generate summary: {response.status_code}")  # Print failure message
        pprint(response.json())
        return None  # Return None if summary generation fails
```

# CamSense AI

**Summary**: "CamSense AI" is an AI-powered application that assesses webcam videos, providing instant insights and alerts. It uses the TwelveLabs Video Understanding Platform to analyze video content and identify significant changes or events.

**Description**: The application addresses the challenge of custom trigger creation based on content understanding of unattended recorded video. This solution is particularly useful in ecology, fire safety, and flood water level monitoring.

The typical workflow is as follows:

1. The TwelveLabs Video Understanding Platform generates embeddings for the reference frame and the subsequent video clips and summarizes them.
2. The application uses Groq to produce natural language descriptions of the differences.
3. The application determines the significance of these differences.
4. Clips that differ significantly are logged along with their timestamps and descriptions.
5. The process concludes with the aggregation of all logs into a final report

The application was developed by Daniel Talero, Paul Kubie, and Todd Gardiner.

**Colab notebook**: [hackathon.ipynb](https://colab.research.google.com/drive/12lisa64aAT2wlM32xXk8E5FLCHmDIlnd?usp=sharing#scrollTo=jo9a1Axp_Uq5) .

## Integration with TwelveLabs

The code below creates a video indexing task that uploads a video to the TwelveLabs Video Understanding Platform by invoking the `create` method of the `task` object:

```python
video_files = glob(reference_filename) # Example: "/videos/*.mp4


print(f"Uploading {reference_filename}")
task = client.task.create(index_id=index_obj.id, file=reference_filename, language="en")
print(f"Task id={task.id}")
print(f"Task_video_id = {task.video_id}")
ref_id = task.video_id


frame_id = []
if len(rawvids) > 0 :
 for i in range(len(rawvids)):
   video_files = glob( ("/content/rawdata/" + str(rawvids[i]) ) ) # Example: "/videos/*.mp4
   print(f"Uploading {rawvids[i]}")
   task = client.task.create(index_id=index_obj.id, file= ("/content/rawdata/" + str(rawvids[i]) ) , language="en")
   print(f"Task id={task.id}")
   frame_id.append(task.video_id)
```

The code below invokes the `create` method of the `embed.task` object to create an embedding for the reference frame:

```python
task = client.embed.task.create(
   engine_name="Marengo-retrieval-2.7",
   video_url="https://storage.googleapis.com/lab-storage-items/sample-5s.mp4")
print(
   f"Created task: id={task.id} engine_name={task.engine_name} status={task.status}"
)


def on_task_update(task: EmbeddingsTask):
   print(f"  Status={task.status}")


status = task.wait_for_done(
 sleep_interval=5,
 callback=on_task_update
 )
print(f"Embedding done: {status}")
task = client.embed.task.retrieve(task.id)
if task.video_embeddings is not None:
   for v in task.video_embeddings:
       print(
           f"embedding_scope={v.embedding_scope} start_offset_sec={v.start_offset_sec} end_offset_sec={v.end_offset_sec}"
       )
       print(f"embeddings: {', '.join([str(x) for x in v.embedding.float])}")
       ref_emb = np.array([str(x) for x in v.embedding.float])

```

The code below creates embeddings for the subsequent frames:

```python
fref_emb = []
for i in range(len(rawvids)):


 task = client.embed.task.create(
     engine_name="Marengo-retrieval-2.7",
     video_url="https://storage.googleapis.com/lab-storage-items/sample-5s.mp4")
 print(
     f"Created task: id={task.id} engine_name={task.engine_name} status={task.status}"
 )

 status = task.wait_for_done(
   sleep_interval=5,
   callback=on_task_update
   )
 print(f"Embedding done: {status}")
 task = client.embed.task.retrieve(task.id)
 if task.video_embeddings is not None:
     for v in task.video_embeddings:
         print(
             f"embedding_scope={v.embedding_scope} start_offset_sec={v.start_offset_sec} end_offset_sec={v.end_offset_sec}"
         )
         print(f"embeddings: {', '.join([str(x) for x in v.embedding.float])}")
         fref_emb.append(np.array([str(x) for x in v.embedding.float]))

```

The code below invokes the `summarize` method of the `generate` object to summarize the reference frame:

```python
res = client.generate.summarize(ref_id, type='summary', prompt="In a detailed way, describe this video clip." )
```

The code below summarizes each subsequent video and stores the results in a list:

```python
The code below summarizes each subsequent video and stores the results in a list:
fres = []
fres_emb = []
for i in range(len(rawvids)):
 res2 = client.generate.summarize(frame_id[i], type='summary', prompt="In a detailed way, describe this video clip." )
 fres.append(res2)
 fres_emb.append(model.encode(res2.summary))

```


# Interactive content

The example projects on this page utilize the TwelveLabs Video Understanding Platform to create social and public goods. These projects demonstrate how multimodal AI can drive positive changes, exemplifying its transformative power.

# Israel Palestine Video Understanding

**Summary**: The "Israel Palestine Video Understanding" application addresses misinformation and promotes empathy regarding the Israel-Palestine conflict.

**Description**: The application aggregates and summarizes content from YouTube and Reddit, presenting diverse viewpoints on the issue. These summaries, covering a range of opinions, are then visualized using an algorithm similar to T-SNE , offering a comprehensive understanding of the conflict's various perspectives. The application was developed by Sasha Sheng.

**GitHub repo**: Israel Palestine Video Understanding

## Integration with TwelveLabs

This application invokes the `/summarize` endpoint to create summaries for videos based on their content, specifically focusing on their stance regarding the Israel-Palestine conflict and the level of violence depicted:

```python
def generate_summary(videoID, videoID_to_filename):
    SUMMARIZE_URL = f"{API_URL}/summarize"
    headers = {
        "x-api-key": API_KEY
    }

    data = {
      "video_id": videoID,
      "type": "summary",
      "prompt": "Summarize if this video is pro-israel or pro-palestine or else and how violent it is."
    }

    response = requests.post(SUMMARIZE_URL, headers=headers, json=data)
    print(f"{videoID}: status code - {response.status_code}")

    summary_data = response.json()
    print(summary_data)

    with open(filename, 'a') as f:
        writer = csv.writer(f, delimiter='\t')
        writer.writerow([videoID, videoID_to_filename[videoID][0], videoID_to_filename[videoID][1], summary_data.get('summary')])
```

# Accelerate SF Notifications

**Summary**: The "Accelerate SF Notifications" application simplifies public hearings for residents and special interest groups, particularly those focused on San Francisco housing developments.

**Description**: The application addresses the challenge of keeping up with numerous and lengthy public hearings, where the critical issue is identifying relevant discussions without watching entire meetings. The application was developed by Rahul Pal, Lloyd Chang, and Haonan Chen.

Key features include:

* **Data scraping**: Extract information from public agendas, live-streamed hearings, and sources like San Francisco Gov TV.
* **Issue tracking**: Utilize algorithms to pinpoint and extract discussions about housing projects and specific issues within hearings.
* Automated notifications: Implement a system that sends real-time alerts.

**GitHub repo**: Accelerate SF Notifications

## Integration with TwelveLabs

The application uses the `/summarize` endpoint to perform the following main functions: summarize videos and generate lists of chapters.

* A summary encapsulates the key points of a video clearly. The code below shows how the application generates summaries:

  ```python

  data = {
      "video_id": "6545f931195730422cc38329",
      "type": "summary"
  }

  # Send request
  response = requests.post(f"{BASE_URL}/summarize", json=data, headers={"x-api-key": api_key})

  ```

* A list of chapters provides a chronological breakdown of all the parts in a video. The following code shows how the application generates lists of chapters:

  ```python
  data = {
      "video_id": "6545f931195730422cc38329",
      "type": "chapter"
  }

  # Send request
  response = requests.post(f"{BASE_URL}/summarize", json=data, headers={"x-api-key": api_key})
  ```

The `/gist` endpoint generates swift breakdowns of the essence of your videos in the form of titles, topics, and hashtags. The following code shows how the application invokes this endpoint:

```python
data = {
    "video_id": "6545f931195730422cc38329",
    "types": [
        "title",
        "hashtag",
        "topic"
    ]
}

# Send request
response = requests.post(f"{BASE_URL}/gist", json=data, headers={"x-api-key": api_key})
```

# Deep Green

**Summary**: The "Dep Green" application uses the TwelveLabs Video Understanding Platform to accurately detect and map ocean trash using aerial and satellite imagery.

**Description**: The application offers a solution to the problem of plastic pollution in the oceans. It detects different types of ocean trash with over 90% accuracy and can scan over 500 hours of video daily. Trash is timestamped and geographically pinpointed, allowing easy data analysis and export. The application was developed by Shalini Ananda and Hans Walker.

**GitHub**: Deep Green

## Integration with TwelveLabs

The `search_trash` function searches for videos containing specific types of trash, returning a list of such videos with key information about each:

```python
def search_trash(query, API_KEY):

      data = {
            "query": query,
            "index_id": INDEX_ID,
            "search_options": ["visual"]
        }

      response = requests.post(f"{API_URL}/search", headers={"x-api-key": API_KEY}, json=data)

      response = response.json()


      results = []

      # Getting thumbnail and relevant data
      for i in range(len(response['data'])):
            score = response['data'][i]['score']
            video_id = response['data'][i]['video_id']
            video_location = Get_Video_Metadata(response['data'][i]['video_id'],API_KEY)['Location Type']
            thumbnail_url = response['data'][i]['thumbnail_url']
            results.append({"score": score,"video_location": video_location, "video_id": video_id, "thumbnail_url": thumbnail_url})

        return results
```

The `search_video_single` function finds specific content within a single video:

```python
def search_video_single(video_id, query, API_KEY):


    headers = {
    "accept": "application/json",
    "x-api-key": API_KEY,
    "Content-Type": "application/json"}
    
    data = {
    "query": query,
    "search_options": ["visual", "conversation", "text_in_video", "logo"],
    "threshold": "high",
    "filter": { "id": [video_id] },
    "index_id": INDEX_ID }


    response = requests.post(f"{API_URL}/search", headers=headers, json=data)

    results = []

    for i in range(len(response.json()['data'])):
        score = response.json()['data'][i]['score']
        video_id = response.json()['data'][i]['video_id']
        results.append({"score": score, 'start_time':response.json()['data'][i]['start'], 
                        'end_time':response.json()['data'][i]['end']})
    
```

The `classify_latest_video` function classifies videos into specific environmental categories:

```python
def classify_latest_video(id, file_name, API_KEY):
    classify_url = f"{API_URL}/classify"
    file_name = file_name.split('.')[0]

    video_list = get_video_list(API_KEY)
    time_initiated = time.time()
    video_uploaded=True
    video_index = 0
    for i, next_video in enumerate(video_list):
        if(next_video['metadata']['filename']==file_name):
            video_uploaded=False
            video_index = i
            break
    while(video_uploaded):
        time.sleep(60)
        video_list = get_video_list(API_KEY)
        for i, next_video in enumerate(video_list):
            print(next_video['metadata']['filename'],"   ",file_name)
            if(next_video['metadata']['filename']==file_name):
                video_uploaded=False
                video_index = i
                break
    
    id = video_list[video_index]["_id"]

    meta_url = f"{API_URL}/indexes/{INDEX_ID}/videos/{id}"

    print("\n\nStarting Metadata",time.time()-time_initiated,"\n\n", file=sys.stderr)
    payload = {
        "page_limit": 10,
        "include_clips": False,
        "threshold": {
            "min_video_score": 15,
            "min_clip_score": 15,
            "min_duration_ratio": 0.5
        },
        "show_detailed_score": False,
        "options": ["conversation"],
        "conversation_option": "semantic",
        "classes": [
            {
                "prompts": ["This video is taken in an urban enviorment", "This means a dense environment", "Lots of people, cars and buildings"],
                "options": ["visual"],
                "conversation_option": "semantic",
                "name": "Urban"
            },
            {
                "prompts": ["This video is taken in a suburban enviorment", "There should be buildings, roads", "Everything should be a lot more spread out", "The majority of the space should be developed"],
                "options": ["visual"],
                "conversation_option": "semantic",
                "name": "Suburban"
            },
            {
                "prompts": ["This video was taken in a rural enviorment", "There shouldn't be a ton of human development", "Buildings should be extremly spread out", "Should mostly be nature", "Very few humans around"],
                "options": ["visual"],
                "conversation_option": "semantic",
                "name": "Rural"
            }
        ],
        "video_ids": [id]
    }
    headers = {
        "accept": "application/json",
        "x-api-key": API_KEY,
        "Content-Type": "application/json"
    }

    response = requests.post(classify_url, json=payload, headers=headers)
    response = response.json()
    
    print(response, file=sys.stderr)
    video_class = response['data'][0]['classes'][0]['name']

    payload = { "metadata": { "Location Type": video_class } }
    headers = {
        "accept": "application/json",
        "x-api-key": API_KEY,
        "Content-Type": "application/json"
    }

    response = requests.put(meta_url, json=payload, headers=headers)
```

# RememberMe - Dementia Assistant

**Summary**: The project addresses the critical challenge of assisting individuals with dementia in retaining their independence and enhancing their quality of life.

**Description**: The application is a comprehensive digital support system with a home screen displaying the current date, important reminders, and action buttons. It also has a chatbot that users can use to ask questions about their lives. The application collects data such as video, audio, and personal notes, and it utilizes the TwelveLabs Video Understanding Platform to convert multimedia information into text for the chatbot database's organizational and storage purposes. The objective is to provide a seamless and intuitive platform that enables users to recall important details about their lives, manage daily tasks, and maintain connections with people and places that matter to them. The application was developed by Tatiane Wu Li, Pedro Goncalves de Paiva, Aleksei (Alex) Korablev, and Na Le.

**GitHub**: RememberMe.

**Presentation**: RememberMe.

## Integration with TwelveLabs

The `submit_video_for_processing` function uploads a video to the platform by invoking the `POST` method of the `/tasks/external-provider` endpoint. Upon receiving the response, the function processes it to determine the outcome. If the upload is successful, the function returns the unique identifier of the submitted task. In case of an error, the function returns an error message that details the specific reason for the failure. This helps developers identify and resolve any issues with the video upload process.

```python
import requests
from pprint import pprint

# Constants
API_URL = "https://api.twelvelabs.io/v1.2"
API_KEY = ""
INDEX_ID = ""  # Replace with your actual index ID obtained from creating an index

# Function to submit a video URL for processing by an external provider
def submit_video_for_processing(video_url):
    """Submit a video URL to an external processing service and return the task ID."""
    TASKS_URL = f"{API_URL}/tasks/external-provider"
    headers = {"x-api-key": API_KEY}
    data = {"index_id": INDEX_ID, "url": video_url}
    response = requests.post(TASKS_URL, headers=headers, json=data)
    if response.status_code == 201:
        task_id = response.json().get("_id")
        print(f"Task submitted successfully. Task ID: {task_id}")
        return task_id
    else:
        print(f"Failed to submit task: {response.status_code}")
        pprint(response.json())
        return None

# Example usage
video_url = "https://www.youtube.com/watch?v=TLwhqmf4Td4&ab_channel=RGSACHIN"
task_id = submit_video_for_processing(video_url)
```

The `get_video_summary` function takes the unique identifier of a video as a parameter and invokes the `POST` method of the `/generate` endpoint to summarize it. If successful, it returns the generated summary; otherwise, it prints an error message and returns `None`.

```python
def get_video_summary(video_id):
    GENERATE_URL = f"{API_URL}/generate"  # Define the URL to generate the summary
    data = {"video_id": video_id, "prompt": "Make a summary"}  # Set up the data payload
    response = requests.post(GENERATE_URL, headers=headers, json=data)  # Make the POST request
    if response.status_code == 200:
        summary = response.json().get('data')  # Get the summary data from the response
        print("Video summary generated successfully.")
        return summary  # Return the summary
    else:
        print(f"Failed to generate summary: {response.status_code}")  # Print failure message
        pprint(response.json())
        return None  # Return None if summary generation fails
```

# CamSense AI

**Summary**: "CamSense AI" is an AI-powered application that assesses webcam videos, providing instant insights and alerts. It uses the TwelveLabs Video Understanding Platform to analyze video content and identify significant changes or events.

**Description**: The application addresses the challenge of custom trigger creation based on content understanding of unattended recorded video. This solution is particularly useful in ecology, fire safety, and flood water level monitoring.

The typical workflow is as follows:

1. The TwelveLabs Video Understanding Platform generates embeddings for the reference frame and the subsequent video clips and summarizes them.
2. The application uses Groq to produce natural language descriptions of the differences.
3. The application determines the significance of these differences.
4. Clips that differ significantly are logged along with their timestamps and descriptions.
5. The process concludes with the aggregation of all logs into a final report

The application was developed by Daniel Talero, Paul Kubie, and Todd Gardiner.

**Colab notebook**: [hackathon.ipynb](https://colab.research.google.com/drive/12lisa64aAT2wlM32xXk8E5FLCHmDIlnd?usp=sharing#scrollTo=jo9a1Axp_Uq5) .

## Integration with TwelveLabs

The code below creates a video indexing task that uploads a video to the TwelveLabs Video Understanding Platform by invoking the `create` method of the `task` object:

```python
video_files = glob(reference_filename) # Example: "/videos/*.mp4


print(f"Uploading {reference_filename}")
task = client.task.create(index_id=index_obj.id, file=reference_filename, language="en")
print(f"Task id={task.id}")
print(f"Task_video_id = {task.video_id}")
ref_id = task.video_id


frame_id = []
if len(rawvids) > 0 :
 for i in range(len(rawvids)):
   video_files = glob( ("/content/rawdata/" + str(rawvids[i]) ) ) # Example: "/videos/*.mp4
   print(f"Uploading {rawvids[i]}")
   task = client.task.create(index_id=index_obj.id, file= ("/content/rawdata/" + str(rawvids[i]) ) , language="en")
   print(f"Task id={task.id}")
   frame_id.append(task.video_id)
```

The code below invokes the `create` method of the `embed.task` object to create an embedding for the reference frame:

```python
task = client.embed.task.create(
   engine_name="Marengo-retrieval-2.7",
   video_url="https://storage.googleapis.com/lab-storage-items/sample-5s.mp4")
print(
   f"Created task: id={task.id} engine_name={task.engine_name} status={task.status}"
)


def on_task_update(task: EmbeddingsTask):
   print(f"  Status={task.status}")


status = task.wait_for_done(
 sleep_interval=5,
 callback=on_task_update
 )
print(f"Embedding done: {status}")
task = client.embed.task.retrieve(task.id)
if task.video_embeddings is not None:
   for v in task.video_embeddings:
       print(
           f"embedding_scope={v.embedding_scope} start_offset_sec={v.start_offset_sec} end_offset_sec={v.end_offset_sec}"
       )
       print(f"embeddings: {', '.join([str(x) for x in v.embedding.float])}")
       ref_emb = np.array([str(x) for x in v.embedding.float])

```

The code below creates embeddings for the subsequent frames:

```python
fref_emb = []
for i in range(len(rawvids)):


 task = client.embed.task.create(
     engine_name="Marengo-retrieval-2.7",
     video_url="https://storage.googleapis.com/lab-storage-items/sample-5s.mp4")
 print(
     f"Created task: id={task.id} engine_name={task.engine_name} status={task.status}"
 )

 status = task.wait_for_done(
   sleep_interval=5,
   callback=on_task_update
   )
 print(f"Embedding done: {status}")
 task = client.embed.task.retrieve(task.id)
 if task.video_embeddings is not None:
     for v in task.video_embeddings:
         print(
             f"embedding_scope={v.embedding_scope} start_offset_sec={v.start_offset_sec} end_offset_sec={v.end_offset_sec}"
         )
         print(f"embeddings: {', '.join([str(x) for x in v.embedding.float])}")
         fref_emb.append(np.array([str(x) for x in v.embedding.float]))

```

The code below invokes the `summarize` method of the `generate` object to summarize the reference frame:

```python
res = client.generate.summarize(ref_id, type='summary', prompt="In a detailed way, describe this video clip." )
```

The code below summarizes each subsequent video and stores the results in a list:

```python
The code below summarizes each subsequent video and stores the results in a list:
fres = []
fres_emb = []
for i in range(len(rawvids)):
 res2 = client.generate.summarize(frame_id[i], type='summary', prompt="In a detailed way, describe this video clip." )
 fres.append(res2)
 fres_emb.append(model.encode(res2.summary))

```


# Sports

The applications on this page demonstrate how the TwelveLabs Video Understanding Platform enhances sports analysis and viewer engagement.

# Chessboxing AI Clips

**Summary**: "Chessboxing AI Clips" is a web application that enables you to semantically search, preview, and compile video clips from the hybrid sport of [chessboxing](https://en.wikipedia.org/wiki/Chess_boxing), showcasing the adaptability of the TwelveLabs Video Understanding Platform to unconventional sports.

**Description**: The application, developed during a TwelveLabs Hackathon, is a React and TypeScript-based web application that offers the following key features:

* **Semantic video search**: You can search for specific moments in chessboxing videos using natural language queries.
* **Clip preview**: Before selecting clips for compilation, you can preview them using the integrated video player.
* **Highlight reel creation**: You can select multiple clips to create custom highlight reels and add AI-generated background music.
* **Shareable content**: The application generates unique URLs for each highlight reel, allowing you to share your compilations on social media platforms.

The application was developed by Ray Deck and Akshay Rakheja.

**GitHub**: [Chessboxing AI Clips](https://github.com/statechangelabs/twelveLabs-hackathon)
**Demo**: [Chessboxing AI Clips](https://share.descript.com/view/PDAHBoF8TnR)

## Integration with TwelveLabs

The code below iterates through a list of video files, uploading each to the TwelveLabs Video Understanding Platform if it hasn't been uploaded yet. It invokes the `client.task.create` function of the [TwelveLabs Python SDK](https://github.com/twelvelabs-io/twelvelabs-python) to create a video indexing task for each upload and monitors the task status until completion or failure:

```python
for video_file in video_files:
    video_filename = os.path.basename(video_file)
    if video_filename in existing_filenames:
        print(
            f"File '{video_filename}' is already uploaded. Skipping to the next video."
        )
        continue

    try:
        print(f"Uploading {video_filename}")
        # Assuming task creation and upload logic remains the same
        task = client.task.create(index_id=INDEX_ID, file=video_file, language="en")
        print(f"Task id={task.id}")

        def on_task_update(task: Task):
            print(f"  Status={task.status}")

        task.wait_for_done(callback=on_task_update)

        if task.status != "ready":
            raise RuntimeError(f"Indexing failed with status {task.status}")

        print(
            f"Uploaded {video_filename}. The unique identifier of your video is {task.video_id}."
        )

    except RuntimeError as e:
        print(f"Error uploading {video_filename}: {e}")
        # Continue with the next video

```

The `search_twelve_labs` function invokes the `POST` method of the `/search` endpoint with specific parameters and a query provided by the user:

```python
def search_twelve_labs(query):
    url = "https://api.twelvelabs.io/v1.2/search"
    payload = {
        "search_options": ["visual", "conversation"],
        "adjust_confidence_level": 0.5,
        "group_by": "clip",
        "threshold": "low",
        "sort_option": "score",
        "operator": "or",
        "conversation_option": "semantic",
        "page_limit": 50,
        "query": query,  # Use the input query here
        "index_id": os.getenv("TL_INDEX")  # Replace with your actual index ID
    }
    headers = {
        "accept": "application/json",
        "x-api-key": os.getenv("TL_API_KEY"),  # Replace with your actual API key
        "Content-Type": "application/json"
    }

    response = requests.post(url, json=payload, headers=headers)

    if response.status_code == 200:
        return response.json()
    else:
        return {"error": "Request failed", "status_code": response.status_code, "message": response.text}
```




# Cricket Video Analyzer

**Summary**: "Cricket Video Analyzer" combines the capabilities of the TwelveLabs Video Understanding Platform with language models to offer comprehensive insights into a batter's performance. It focuses on analyzing Glenn Maxwell of Australia as a case study.
**Description**:  The application has been developed to meet the growing demand for sophisticated video analysis in cricket. Key features include:

* **Video understanding**: Uses the TwelveLabs API to analyze cricket matches, extracting detailed information about the batter's playing style and techniques.
* **Data augmentation**: Integrates ball-by-ball commentary data from [ESPN cricinfo](https://www.espncricinfo.com) APIs to enhance the video analysis with textual descriptions of the gameplay.
* **Vector store creation**: Builds a vector store from the combined dataset of video analysis and ball-by-ball commentary, enabling efficient information retrieval.
* **AI-powered query answering**: Utilizes GPT-4 in a Retrieval-Augmented Generation (RAG) setup to answer complex questions about a batter's performance and style based on the analyzed data.

**GitHub**: [Cricket video analyzer](https://github.com/Raghavan1988/12labs_hack_cricket_strategy)
**Demo**: [Cricket Batsman Analyzer](https://www.loom.com/share/6dfea448573348f8be4dbf104ad22e2c?sid=07c164f7-0efa-4bb1-8abf-a3063865fcf4)

The application was developed by Raghavan Muthuregunathan

## Integration with TwelveLabs

The code below automates the process of extracting detailed batting information from multiple videos, creating a dataset for further analysis. It invokes the `generate.text` method of the [TwelveLabs Python SDK](https://github.com/twelvelabs-io/twelvelabs-python) to generate a ball-by-ball commentary for Maxwell's cricket innings. It compiles all the generated commentaries into a single text file named "maxwell\_innings\_video\_analysis\_concat."

```python
video_ids = ["66370d91d1cd5a287c957d14", "66370d91d1cd5a287c957d15", "66370d95d1cd5a287c957d16","663715d0d1cd5a287c957d17"]

summary = ""

for id in video_ids:
    print(id)
    res = client.generate.text(video_id= id,prompt=" Listen to the commentary. Generate a ball by ball commentary of how Maxwell played. For  every ball, generate a description of the shot that Maxwell played. if Maxwell got out, explain how he got out?")
    summary += res.data
    print(len(summary))

with open("maxwell_innings_video_analysis_concat.txt", "w") as w:
    w.write(summary)
```

# Ready Player N

**Summary**: Ready Player N is a Streamlit-based application that helps you discover unique features in hiking trails by analyzing hiking videos using the Marengo video understanding engine.
**Description**: Ready Player N streamlines the discovery of special attractions along hiking trails, such as waterfalls. The application indexes a set of hiking videos, allowing you to search for relevant information. In order to improve search accuracy, the queries are refined using Groq. In response, the application provides the following:

* A thumbnail of the matching video segment.
* A description of the featured attraction.
* A link to a hiking website with additional details.
* The ability to watch the matching video segment directly within the integrated player.

The application was developed by Bryce Hirschfeld, James Beasley, Matt Florence, Drakosfire Meigs, and Nooshin Hashemi.




# SportsSnap

**Summary**:  SportsSnap is a browser extension that enhances the sports viewing experience by providing real-time player, team, and game information overlays. It uses the TwelveLabs Video Understanding Platform for multimodal search.
**Description**: The application bridges the gap between video content and real-time sports data, offering instant access to detailed information while watching game highlights or replays.

The application was developed by Axel VandenHeuvel and Navan Chauhan.

Key features include:

* Real-time information overlay on sports videos
* Player and team statistics integration
* Game situation analysis
* Multimodal search capabilities




## Integration with TwelveLabs

The integration with the TwelveLabs Video Understanding Platform provides the following functionalities:

* **Video understanding**: Extract and analyze visual and audio information from sports content, improving the accuracy of video identification and content analysis.

* **Advanced search capabilities:** Perform complex queries across various data types (visual, audio, and textual) to enhance the app's ability to provide relevant information to users.

* **Improved video-to-game matching**: Improve the accuracy of matching videos to specific games in ESPN's database, addressing one of the main challenges faced during development.

* **Enhanced user experience**: Provide users with more accurate and contextually relevant information, creating a more seamless and informative viewing experience.

# SnowRivals

**Summary**: SnowRivals is an AI-powered application for skiers and snowboarders that analyzes trick videos, provides improvement feedback, and aligns with competition judging criteria. The application offers detailed insights and suggestions for enhancing trick execution and scoring.

**Description**: SnowRivals uses TwelveLabs' video summarization and embedding technology to analyze snowboarding and skiing trick videos by extracting the following structured data:

* Trick identification
* Athlete performance
* Success and failure detection
* Timing information

The application was developed by Jeremy London, Francesco Vassalli, Neil Fonseca, and Reid Rumack.

SnowRivals was one of the winners at the [AI Tinkerers Multimodal Hackathon](https://multimodal-ai-tinkerers.devpost.com) .

GitHub: [multimodal-ui](https://github.com/jeremy-london/multimodal-ui)  and [MultiModal](https://github.com/FrancescoVassalli/MultiModal).




## Integration with TwelveLabs

The [`generate`](https://github.com/FrancescoVassalli/MultiModal/blob/81b8a16add509dcde821a89100a1f22a51b92247/multi_snow/twelve_start.py#L17)  function uses the \[Analyze] API]\(/docs/guides/analyze-videos)  suite to analyze a snowboarding competition video. It sends a `POST` request to the `/summarize` endpoint with the following parameters:

* `video_id`: Identifier of the video to analyze.
* `type`: Set to "summary".
* `prompt`: Detailed instructions for generating a table of snowboarding tricks. It includes a predefined list of trick descriptions to aid in trick identification.
* `temperature`: Controls randomness in the output (set to 0.5).

If successful, the function returns a summary table containing the following:

* **Athlete**: Identifier for the person performing each trick
* **Trick**: Name of the trick performed
* **Result**: Success or failure of the trick
* **Half**: Presence of a half pipe ("Yes"/"No")
* **Rail**: Presence of a rail ("Yes"/"No")

If the summary generation fails, the function prints an error message with the `video_id` and the API response.

```python
def generate(video_id:str)->str:
    BASE_URL = "https://api.twelvelabs.io/v1.2"
    trick_descriptions = '''Indy: Grabbing the toe edge of the board between the bindings with the back hand.
Mute: Grabbing the toe edge of the board between the bindings with the front hand.
Stalefish: Grabbing the heel edge of the board between the bindings with the back hand.
Melon: Grabbing the heel edge of the board between the bindings with the front hand.
Tail Grab: Grabbing the tail of the board.
Nose Grab: Grabbing the nose of the board.
Method: Grabbing the heel edge of the board with the front hand while arching the back and extending the legs.
Japan: Grabbing the toe edge of the board with the front hand while tucking the knees and rotating the board.
Seatbelt: Grabbing the nose of the board with the back hand.
Truck Driver: Grabbing the nose of the board with the front hand and the tail with the back hand simultaneously.
Rodeo: A backward flip with a 180 or 540-degree spin.
Misty: A forward flip with a 180 or 540-degree spin.
Cork: An off-axis spin, where the boarder is tilted.
Double Cork: A double off-axis spin.
Wildcat: A backflip with a rotation around the snowboarder’s side.
Jumps
Ollie: Lifting the front foot, followed by the back foot, to jump.
Nollie: Lifting the back foot, followed by the front foot, to jump.
Slides and Grinds
Boardslide: Sliding with the board perpendicular to the rail.
Lipslide: Approaching from the opposite side and sliding with the board perpendicular to the rail.
50-50: Sliding straight along a rail with the board parallel.
Nose Slide: Sliding on the nose of the board.
Tail Slide: Sliding on the tail of the board.
Blunt Slide: Sliding on the tail of the board with the nose raised.
Buttering
Nose Butter: Pressing down on the nose of the board while rotating the tail.
Tail Butter: Pressing down on the tail of the board while rotating the nose.
Nose Roll: A 180-degree rotation while buttering on the nose.
Tail Roll: A 180-degree rotation while buttering on the tail.
'''


    data = {
        "video_id": video_id,
        "type": "summary",
        "prompt": f"This video has a snowboarding competition. Use expert Generate a table that records one row per trick performed. Add one column called Athlete to label which person performed each trick. If you do not know their names just refer to them as person-1 or person-2 as they appear sequentially in the video. In a second column called Trick, name the trick. In the third column called Result note if the trick was a Success or a Failure. In a fourth column called Half say Yes if the video has a half pipe and No if it does not. In a fifth column called Rail say Yes if the video has a rail and no if it does not. Keep your response brief and do not include anything aside from the table. If you are unsure of what to call a track you may reference this vocabulary list {trick_descriptions}",
        "temperature": 0.5
    }

    response = requests.post(f"{BASE_URL}/summarize", json=data, headers={"x-api-key": key})
    response_dict = json.loads(response.text)
    if 'summary' in response_dict:
        return response_dict['summary']
    else:
        print(f"Summary for {video_id} unable to be generated. Got response {response}")

```




# Smart Sweat AI

**Summary**: Smart Sweat AI is a mobile application that uses artificial intelligence to provide real-time feedback on exercise form during workouts. It uses the device's camera to analyze your movements and offer personalized corrections and improvements.

**Description**: The application serves as a digital personal trainer, utilizing AI technology to:

* Analyze exercise form in real-time through your phone camera
* Provide instant feedback and corrections on posture and movement
* Offer a comprehensive library of exercises
* Track your progress and personalize workout experiences

Smart Sweat AI uses the [Analyze API](/docs/guides/analyze-videos)  suite to provide advanced video analysis and personalized fitness instruction.

The application was developed by Jesse Neumann, Hannah Neumann, and Roblynn Neumann.

Smart Sweat AI was one of the winners at the [AI Tinkerers Multimodal Hackathon](https://multimodal-ai-tinkerers.devpost.com) .

Website: [Smart Sweat AI](https://www.smartsweat.ai)




# National Park Advisor

**Summary**: National Park Advisor is an application designed to assist you in researching and planning trips to National Parks.

**Description**: The application utilizes two distinct RAG pipelines:

* **Park information pipeline**: Retrieves and presents detailed information about specific National Parks. It uses a data store to search for park information efficiently. When the data store does not have sufficient data for a particular park, the application supplements it by searching the internet, ensuring comprehensive coverage.
* **Trip planning pipeline**: Assists in planning visits to National Parks.

Key features include:

* National Park research assistance
* Trip planning for National Parks
* Video content search and classification
* Transcript extraction from videos
* Internet search capability for supplemental park information

The application utilizes:

* [TwelveLabs Python SDK](https://github.com/twelvelabs-io/twelvelabs-python): Video search, transcript extraction, and content classification.
* [Pinecone](https://www.pinecone.io): Stores data.
* [Tavily](https://tavily.com): Internet search for additional content.
* [Langgraph](https://www.langchain.com/langgraph): Workflow management.
* [Groq](https://groq.com): Language model (LLM) for fast inference.

The application was developed by Sarah Kainec and Lexie Marinelli.

**GitHub**: [ntl-park-advisor](https://github.com/lexie-marie/ntl-park-advisor/)
**Demo**: [National Park Advisor](https://oyster-app-r4rze.ondigitalocean.app/#)

## Integration with TwelveLabs

The [`get_videos_from_twelve`](https://github.com/lexie-marie/ntl-park-advisor/blob/be00e5991a49cc73262751e4fa9a77324f2afd3a/app/retrievers/get_videos_from_twelve.py#L16) function performs search requests and retrieves transcriptions for up to four videos that match the provided query. It returns a dictionary that includes all key-value pairs from the `state` parameter, to which it adds the video URLs and transcription data:

```python
def get_videos_from_twelve(state: dict) -> dict:
   print("getting videos")
   load_dotenv()
   client = TwelveLabs(api_key=os.getenv("TL_API_KEY"))

   search_results = client.search.query(
       index_id=TL_INDEX_ID,
       query_text=state["query"],
       options=["visual"]
   )

   filtered_search = []
   unique_vids_id = []
   for clips in search_results.data.root:
       if clips.video_id not in unique_vids_id:
           unique_vids_id.append(clips.video_id)
           filtered_search.append(clips)

   transcript_data = []
   urls = []
   for id in unique_vids_id[0:4]:
       script = client.index.video.transcription(
           index_id=TL_INDEX_ID,
           id=id
       )
       url = client.index.video.retrieve(index_id=TL_INDEX_ID, id=id)
       urls.append(url)
       whole_script = ""
       for vid_value in script.root:
           whole_script = whole_script + " " + vid_value.value

       transcript_data.append({"video_id": id, "video_url": url.hls.video_url, "transcript": whole_script})

   return {
       **state,
       "video_urls": [url.hls.video_url for url in urls],
       "transcript_data": transcript_data
   }

```

The [`get_classified_videos_from_twelve`](https://github.com/lexie-marie/ntl-park-advisor/blob/be00e5991a49cc73262751e4fa9a77324f2afd3a/app/retrievers/get_video_classificiation_from_twelve.py#L40) function classifies videos based on predefined, filters for the specified destination, and returns an updated state dictionary with URLs for up to four matching videos.

```python
classification = [
   {
       "name": "Rocky Mountain National Park",
       "prompts": [
           "Things to do in Rocky Mountain National Park",
           "Learn about Rocky Mountain National Park",
           "Prepare for a trip to Rocky Mountain National Park",
           "RMNP",
           "Colorado"
       ]
   },
   {
       "name": "Glacier National Park",
       "prompts": [
           "Things to do in Glacier National Park",
           "Wildlife at Glacier National Park",
           "Ecology of Glacier National Park",
           "Glacier Science",
           "Montana"
       ]
   }
]


def get_classified_videos_from_twelve(state: dict) -> dict:
   load_dotenv()
   client = TwelveLabs(api_key=os.getenv("TL_API_KEY"))

   classified_result = client.classify.index(
       index_id=TL_INDEX_ID,
       options=["visual"],
       classes=classification,
   )

   filtered_search = []
   unique_vids_id = []
   for clips in classified_result.data.root:
       if clips.video_id not in unique_vids_id and clips.classes.root[0].name == state["destination"]:
           unique_vids_id.append(clips.video_id)
           filtered_search.append(clips)
   urls = []
   for id in unique_vids_id[0:4]:
       url = client.index.video.retrieve(index_id=TL_INDEX_ID, id=id)
       urls.append(url)

   return {
       **state,
       "video_urls": [url.hls.video_url for url in urls],
   }

```


# Media and entertainment

The applications on this page demonstrate the capabilities of the TwelveLabs Video Understanding Platform in transforming how you interact with and consume digital content.

# Product Placement Assistant

**Summary**: Product Placement Assistant analyzes videos to identify optimal segments for brand placements. It utilizes the TwelveLabs Video Understanding Platform to provide actionable insights that enhance the impact of product placements.

**Description**: The application allows you to upload videos and receive detailed insights into segments that are ideal for product placements, based on a customizable prompt. This feature helps brands enhance their visibility. The process involves creating an index, uploading and indexing videos, and generating  insights.

**GitHub repo**: [akshaymijar17/productPlacement](https://github.com/akshaymijar17/productPlacement?tab=readme-ov-file).

**Demo**: [Product Placement Assistant](https://brandplacement.streamlit.app/).

## Integration with TwelveLabs

Product Placement Assistant integrates the TwelveLabs Video Understanding Platform to analyze video content and recommend product placement. The following code snippets highlight key integration points.

### Create indexes

The `create_index` function creates a new index using the `client.index.create` method. It enables the Marengo 2.7 and Pegasus 1.2 video understanding models for visual and audio analysis, along with the `thumbnail` addon.

```Python Python
def create_index(client: TwelveLabs, index_name: str):
    """
    Create a new 12Labs index with specified models/addons.
    """
    models = [
        {"name": "marengo2.7", "options": ["visual", "audio"]},
        {"name": "pegasus1.2", "options": ["visual", "audio"]},
    ]
    try:
        created_index = client.index.create(
            name=index_name,
            models=models,
            addons=["thumbnail"]
        )
        return created_index
    except Exception as e:
        raise RuntimeError(f"Failed to create index: {e}")
```

### Upload and index videos

The `upload_video_and_wait` function uploads a video to the specified index using the `client.task.create` method and monitors the indexing process with the `wait_for_done` method. It provides users with real-time status updates via a Streamlit placeholder.

```Python Python
def upload_video_and_wait(client: TwelveLabs, index_id: str, video_file):
    """
    Upload the video to the specified index and wait for indexing to complete.
    Updates the status message in place every 30 seconds.
    """
    try:
        task = client.task.create(
            index_id=index_id,
            file=video_file,
        )

        # Create a placeholder to update status in place.
        status_placeholder = st.empty()

        def on_task_update(t: Task):
            # Update the same placeholder each time
            status_placeholder.write(f"Indexing Status: {t.status}")

        # Sleep interval = 30s, so it updates the placeholder every 30 seconds
        task.wait_for_done(sleep_interval=30, callback=on_task_update)

        if task.status != "ready":
            raise RuntimeError(f"Indexing failed with status '{task.status}'")

        return task.video_id
    except Exception as e:
        raise RuntimeError(f"Video upload/indexing failed: {e}")
```

### Generate placement insights

The `generate_text_from_video` function generates text-based insights from the indexed video using the `client.generate.text` method. It uses a customizable prompt, which users can override to extract specific recommendations for product placements.

```Python Python
def generate_text_from_video(client: TwelveLabs, video_id: str, prompt: str) -> str:
    """
    Generate text from an indexed video using the provided prompt.
    """
    try:
        result = client.generate.text(video_id=video_id, prompt=prompt, temperature=0.7)
        return result.data
    except Exception as e:
        raise RuntimeError(f"Text generation failed: {e}")
```

# Verbatim

**Summary**: Verbatim is an application that enables you to translate videos into over 20 languages while synchronizing lip movements. It also offers summarization and interactive Q\&A features based on an analysis of the video content.

**Description**: With Verbatim, you can access videos that have been translated and voice-cloned, and you can use a chatbot that answers questions and finds specific timestamps by analyzing both audio and visual content. The application utilizes the TwelveLabs Video Understanding Platform to create an interactive Q\&A interface.

Verbatim was created by Sonny Chen, Cindy Yang, Karthik Thyagarajan, and Pranav Neti.

**GitHub repo**: [TheXDShrimp/verbatim](https://github.com/TheXDShrimp/verbatim)

**Website**: [Verbatim](https://www.getverbatim.tech)

## Integration with TwelveLabs

Verbatim utilizes the TwelveLabs Video Understanding Platform to enable intelligent video analysis and interaction. The following code snippets demonstrate key integration points.

## Generate titles, topics, and hashtags

The `generateMetadata` function utilizes the `generate.gist` method to extract the title, topics, and hashtags from video content. These metadata elements enhance searchability and organization within the user interface.

```JavaScript Node.js
// ------------> GENERATE A TITLE, AND RELATED DETAILS (USE TO TAG VIDEOS IN UI) <---------------
export async function generateMetadata(videoId) {
  const gist = await client.generate.gist(videoId, [
    "title",
    "topic",
    "hashtag",
  ]);
  console.log(
    `Title: ${gist.title}\nTopics=${gist.topics}\nHashtags=${gist.hashtags}`
  );
  return gist;
}
```

## Generate chapters

The `generateChapters` function generates chapter markers for videos by calling the `generate.summarize` method with a custom prompt. The function formats timestamps and returns structured chapter information that helps you navigate through content more efficiently.

```JavaScript Node.js
// --------------------> GENERATE CHAPTERS (DO THIS INITIALLY TO EXPLAIN VIDEO) <----------------
function formatTime(seconds) {
  const minutes = Math.floor(seconds / 60);
  const remainingSeconds = seconds % 60;
  return `${minutes}:${remainingSeconds.toString().padStart(2, "0")}`;
}

export async function generateChapters(videoId) {
  const chapters = await client.generate.summarize(
    videoId,
    "chapter",
    "Generate chapters (max 10) while matching the teaching style of the video. Make sure to keep the titles relatively broad, while making the chapter summaries use simple but specific language"
  );

  for (const chapter of chapters.chapters) {
    console.log(
      `Chapter ${chapter.chapterNumber} - ${
        chapter.chapterTitle
      }\nTime: ${formatTime(chapter.start)} - ${formatTime(
        chapter.end
      )}\nSummary: ${chapter.chapterSummary}\n`
    );
  }

  return chapters.chapters.map((chapter) => ({
    chapterNumber: chapter.chapterNumber,
    chapterTitle: chapter.chapterTitle,
    start: formatTime(chapter.start),
    end: formatTime(chapter.end),
    summary: chapter.chapterSummary,
  }));
}
```

## Generate open-ended text

The `generateText` function answers questions about video content using the `generate.text` method. It ensures that responses correspond to the user's query language by translating them when necessary, thus enhancing accessibility for international users.

```JavaScript Node.js
// --------------> QUESTIONS QUERY NOT STREAMING (formated output for questions) <---------------
export async function generateText(videoId, prompt) {
  const promptLang = await detectLanguage(prompt);
  const text = await client.generate.text(videoId, prompt);
  const responseLang = await detectLanguage(text.data);
  console.log(text.data);

  if(promptLang === responseLang) {
    return text.data;
  }

  else {
    const translatedResponse = await translateText(text.data, promptLang);
    return translatedResponse;
  }
}
```

## Search

This `searchQuery` function identifies specific moments in videos using the `search.query` method. The function formats and processes search results to show relevant timestamps for segments that match the query.

```JavaScript Node.js
// --------------> SEARCH FOR TEXT QUERIES (find time stamps of intrest) <-----------------
export async function searchQuery(
  indexId,
  queryText,
  limitResults,
  desiredVideo
) {
  let searchResults = await client.search.query({
    indexId: indexId,
    queryText: queryText,
    options: ["visual", "audio"],
    operator: "and",
  });

  // printPage(searchResults.data, limitResults, desiredVideo);

  // while (true) {
  //   const page = await searchResults.next();
  //   if (page === null) break;
  //   else printPage(page, limitResults, desiredVideo);
  // }
  // console.log("Search results: ", searchResults.data);
  return searchResults.data;
}

// Print search results
function printPage(searchData, limitResults, desiredVideo) {
  if (!Array.isArray(searchData)) {
    console.error("Expected searchData to be an array");
    return;
  }

  searchData.forEach((clip) => {
    if (limitResults && clip.videoId === desiredVideo) {
      console.log(
        `Video ID: ${clip.videoId}\n` +
          `Score: ${clip.score}\n` +
          `Start: ${clip.start}\n` +
          `End: ${clip.end}\n` +
          `Confidence: ${clip.confidence}\n`
      );
    } else if (!limitResults) {
      console.log(
        `Video ID: ${clip.videoId}\n` +
          `Score: ${clip.score}\n` +
          `Start: ${clip.start}\n` +
          `End: ${clip.end}\n` +
          `Confidence: ${clip.confidence}\n`
      );
    }
  });
}
```

# Ghool

**Summary**: Ghool is a trailer creation application that uses the TwelveLabs Video Understanding Platform to search, retrieve, and concatenate video clips based on specific prompts and quality metrics.

**Description**: The application uses both the Marengo and Pegasus video understanding models to process video content and create optimized video sequences with minimal user intervention. It retrieves relevant video clips based on user prompts. It then processes these videos by concatenating them and evaluating the transitions between different videos based on specific quality metrics, such as smoothness.

Ghool was created by Fazil Onuralp Adic and Oscar Chen and was one of the winners at the [Cerebral Beach Hacks – LA Tech Week 2024 Kickoff Hackathon](https://cerebral-beach-hacks.devpost.com/).

**GitHub repo**: [cerebral\_ghool](https://github.com/foainla/cerebral_ghool)

## Integration with TwelveLabs

Ghool integrates with the TwelveLabs Video Understanding Platform to search, retrieve, and analyze video content based on specific prompts. The integration enables automated evaluation of video transitions and quality assessment without manual intervention.

The `find_video` function uses the TwelveLabs Python SDK to perform text queries against a specified index. It filters results by confidence level and evaluates each clip's quality against user-defined criteria.

```python Python
def find_video(prompt,wanted,quality):

page = client.search.query(index_id="66f1cde8163dbc55ba3bb220", query_text=prompt, options=["visual"])

video_vec = []

i = 0

for clip in page.data:

if clip.confidence == "high" and i str:
   url = f"{BASE_TWELVE_URL}/generate"
   payload = {
       "prompt": "Given the following video, segment the videos and provide corresponding timestamps based on the different activities and places that the subject does and visits so that the segmented videos can later be used to build an itinerary.\nMake the response concise & precise.",
       "video_id": f"{video_id}",
       "temperature": 0.4,
   }
   headers = {
       "accept": "application/json",
       "x-api-key": TWELVE_LABS_API_KEY,
       "Content-Type": "application/json",
   }


   logging.info("Generating segmented itinerary")
   async with aiohttp.ClientSession() as session:
       async with session.post(url, json=payload, headers=headers) as response:
           if response.status == 200:
               try:
                   result = await response.text()
                   segmented_itinerary = json.loads(result).get("data")
                   return segmented_itinerary
               except Exception as e:
                   logging.exception(e)
           else:
               logging.info(response.status)

```

# Hello Garfield

**Summary**: The "Hello Garfield" application provides an immersive virtual reality experience combining traditional movie theaters with cutting-edge technology. It features a personalized AI concierge, themed environments, and interactive elements to enhance the movie-watching experience.\
**Description**: The application transforms how you engage with movies in a virtual space. Upon entering the virtual theater, you are greeted by an AI concierge. This concierge offers personalized movie recommendations based on your preferences and viewing history.

Key features include:

* **Video Q\&A chatbot**: The application uses the [Analyze API](/docs/guides/analyze-videos)  to allow you to ask questions about the movies you're watching.
* **Immersive VR/MR environment**: A realistic virtual movie theater with a large screen, created using VR/MR development platforms such as Unity and Unreal Engine.
* **AI concierge**: A chatbot that provides personalized movie suggestions and enhances the user experience through friendly interaction.
* **Enhanced viewing experience**: The concierge suggests themed snacks, recipes, and merchandise related to the chosen movie, creating a more immersive and enjoyable viewing experience.
* **AR filters**: You can "try on" costumes from your favorite films and decorate your virtual spaces using augmented reality technology.
* **Community interaction**: A shared virtual theater space that allows you to connect with other film enthusiasts, fostering a sense of community.

The application was developed by Lauren Descher, Dulce Baerga, and Catherine Rhee.

**GitHub**: [aila-hack](https://github.com/dulce303/aila-hack)




## Integration with TwelveLabs

The code below handles different types of requests:

1. It checks the type of request and prepares appropriate data for each.
2. For each type of request, it constructs a data object with a specific video ID and other relevant parameters.
3. It sends a `POST` request to the `/gist` or `/summarize` endpoints.

```javascript
  if (event.request === "gist") {
    if (event.trailername === "garfield") {

      data = {
        "video_id": "666581dbd22b3a3c97bf1d57",
        "types": [
          "title",
          "hashtag",
          "topic"
        ]
      };
    }
    data = {
      "video_id": "666581dbd22b3a3c97bf1d57",
      "types": [
        "title",
        "hashtag",
        "topic"
      ]
    };
    response = await fetch(baseUrl + "/gist", {
      method: "POST",
      headers: { "x-api-key": apiKey, "Content-Type": "application/json" },
      body: JSON.stringify(data)
    });
  }
  else if (event.request === "summary") {
    // SUMMARY REQUESTED
    data = {
      "video_id": "666581dbd22b3a3c97bf1d57",
      "type": "summary"
    };
    if (event.trailername === "garfield") {
      data = {
        "video_id": "666581dbd22b3a3c97bf1d57",
        "type": "summary"
      };
      response = await fetch(baseUrl + "/summarize", {
        method: "POST",
        headers: { "x-api-key": apiKey, "Content-Type": "application/json" },
        body: JSON.stringify(data)
      });
    }
    response = await fetch(baseUrl + "/summarize", {
      method: "POST",
      headers: { "x-api-key": apiKey, "Content-Type": "application/json" },
      body: JSON.stringify(data)
    });

  }
  else if (event.request === "chapters") {
    data = {
      "video_id": "666581dbd22b3a3c97bf1d57",
      "type": "chapter"
    };

    if (event.trailername === "garfield") {
      data = {
        "video_id": "666581dbd22b3a3c97bf1d57",
        "type": "chapter"
      };
      response = await fetch(baseUrl + "/summarize", {
        method: "POST",
        headers: { "x-api-key": apiKey, "Content-Type": "application/json" },
        body: JSON.stringify(data)
      });
    }
    response = await fetch(baseUrl + "/summarize", {
      method: "POST",
      headers: { "x-api-key": apiKey, "Content-Type": "application/json" },
      body: JSON.stringify(data)
    });
  }
  else if (event.request === "highlights") {
    data = {
      "video_id": "666581dbd22b3a3c97bf1d57",
      "type": "highlight",
      "prompt": "tell me about food\n"
    };

    if (event.trailername === "garfield") {
      data = {
        "video_id": "666581dbd22b3a3c97bf1d57",
        "type": "highlight",
        "prompt": "tell me about food\n"
      };
      response = await fetch(baseUrl + "/summarize", {
        method: "POST",
        headers: { "x-api-key": apiKey, "Content-Type": "application/json" },
        body: JSON.stringify(data)
      });
    }
    response = await fetch(baseUrl + "/summarize", {
      method: "POST",
      headers: { "x-api-key": apiKey, "Content-Type": "application/json" },
      body: JSON.stringify(data)
    });
  }

```

# Sports Recap

**Summary**: Sports Recap is a NextJS-based application that generates video highlights and summaries from sports press conferences.

**Description**: The application is designed to transform lengthy sports press conferences into concise, engaging highlight reels. The application utilizes the [Analyze API](/docs/guides/analyze-videos)  to create relevant highlights based on user-specified criteria.

The application was developed by Daniel Jacobs, Yurko Turskiy, Suxu Li, and Melissa Regan.

**GitHub**: [hackathone-challenge-3](https://github.com/yurkoturskiy/hackathone-challenge-3)

## Integration with TwelveLabs

The function below extracts data from the incoming request and invokes the `POST` method of the `/summarize` endpoint, passing the unique identifier of a video and a prompt to generate highlights:

```javascript
export async function POST(req: NextRequest) {
  const { projectId, videoId, prompt } = await req.json();
  const baseUrl = "https://api.twelvelabs.io/v1.2";
  const apiKey = process.env.TWELVELABS_API as string;
  const data = {
    prompt: prompt,
    video_id: videoId,
    type: "highlight",
  };

  // Send request
  const response = await fetch(baseUrl + "/summarize", {
    method: "POST",
    headers: { "x-api-key": apiKey, "Content-Type": "application/json" },
    body: JSON.stringify(data),
  });
```

The function below extracts the unique identifier of a video from the incoming request and invokes the `POST` method of the `/summarize` endpoint. It passes the video ID along with a predefined prompt to generate a summary that identifies the main character and lists key topics discussed:

```javascript
export async function POST(req: NextRequest) {
  const { videoId } = await req.json();

  // Variables
  const baseUrl = "https://api.twelvelabs.io/v1.2";
  const apiKey = process.env.TWELVELABS_API as string;
  const data = {
    video_id: videoId,
    type: "summary",
    prompt:
      "Specify the name of the main character of the video. Generate bullet list of key topics the main character is talking about",
  };

  // Send request
  const response = await fetch(baseUrl + "/summarize", {
    method: "POST",
    headers: { "x-api-key": apiKey, "Content-Type": "application/json" },
    body: JSON.stringify(data),
  });
```

# Human, Please

**Summary**: The application combines human verification, ad monetization, and AI analysis. Users watch a short video and describe it with keywords, which the application evaluates to verify human-like responses.

**Description**: The application utilizes Langflow to prompt OpenAI and TwelveLabs to perform offline processing of videos. It performs the following main tasks:

* **Presents a video-based captcha**: The application displays a selection of short videos, such as advertisements or wildlife clips, to the user. After viewing a video, the application prompts the user to enter a few keywords describing the video content.

* **Verifies user input with AI**: The application compares the input and determines if the response is human-like, then shows the user a result indicating whether they passed the verification.

* **Monetizes screen real estate**: The application integrates advertisements or branded content into the CAPTCHA process. This approach demonstrates a method to monetize user attention while performing bot checks.

**GitHub**: [Human, Please](https://github.com/huangshu91/Langflow-AiGents-Hack)

# TwelveSocial

**Summary**: The application transforms long videos into social media-ready clips. It uses AI-driven video analysis, a chatbot for user guidance, and automated clip and post generation to create engaging, shareable content.

**Description**: The applicaiton simplifies the process of creating short, social media-friendly video clips from long-form videos. It performs the following tasks:

* **Uploads video files**: Users can upload video files, such as MP4 or MOV, through a user-friendly interface.
  **Guides Users with an AI chatbot**: After uploading a video, an AI chatbot powered by LangChain and OpenAI engages users. The chatbot asks about their goals, such as highlighting specific moments, breaking tutorials into steps, or creating clips for different topics.
* **Analyzes and searches video content**: The app uses the welve Labs Video Understanding Platform to index the uploaded video and search for relevant segments based on user input.
* **Generates short clips**: Selected video segments are stitched into short, social media-ready clips using FFmpeg.
* **Creates social media posts**: The application generates suggested social media text posts with hashtags and emojis, tailored to the content of the generated clips.
* **Previews and downloads clips**: Users can preview, download, edit, or select their favorite clips and social posts, with options to share directly to platforms like Instagram.

**GitHub**: [TwelveSocial](https://github.com/danschewy/twelvesocial)

# Stich Studios

**Summary**: Stich Studios is an MCP server for managing, searching, and processing sports video metadata. It integrates with the TwelveLabs Video Understanding platform to analyze sports videos, detect key events like goals, and generate highlight clips.

**Description**: Stich Studios manages and processes sports video metadata, with the following key features:

* **Metadata management**: Provides CRUD operations and advanced search capabilities for sports video metadata stored in PostgreSQL.
* **Video processing**: Uploads and processes sports videos, using the TwelveLabs Video Understanding Platform to analyze content, detect events, and generate highlights.
* **Highlight generation**: Creates shareable highlight clips from the main video based on the detected events.

**GitHub**: [Stich Studios](https://github.com/stich-studios/metadata-mcp)


# Migration guide

This guide shows how to migrate your applications to the 1.3 version of the API, which introduces significant improvements to the video understanding capabilities of the platform, simplified modalities, and a streamlined endpoint structure.


  You must use SDK version 0.4.x or later to access API version 1.3.


# What's new in v1.3?

* **Marengo 2.7**: A new version of the  Marengo video understanding model has been released. The 1.3 version of the API version only supports Marengo  2.7. This new version improves accuracy and performance in the following areas:
  * Multimodal processing that combines visual, audio, and text elements.
  * Fine-grained image-to-video search: detect brand logos, text, and small objects (as small as 10% of the video frame).
  * Improvement in motion search capability.
  * Counting capabilities.
  * More nuanced audio comprehension: music, lyrics, sound, and silence.
* **Simplified modalities**:
  * `visual`: includes objects, actions, text OCR, logos.
  * `audio`: includes speech, music, and ambient sounds.
  * `conversation` has been deprecated.
  * `text_in_video` and `logo` are now part of `visual`.
* **Streamlined endpoint structure**: Several endpoints and parameters have been deprecated, removed, or renamed.


  This guide presents the changes to the API. Since the SDKs reflect the structure of the API, review the [Migration examples](#2-migration-examples)  section below and the relevant SDK reference sections to understand how these changes have been implemented:

  * [Python SDK](/v1.3/sdk-reference/python)
  * [Node.js SDK](/v1.3/sdk-reference/node-js)


# Breaking changes

This section presents the changes that require updates to your code and includes the following subsections:

* Global changes that affect multiple endpoints
* Changes organized by endpoint and functionality (example: upload videos, manage indexes, etc.)

In the sections below, see the **Required Action** column for each change, then use the corresponding  example in the [Migration examples](#2-migration-examples) section to update your code.

## Global changes

| Description                                                                                                                                                                             | Required action                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Marengo 2.7 generates embeddings that are not backward compatible.                                                                                                                      | Reindex all your videos and regenerate all your embeddings with Marengo 2.7.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| The 1.3 version of the API version only supports Marengo 2.7.                                                                                                                           | Update the version strings in your code. For indexes, use "marengo2.7" instead of "marengo2.6". For embeddings, use "marengo-retrieval-2.7" instead of "marengo-retrieval-2.6". For examples of using Marengo 2.7, see the [Create indexes](#create-indexes) and [Create emeddings](#create-embeddings) sections below.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| The following parameters and fields in the responses have been renamed:

- `engines` → `models`
- `engine_name` → `model_name`
- `engine_options` → `model_options` | Update parameter names and response parsing in your code. For an example of using these parameters, see the [Create indexes](#create-indexes) section below.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| The previous modalities of \[`visual`, `conversation`, `text_in_video`, `logo`] have been simplified to \[`visual`, `audio`].                                                           | Update parameter names and response parsing in your code:

- `conversation` -> `audio`
- Remove `logo` and `text_in_video`.

Note that, while `logo` and `text_in_video` have been deprecated, no functionality has been removed. To achieve the same outcome, see the [Detect logos](#detect-logos) and [Search for text shown in videos](#search-for-text-shown-in-videos) sections below.

**NOTE:** For indexes created with Marengo v2.6, the `/indexes` endpoints continue to return the legacy modality values (`text_in_video`, `logo`, and `conversation`). You cannot perform any downstream tasks on these indexes.

For examples of using the new values, see the [Create indexes](#create-indexes) and [Perform a search requests](#perform-a-search-request) sections below. |
| The `/search-v2` endpoint becomes the new `/search` endpoint.                                                                                                                           | If you're using an HTTP client to interact with the API, update all search requests to use the new `/search` endpoint, which now supports both text and image queries.
No action is required if you're using one of the official SDKs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |

## Deprecated endpoints

| Endpoint                                                                                                                                                                                                                          | Required action                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `/classify`                                                                                                                                                                                                                       | You can no longer use this endpoint. TwelveLabs recommends classifying videos using the Analyze API. For details, see the [Use Pegasus to classify videos](#use-pegasus-to-classify-videos) section.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| - `/engines`
- `/engines/{engine-id}`                                                                                                                                                                                        | Update your code, removing any calls to these endpoints.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| - `/indexes/{index-id}/videos/{video-id}/text-in-video`
- `/indexes/{index-id}/videos/{video-id}/logo`
- `/indexes/{index_id}/videos/{video_id}/thumbnail`
- `/indexes/{index-id}/videos/{video-id}/transcription` | You can no longer use these endpoints. TwelveLabs recommends updating your code by replacing any calls to the specified endpoints with the alternative options provided below:

- **Text in video**: Search using text queries. For details, see the [Search for text shown in videos](#search-for-text-shown-in-videos) section.
- **Logos**: Search using text or image queries to detect logos. For details see the [Detect logos](#detect-logos) section.
- **Thumbnails**: The [`/search`](/v1.3/api-reference/any-to-video-search/make-search-request) endpoint returns thumbnails for each results. You can use them in your application.
- **Transcriptions**: Search using text queries. For details, see the [Search for text shown in videos](#search-for-text-shown-in-videos) section. |
| `/search/combined`                                                                                                                                                                                                                | You can no longer use this endpoint. TwelveLabs recommends updating your code to use the new [`/search`](/v1.3/api-reference/any-to-video-search/make-search-request) endpoint instead of `/search/combined`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |

## Upload videos

| Method                                              | Description                                                                                                  | Required action                                                                                                                                                                                       |
| --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`POST`](/v1.3/api-reference/tasks/create) `/tasks` | The `disable_video_stream` parameter has been renamed to `enable_video_stream`. The default value is `true`. | - Find and replace all occurences of `disable_video_stream` with `enable_video_stream`
- Review your logic - no need to set the `enable_video_stream` parameter if you want to enable streaming. |
| [`GET`](/v1.3/api-reference/tasks/list) `/tasks`    | The following parameters have been deprecated:

- `id`
- `estimated_time`                     | Update your code - you no longer can filter tasks based on these parameters.                                                                                                                          |

## Manage indexes

| Method                                                                | Description                              | Required action                                                             |
| :-------------------------------------------------------------------- | :--------------------------------------- | :-------------------------------------------------------------------------- |
| [`GET`](/v1.3/api-reference/indexes/retrieve)   `/indexes/{index-id}` | The `_id` parameter has been deprecated. | Update your code - you no longer can filter videos based on this parameter. |

## Manage videos

| Method                                                                               | Description                                                                | Required action                                                  |
| :----------------------------------------------------------------------------------- | :------------------------------------------------------------------------- | :--------------------------------------------------------------- |
| [`GET`](/v1.3/api-reference/indexes/retrieve)  `/indexes/{index-id}/videos`          | The `metadata` parameter has been renamed to `user_metadata`               | Update your code to use `user_metadata` instead of `metadata`.   |
| [`GET`](/v1.3/api-reference/videos/list)   `/indexes/{index-id}/videos`              | The `metadata` field in the response has been renamed to `system_metadata` | Update your code to use `system_metadata` instead of `metadata`. |
| [`GET`](/v1.3/api-reference/videos/retrieve) `/indexes/{index-id}/videos/{video-id}` | The `metadata` field in the response has been renamed to `system_metadata` | Update your code to use `system_metadata` instead of `metadata`. |




## Search

| Method                                                                                                                                                                            | Description                                                                                            | Required action                                                         |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------- |
| [`POST`](/v1.3/api-reference/any-to-video-search/make-search-request)  `/search`                                                                                                  | The `conversation_option` parameter has been deprecated.                                               | Remove `conversation_option` from your  search requests .               |
| - [`POST`](/v1.3/api-reference/any-to-video-search/make-search-request)  `/search` 
 - [`GET`](/v1.3/api-reference/any-to-video-search/retrieve-page) `/search/{page-token}` | The `page_info.page_expired_at` field in the response has been renamed to `page_info.page_expires_at`. | Update your code to use `page_expires_at` instead of `page_expired_at`. |
| - [`POST`](/v1.3/api-reference/any-to-video-search/make-search-request) `/search` 
 - [`GET`](/v1.3/api-reference/any-to-video-search/retrieve-page) `/search/{page-token}`  | The `metadata` and `modules` fields in the response have been deprecated.                              | Update your code - the platform no longer returns these fields.         |

## The Generate API has been renamed to the Analyze API

The Generate API has been renamed to the Analyze API to more accurately reflect its purpose of analyzing videos to generate text. This update includes changes to specific API endpoints and SDK methods, outlined below. You can continue using the Generate API until July 30, 2025. After this date, the Generate API will be deprecated, and you must transition to the Analyze API.

**API endpoint changes**:

* The `/generate` endpoint is now the `/analyze` endpoint.
* The `/gist` endpoint remains unchanged.
* The `/summarize` endpoint remains unchanged.

**SDK method changes**:

The `generate` prefix has been removed from method names, and the methods below have been renamed as follows:

* `generate.gist` is now `gist`
* `generate.summarize` is now `summarize`
* `generate.text` is now `analyze`
* `generate.text_stream` is now `analyze_stream ` (Python)
* `generate.textStream` is now `analyzeStream` (Node.js)

**Parameter changes**:

| Method                                                           | Description                                    | Required action                                                                                                                          |
| :--------------------------------------------------------------- | :--------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------- |
| [`POST `](/v1.3/api-reference/analyze-videos/analyze) `/analyze` | The `stream` parameter now defaults to `true`. | Review your logic. Set the `stream `parameter to `false` to turn off streaming responses. Otherwise, the platform will stream responses. |

To maintain compatibility, update your API calls and SDK methods to the new names before July 30, 2025. For additional details, refer to the following resources:

* [API Reference](/api-reference/analyze-videos)
* [Python SDK Reference](/sdk-reference/python/analyze-videos)
* [Node.js SDK Reference](/sdk-reference/node-js/analyze-videos)

# Non-breaking changes

These changes add new functionality while maintaining backward compatibility.

## Upload videos

| Endpoint                                                                                                                                                                                 | Description                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------- |
| - [`POST`](/v1.3/api-reference/tasks/create)  `/tasks`  
- [`GET`](/v1.3/api-reference/tasks/list)  `/tasks` 
- [`GET`](/v1.3/api-reference/tasks/retrieve)  `/tasks/{task-id` | These endpoints now return a field named `video_id`  with the unique identifier of your video.                                |
| [`GET`](/v1.3/api-reference/tasks/list)  `/tasks`                                                                                                                                        | This method now accepts a query parameter named `status`. You can use it to filter your video indexing tasks by their status. |

# Migration steps

Migrating to v1.3 involves two main steps:

1. Update your integration
2. Update your code. Refer to the [Migration Examples](#2-migration-examples) setion for details.

## 1. Update your integration

Choose the appropriate method based on how you interact with the TwelveLabs API:

* **Official SDKs**: Install version 0.4.x or later.
* **HTTP client**: Update your base URL.


  ```shell Python SDK
  pip3 install twelvelabs --upgrade
  ```

  ```shell Node.js SDK
   npm install twelvelabs-js@latest
  ```

  ```shell cURL
  API_URL="https://api.twelvelabs.io/v1.3"
  ```


## 2. Migration examples

Below are examples showing how to update your code for key breaking changes. Choose the examples matching your integration type.

### Create indexes

Creating an index in version 1.3 includes the following key changes:

* **Renamed parameters**: The parameters that previously began with `engine*` have now been renamed to `model*`.
* **Simplified modalities**: The previous modalities of \[`visual`, `conversation`, `text_in_video`, `logo`] have been simplified to \[`visual`, `audio`].
* **Marengo version update**: Use "marengo2.7" instead of "marengo2.6".


  ```python Python SDK
  models = [
          {
            "name": "marengo2.7",
            "options": ["visual", "audio"]
          },
          {
              "name": "pegasus1.2",
              "options": ["visual", "audio"]
          }
      ]
  index = client.index.create(
      name="",
      models=models,
      addons=["thumbnail"] # Optional
  )
  ```

  ```javascript Node.js SDK
  const models = [
    {
      name: "marengo2.7",
      options: ["visual", "audio"],
    },
    {
      name: "pegasus1.2",
      options: ["visual", "audio"],
    },
  ];

  let index = await client.index.create({
    name: "",
    models: models,
    addons: ["thumbnail"], // Optional
  });
  ```

  ```shell cURL
  curl --request POST \
    --url https://api.twelvelabs.xyz/v1.3/indexes \
    --header 'Content-Type: application/json' \
    --header 'x-api-key: ' \
    --data '{
    "models": [
      {
        "model_name": "marengo2.7",
        "model_options": ["visual", "audio"]
      },
      {
        "model_name": "pegasus1.2",
        "model_options": ["visual", "audio"]
      }
    ],
    "addons": ["thumbnail"],
    "index_name":""
  }'
  ```


### Perform a search request

Performing a search request includes the following key changes:

* **Simplified modalities**: The previous modalities of \[`visual`, `conversation`, `text_in_video`, `logo`] have been simplified to \[`visual`, `audio`].
* **Deprecated parameter**: The `conversation_option` parameter has been deprecated.
* **Streamlined response**: The `metadata` and `modules` fields in the response have been deprecated.


  ```python Python SDK
  search_results = client.search.query(
    index_id="", 
    query_text="", 
    options=["visual", "audio"]
  )
  ```

  ```javascript Node.js SDK
  let searchResults = await client.search.query({
    indexId: "",
    queryText: "",
    options: ["visual"],
  });
  ```

  ```shell cURL
  curl --request POST \
    --url https://api.twelvelabs.xyz/v1.3/search \
    --header 'Content-Type: multipart/form-data' \
    --header 'x-api-key: ' \
    --form index_id= \
    --form search_options=visual \
    --form search_options=conversation \
    --form 'query_text='
  ```


### Create embeddings

Creating embeddings includes the following key changes:

* **Marengo version update**: Use "Marengo-retrieval-2.7" instead of "Marengo-retrieval-2.6".
* **Renamed  parameter**: The parameters that previously began with `engine*` have now been renamed to `model*`.

The following example creates a text embedding, but the principles demonstrated are similar for image, audio, and video embeddings:


  ```python Python SDK
  res = client.embed.create(
    model_name="Marengo-retrieval-2.7",
    text="",
  )
  ```

  ```javascript Node.js SDK
  let res = await client.embed.create({
    modelName: "Marengo-retrieval-2.7",
    text: "",
  });
  ```

  ```shell cURL
  curl --request POST \
    --url https://api-dev.twelvelabs.xyz/v1.3/embed \
    --header 'Content-Type: multipart/form-data' \
    --header 'x-api-key: t'
  ```


### Use Pegasus to classify videos

The Pegasus video understanding model analyzes video content and generates descriptive text to enable flexible video classification. You can use established category systems like [YouTube video categories](https://developers.google.com/youtube/v3/docs/videoCategories/list) or [IAB Tech Lab Content Taxonomy](https://iabtechlab.com/standards/content-taxonomy/) . You can also define custom categories for your specific needs.

The example below classifies a video based on YouTube's video categories:


  ```python Python
  res = client.analyze(
    video_id="",
    prompt="Classify this video using up to five labels from YouTube standard content categories. Provide the results in the JSON format."
  )
  ```

  ```JavaScript Node.js SDK
  const text = await client.analyze(
    "",
    "Classify this video using up to five labels from YouTube standard content categories. Provide the results in the JSON format.",
  );
  ```

  ```shell cURL
  curl --request POST \
    --url https://api.twelvelabs.xyz/v1.3/analyze \
    --header 'Content-Type: application/json' \
    --header 'x-api-key: ' \
    --data '{
    "video_id": "",
    "prompt": "Classify this video using up to five labels from YouTube standard content categories. Provide the results in the JSON format."
  }'
  ```


### Detect logos

You can search for logos using  text  or image queries:

* **Text queries**: For logos that include text (example: Nike)
* **Image queries**: For logos without text (example: Apple's apple symbol).

The following example searches for the Nike logo using a text query:


  ```python Python SDK
  search_results = client.search.query(
    index_id="", 
    query_text="Nike", 
    options=["visual"]
  )
  ```

  ```javascript Node.js SDK
  let searchResults = await client.search.query({
    indexId: "",
    queryText: "Nike",
    options: ["visual"],
  });
  ```

  ```shell cURL
  curl --request POST \
    --url https://api.twelvelabs.xyz/v1.3/search \
    --header 'Content-Type: multipart/form-data' \
    --header 'x-api-key: ' \
    --form index_id= \
    --form search_options=visual \
    --form 'query_text=Nike'
  ```


The following example searches for the Apple logo using an image query:


  ```python Python SDK
  search_results = client.search.query(
      index_id="",
      query_media_type="image",
      query_media_url="https://logodownload.org/wp-content/uploads/2013/12/apple-logo-16.png,
      options=["visual"]
  )
  ```

  ```javascript Node.js SDK
  let searchResults = await client.search.query({
    indexId: "",
    queryMediaType: "image",
    queryMediaUrl: "https://logodownload.org/wp-content/uploads/2013/12/apple-logo-16.png",
    options: ["visual"]
  });
  ```

  ```shell cURL
  curl --request POST \
    --url https://api.twelvelabs.io/v1.3/search \
    --header 'Content-Type: multipart/form-data' \
    --header 'x-api-key: ' \
    --form index_id= \
    --form search_options=visual \
    --form query_media_type=image \
    --form query_media_url=https://logodownload.org/wp-content/uploads/2013/12/apple-logo-16.png
  ```


### Search for text shown in videos

To search for text in videos, use text queries that target either on-screen text or spoken words in transcriptions rather than objects or concepts. The platform searches across both:

* Text shown on screen (such as titles, captions, or signs)
* Spoken words from audio transcriptions

Note that the platform may return both textual and visual matches. For example, searching for the word "smartphone" might return:

* Segments where "smartphone" appears as on-screen text.
* Segments where "smartphone" is spoken.
* Segments where smartphones are visible as objects.

The example below finds all the segments where the word "innovation" appears as on-screen text or as a spoken word in transcriptions:


  ```javascript Python SDK
  search_results = client.search.query(
    index_id="", 
    query_text="Innovation", 
    options=["visual"]
  )
  ```

  ```python Node.js SDK
  let searchResults = await client.search.query({
    indexId: ""
    queryText: "Innovation",
    options: ["visual"],
  });
  ```

  ```shell cURL
  curl --request POST \
    --url 'https://api.twelvelabs.io/v1.3/search' \
    --header 'Content-Type: multipart/form-data' \
    --header 'x-api-key: ' \
    --form 'query_text=Innovation' \
    --form index_id= \
    --form search_options=visual \
  ```


# Additional resources


  

  

  

  



# Python SDK

{/* 

The 1.3 version of the API includes significant improvements and introduces breaking changes. If you are using v1.2, refer to the [Migration guide](/v1.3/docs/resources/migration-guide) page for a detailed list of changes, migration instructions, and code examples.

You must use SDK version 0.4.x or later to access API version 1.3.
 */}

The [TwelveLabs Python SDK](https://github.com/twelvelabs-io/twelvelabs-python) provides a robust interface for interacting with the TwelveLabs Video Understanding Platform. It simplifies authentication and efficiently processes asynchronous tasks.


  * This section provides an overview of the key methods and their usage. However, it is a partial list of all available methods and fields in the SDK.
  * The SDK is designed to closely follow the structure of the TwelveLabs API. In most cases, you'll find a corresponding method in the SDK for each method in the API.


# Sample applications

To get started quickly, see the sample applications below that demonstrate the capabilities of the TwelveLabs Python SDK. These applications show how to implement common use cases and the best practices for using the SDK.

* [Building an Olympic Video Classification Application with TwelveLabs](https://www.twelvelabs.io/blog/olympics-classification)


# Typical workflows

This page provides an overview of common workflows for using the TwelveLabs Python SDK. Each workflow consists of a series of steps, with links to detailed documentation for each step.

All workflows involving uploading video content to the platform require asynchronous processing. You must wait for the video processing to complete before proceeding with the subsequent steps.

# Prerequisites

Ensure that the following prerequisites are met before using the Python SDK:

* [Python](https://www.python.org) 3.7 or newer must be installed on your machine.
* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

# Install the SDK

Install the `twelvelabs` package:

```shell Shell
pip install twelvelabs
```

# Initialize the SDK

1. Import the required packages:

```python Python
from twelvelabs import TwelveLabs
```

2. Instantiate the SDK client with your API key.:

```python Python
client = TwelveLabs(api_key="")
```

# Search

Follow the steps in this section to search your video content for specific moments, scenes, or information.

**Steps**:

1. [Create an index](/v1.3/sdk-reference/python/manage-indexes#create-an-index), enabling the Marengo video understanding model.
2. [Upload videos](/v1.3/sdk-reference/python/upload-videos#create-a-video-indexing-task) and monitor the processing.
3. [Perform a search request](/v1.3/sdk-reference/python/search#make-a-search-request),  using text or images as queries.


  * The search scope is an individual index.
  * Results support pagination, filtering, sorting, and grouping.


# Create text, image, and audio embeddings

This workflow guides you through creating embeddings for text.

**Steps**:

1. [Create text, image, and audio embeddings](/v1.3/sdk-reference/python/create-text-image-and-audio-embeddings)


  Creating text, image, and audio embeddings is a synchronous process.


# Create video embeddings

This workflow guides you through creating embeddings for videos.

**Steps**:

1. [Upload a video](/v1.3/sdk-reference/python/create-video-embeddings#create-a-video-embedding-task) and monitor the processing.
2. [Retrieve the embeddings](/v1.3/sdk-reference/python/create-video-embeddings#retrieve-video-embeddings).


  Creating video  embeddings is an asynchronous process.


# Analyze videos

Follow the steps in this section to analyze your videos and generate text based on their content.

**Steps**:

1. [Create an index](/v1.3/sdk-reference/python/manage-indexes#create-an-index), enabling the Pegasus video understanding model.
2. [Upload videos](/v1.3/sdk-reference/python/upload-videos#create-a-video-indexing-task) and monitor the processing.
3. Depending on your use case:

* [Generate titles, topics and hastags](/v1.3/sdk-reference/python/analyze-videos#titles-topics-and-hashtags)
* [Generate Summaries, chapters, and highlights](/v1.3/sdk-reference/python/analyze-videos#summaries-chapters-and-highlights)
* [Perform open-ended analysis](/v1.3/sdk-reference/python/analyze-videos#open-ended-analysis).


# The TwelveLabs class

The [`TwelveLabs`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/client.py)  class is the main entry point for the SDK. It initializes the client and provides access to all the resources.

# Properties

| Name     | Type                                                                                                              | Description                                 |
| -------- | ----------------------------------------------------------------------------------------------------------------- | ------------------------------------------- |
| `index`  | [`resources.Index`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/index.py)   | Use this object to manage your indexes.     |
| `task`   | [`resources.Task`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/task.py)     | Use this object to upload videos.           |
| `search` | [`resources.Search`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/search.py) | Use this object to perform search requests. |
| `embed`  | [`resources.Embed`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/embed.py)   | Use this object to create embeddings.       |

# Methods

## The initializer

**Description**: The constructor creates a new instance of the [TwelveLabs class](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/client.py).

**Function signature and example**:


  ```python Function signature
  def __init__(
      self,
      api_key: str,
      version: Union[str, Literal["v1.1", "v1.2", "v1.3"]] = DEFAULT_API_VERSION
  ) -> None
  ```

  ```python Python example
  client = TwelveLabs(api_key="")
  ```


**Parameters**:

| Name      | Type                              | Required | Description                                             |
| :-------- | :-------------------------------- | :------- | :------------------------------------------------------ |
| `api_key` | `str`                             | Yes      | Your TwelveLabs API key.                                |
| `version` | `Literal["v1.1", "v1.2", "v1.3"]` | No       | The API version to use. Defaults to the latest version. |

**Return value**:  None. This method initializes the instance.


# Manage indexes

An index is a basic unit for organizing and storing video data consisting of video embeddings and metadata. Indexes facilitate information retrieval and processing. The [`resources.Index`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/index.py) class provides methods to manage your indexes.

# Properties

| Name    | Type                                                                                                            | Description                                                    |
| :------ | :-------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------- |
| `video` | [`resources.Video`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/video.py) | Use this property to manage the videos uploaded to this index. |

# Methods

## Create an index

**Description**: This method creates a new index based on the provided parameters.

**Function signature and example**:


  ```python Function signature
  def create(
      self,
      name: str,
      models: List[types.IndexModel],
      *,
      addons: Optional[List[str]] = None,
      **kwargs
  ) -> models.Index
  ```

  ```python Python example
  models = [
          {
            "name": "marengo2.7",
            "options": ["visual", "audio"]
          },
          {
              "name": "pegasus1.2",
              "options": ["visual", "audio"]
          }
      ]
  created_index = client.index.create(
      name="",
      models=models,
      addons=["thumbnail"]
  )
  print(f"ID: {created_index.id}")
  print(f"Name: {created_index.name}")
  print("Models:")
  for i, model in enumerate(created_index.models, 1):
      print(f"  Model {i}:")
      print(f"    Name: {model.name}")
      print(f"    Options: {model.options}")
  print(f"Video count: {created_index.video_count}")
  print(f"Total duration: {created_index.total_duration} seconds")
  print(f"created At: {created_index.created_at}")
  if created_index.updated_at:
      print(f"Updated at: {created_index.updated_at}")
  ```


**Parameters**

| Name       | Type                     | Required | Description                                                                                                                                                                                                                                                                                      |
| :--------- | :----------------------- | :------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `name`     | `str`                    | Yes      | The name of the new index.                                                                                                                                                                                                                                                                       |
| `options`  | `List[types.IndexModel]` | Yes      | A list of  [`IndexModel`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/types/index.py)  objects specifying the video understanding models and the model options you want to enable for this index. Each  object is a dictionary with two keys:  `name` and `options`. |
| `addons`   | `Optional[List[str]]`    | No       | A list specifying which add-ons should be enabled.                                                                                                                                                                                                                                               |
| `**kwargs` | `dict`                   | No       | Additional keyword arguments for the request.                                                                                                                                                                                                                                                    |

**Return value**: Returns a [`models.Index`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/index.py)  object representing the newly created index.

**API Reference**: For a description of each field in the request and response, see the [Create an index](/v1.3/api-reference/indexes/create) page.

**Related guide**: [Create indexes](/v1.3/docs/concepts/indexes).

## Retrieve an index

**Description**: This method retrieves details of a specific index.

**Function signature and example**:


  ```python Function signature
  def retrieve(self, id: str, **kwargs) -> models.Index
  ```

  ```javascript Python example
  retrieved_index = client.index.retrieve("")
  print(f"ID: {retrieved_index.id}")
  print(f"Name: {retrieved_index.name}")
  print("Models:")
  for i, model in enumerate(retrieved_index.models, 1):
      print(f"  Model {i}:")
      print(f"    Name: {model.name}")
      print(f"    Options: {model.options}")
  print(f"Video count: {retrieved_index.video_count}")
  print(f"Total duration: {retrieved_index.total_duration} seconds")
  print(f"Created at: {retrieved_index.created_at}")
  if retrieved_index.updated_at:
      print(f"Updated at: {retrieved_index.updated_at}")
  if retrieved_index.expires_at:
      print(f"Expires at: {retrieved_index.expires_at}")
  ```


**Parameters**

| Name       | Type   | Required | Description                                           |
| :--------- | :----- | :------- | :---------------------------------------------------- |
| `id`       | `str`  | Yes      | The unique identifier of the index to retrieve.       |
| `**kwargs` | `dict` | No       | Additional options for the request. Defaults to `{}`. |

**Return value**: Returns a [`models.Index`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/index.py) object representing the retrieved index.

**API Reference**: For a description of each field in the request and response, see the [Retrieve an index](/v1.3/api-reference/indexes/retrieve) page.

## List indexes with direct pagination

**Description**: The list method retrieves a paginated list of indexes based on the provided parameters. Choose this method mainly when the total number of items is manageable or you must fetch a single page of results. By default, the platform returns your indexes sorted by creation date, with the newest at the top of the list.

**Function signature and example**:


  ```python Function signature
  def list(
      self,
      *,
      id: Optional[str] = None,
      name: Optional[str] = None,
      model_options: Optional[List[Union[str, Literal["visual", "audio"]]]] = None,
      model_family: Optional[Union[str, Literal["marengo", "pegasus"]]] = None,
      page: Optional[int] = 1,
      page_limit: Optional[int] = 10,
      sort_by: Optional[str] = "created_at",
      sort_option: Optional[str] = "desc",
      created_at: Optional[Union[str, Dict[str, str]]] = None,
      updated_at: Optional[Union[str, Dict[str, str]]] = None,
      **kwargs
  ) -> RootModelList[models.Index]
  ```

  ```python Python example
  indexes = client.index.list(
      id="",
      name="",
      page=1,
      page_limit=5,
      model_options=["visual", "audio"],
      model_family="marengo",
      sort_by = "updated_at",
      sort_option="asc",
      created_at="2024-09-17T07:53:46.365Z",
      updated_at="2024-09-17T07:53:46.365Z"
  )
  for index in indexes:
      print(f"ID: {index.id}")
      print(f"  Name: {index.name}")
      print("  Models:")
      for i, model in enumerate(index.models, 1):
          print(f"    Model {i}:")
          print(f"      Name: {model.name}")
          print(f"      Options: {model.options}")
      print(f"  Video count: {index.video_count}")
      print(f"  Total duration: {index.total_duration} seconds")
      print(f"  Created at: {index.created_at}")
      if index.updated_at:
          print(f"  Updated at: {index.updated_at}")
  ```


**Parameters**

| Name            | Type                                             | Required | Description                                                                                |
| :-------------- | :----------------------------------------------- | :------- | :----------------------------------------------------------------------------------------- |
| `id`            | `Optional[str]`                                  | No       | Filter by the unique identifier of an index.                                               |
| `name`          | `Optional[str]`                                  | No       | Filter by the name of an index.                                                            |
| `model_options` | `Optional [Literal["visual", "audio"]] = None`   | No       | Filter by model options.                                                                   |
| `model_family`  | `Optional[Literal["marengo", "pegasus"]] = None` | No       | Filter by model family.                                                                    |
| `page`          | `Optional[int]`                                  | No       | Page number for pagination. Defaults to 1.                                                 |
| `page_limit`    | `Optional[int]`                                  | No       | Number of items per page. Defaults to 10.                                                  |
| `sort_by`       | `Optional[str]`                                  | No       | Field to sort by ("created\_at" or "updated\_at"). Defaults to "created\_at".              |
| `sort_option`   | `Optional[str]`                                  | No       | Sort order ("asc" or "desc"). Defaults to "desc".                                          |
| `created_at`    | `Optional[Union[str, Dict[str, str]]] = None`    | No       | Filter by creation date. This parameter can be a string or a dictionary for range queries. |
| `updated_at`    | `Optional[Union[str, Dict[str, str]]] = None`    | No       | Filter by update date. This parameter can be a string or a dictionary for range queries.   |
| `**kwargs`      | `dict`                                           | No       | Additional keyword arguments for the request.                                              |

**Return value**: Returns a [`RootModelList`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/_base.py)  containing [`models.Index`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/index.py) objects, representing the list of indexes that match the specified criteria.

**API Reference**: For a description of each field in the request and response, see the [List indexes](/v1.3/api-reference/indexes/list) page.

## List indexes with iterative pagination

**Description**: This method iterates through a paginated list of indexes based on the provided parameters. Choose this method mainly when your application must retrieve a large number of items. By default, the platform returns your indexes sorted by creation date, with the newest at the top of the list.

**Function signature and example**:


  ```python Function signature
  def list_pagination(
      self,
      *,
      id: Optional[str] = None,
      name: Optional[str] = None,
      model_options: Optional[Literal["visual", "audio"]] = None,
      model_family: Optional[Union[str, Literal["marengo", "pegasus"]]] = None,
      page: Optional[int] = 1,
      page_limit: Optional[int] = 10,
      sort_by: Optional[str] = "created_at",
      sort_option: Optional[str] = "desc",
      created_at: Optional[Union[str, Dict[str, str]]] = None,
      updated_at: Optional[Union[str, Dict[str, str]]] = None,
      **kwargs
  ) -> models.IndexListWithPagination
  ```

  ```python Python example
  def print_page(page):
      for index in page:
        print(f"ID: {index.id}")
        print(f"  Name: {index.name}")
        print("  Models:")
        for i, model in enumerate(index.models, 1):
            print(f"    Model {i}:")
            print(f"      Name: {model.name}")
            print(f"      Options: {model.options}")
        print(f"  Video count: {index.video_count}")
        print(f"  Total duration: {index.total_duration} seconds")
        print(f"  Created at: {index.created_at}")
        if index.updated_at:
            print(f"  Updated at: {index.updated_at}")

  # Fetch the initial page of results
  index_paginator = client.index.list_pagination(
      id="",
      name="",
      page=1,
      page_limit=5,
      model_options=["visual", "audio"],
      model_family="marengo",
      sort_by = "updated_at",
      sort_option="asc",
      created_at="2024-09-17T07:53:46.365Z",
      updated_at="2024-09-17T07:53:46.365Z"
  )

  # Print the first page of results
  print_page(index_paginator.data)

  # Iterate through subsequent pages
  while True:
      try:
          next_index_page = next(index_paginator)
          print_page(next_index_page)
      except StopIteration:
          break
     
  ```


**Parameters**

| Name            | Type                                             | Required | Description                                                                                |
| :-------------- | :----------------------------------------------- | :------- | :----------------------------------------------------------------------------------------- |
| `id`            | `Optional[str]`                                  | No       | Filter by the unique identifier of an index.                                               |
| `name`          | `Optional[str]`                                  | No       | Filter by the name of an index.                                                            |
| `model_options` | `Optional[Literal["visual", "audio"]] = None`    | No       | Filter by model options.                                                                   |
| `model_family`  | `Optional[Literal["marengo", "pegasus"]] = None` | No       | Filter by model family.                                                                    |
| `page`          | `Optional[int]`                                  | No       | Page number for pagination. Defaults to 1.                                                 |
| `page_limit`    | `Optional[int]`                                  | No       | Number of items per page. Defaults to 10.                                                  |
| `sort_by`       | `Optional[str]`                                  | No       | Field to sort by ("created\_at" or "uploaded\_at"). Defaults to "created\_at".             |
| `sort_option`   | `Optional[str]`                                  | No       | Sort order ("asc" or "desc"). Defaults to "desc".                                          |
| `created_at`    | `Optional[Union[str, Dict[str, str]]]`           | No       | Filter by creation date. This parameter can be a string or a dictionary for range queries. |
| `updated_at`    | `Optional[Union[str, Dict[str, str]]]`           | No       | Filter by update date. This parameter can be a string or a dictionary for range queries.   |
| `**kwargs`      | `dict`                                           | No       | Additional keyword arguments for the request.                                              |

**Return value**:  Returns a [`models.IndexListWithPagination`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/index.py)  object, containing the list of indexes that match the specified criteria and pagination information.


  To retrieve subsequent pages of results, use the iterator protocol:

  1. Invoke the `next` function, passing the `IndexListWithPagination` object as a parameter.
  2. Repeat this call until a `StopIteration` exception occurs, indicating no more pages exist.


**API Reference**: For a description of each field in the request and response, see the [List indexes](/v1.3/api-reference/indexes/list) page.

## Update an index

**Description**: This method updates the name of an existing index.

**Function signature and example**:


  ```python Function signature
  def update(self, id: str, name: str, **kwargs) -> None
  ```

  ```python Python example
  client.index.update(id="", name="")
  ```


**Parameters**:

| Name       | Type   | Required | Description                                       |
| :--------- | :----- | :------- | :------------------------------------------------ |
| `id`       | `str`  | Yes      | The unique identifier of the index to be updated. |
| `name`     | `str`  | Yes      | The new name of the index.                        |
| `**kwargs` | `dict` | No       | Additional keyword arguments for the request.     |

**Return value**: `None`. This method doesn't return any data upon successful completion.

**API Reference**: [Update an index](/v1.3/api-reference/indexes/update).

## Delete an index

**Description**: This method deletes an existing index.

**Function signature and example**:


  ```python Function signature
  def delete(self, id: str, **kwargs) -> None
  ```

  ```python Python  example
  client.index.delete(id="")
  ```


**Parameters**:

| Name       | Type     | Required | Description                                   |
| :--------- | :------- | :------- | :-------------------------------------------- |
| `id`       | `string` | Yes      | The unique identifier of the index to delete. |
| `**kwargs` | `dict`   | No       | Additional keyword arguments for the request. |

**Return value**:  `None`. This method doesn't return any data upon successful completion.

**API Reference**: [Delete an index](/v1.3/api-reference/indexes/delete)

# Error codes

This section lists the most common error messages you may encounter while managing indexes.

* `index_option_cannot_be_changed`
  * Index option cannot be changed. Please remove index\_options parameter and try again. If you want to change index option, please create new index.
* `index_engine_cannot_be_changed`
  * Index engine cannot be changed. Please remove engine\_id parameter and try again. If you want to change engine, please create new index.
* `index_name_already_exists`
  * Index name `{index_name}` already exists. Please use another unique name and try again.


# Upload videos

A video indexing task represents a request to upload and index a video. The [`resources.Task`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/task.py) class provides methods to manage your video indexing tasks.

# Methods

## Create a video indexing task

**Description**: This method creates a new video indexing task that uploads and indexes a video.


  The videos you wish to upload must meet the following requirements:

  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the [FFmpeg Formats Documentation](https://ffmpeg.org/ffmpeg-formats.html) page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).
  * **Duration**: For Marengo, it must be between 4 seconds and 2 hours (7,200s). For Pegasus, it must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration for Pegasus will be 2 hours (7,200 seconds).
  * **File size**: Must not exceed 2 GB.
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  If both Marengo and Pegasus are enabled for your index, the most restrictive prerequisites will apply.


**Function signature and example**:


  ```python Function signature
  def create(
      self,
      index_id: str,
      *,
      file: Union[str, BinaryIO, None] = None,
      url: Optional[str] = None,
      enable_video_stream: Optional[bool] = None,
      **kwargs,
  ) -> models.Task
  ```

  ```python Python example
  from twelvelabs.models.task import Task

  task = client.task.create(
    index_id=<"",
    file=""
  )
  print(f"Task id={task.id}")

  # Utility function to print the status of a video indexing task
  def on_task_update(task: Task):
        print(f"  Status={task.status}")

  task.wait_for_done(sleep_interval=5, callback=on_task_update)

  if task.status != "ready":
    raise RuntimeError(f"Indexing failed with status {task.status}")
  print(f"Video ID: {task.video_id}")
  ```


**Parameters**:

| Name                  | Type                         | Required | Description                                                             |
| --------------------- | ---------------------------- | -------- | ----------------------------------------------------------------------- |
| `index_id`            | `str`                        | Yes      | The unique identifier of the index to which the video will be uploaded. |
| `file`                | `Union[str, BinaryIO, None]` | No       | Path to the video file or a file-like object.                           |
| `url`                 | `Optional[str]`              | No       | The publicly accessible URL of the video you want to upload.            |
| `enable_video_stream` | `Optional[str]`              | No       | Indicates if the platform stores the video for streaming.               |
| `**kwargs`            | `dict`                       | No       | Additional keyword arguments for the request.                           |

**Return value**: Returns a [`models.Task`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/task.py) instance representing the newly created video indexing task.


  As shown in the example code above, you can use the  [`waitForDone`](https://github.com/twelvelabs-io/twelvelabs-python/blob/ca38803e3b82d42e64a8d255b24bc579550c94de/twelvelabs/models/task.py)  method of the [`models.Task`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/task.py)  class to monitor the status of a video indexing task until it completes.


**API Reference**: For a description of each field in the request and response, see the [Create a video indexing task](/v1.3/api-reference/tasks/create) page.

## Retrieve a video indexing task

**Description**: This method retrieves the details of a specific task.

**Function signature and example**:


  ```python Function signature
  def retrieve(self, id: str, **kwargs) -> models.Task
  ```

  ```javascript Python example
  retrieved_task = client.task.retrieve("")

  print(f"Task ID: {retrieved_task.id}")
  print(f"Index ID: {retrieved_task.index_id}")
  print(f"Video ID: {retrieved_task.video_id}")
  print(f"Status: {retrieved_task.status}")
  print("System metadata:")
  for key, value in retrieved_task.system_metadata.items():
      print(f"  {key}: {value}")

  if retrieved_task.hls:
      print("HLS:")
      print(f"  Video URL: {retrieved_task.hls.video_url}")
      print("  Thumbnail URLs:")
      for url in retrieved_task.hls.thumbnail_urls or []:
          print(f"    {url}")
      print(f"  Status: {retrieved_task.hls.status}")
      print(f"  Updated at: {retrieved_task.hls.updated_at}")

  if retrieved_task.process:
      print("Process:")
      print(f"  Percentage: {retrieved_task.process.percentage}%")
      print(f"  Remaining Seconds: {retrieved_task.process.remain_seconds}")

  print(f"Created at: {retrieved_task.created_at}")
  print(f"Updated at: {retrieved_task.updated_at}")
  ```


**Parameters**

| Name       | Type   | Required | Description                                    |
| :--------- | :----- | :------- | :--------------------------------------------- |
| `id`       | `str`  | Yes      | The unique identifier of the task to retrieve. |
| `**kwargs` | `dict` | No       | Additional keyword arguments for the request.  |

**Return value**: Returns a [`models.Task`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/task.py) object representing the retrieved video indexing task.

**API Reference**: For a description of each field in the response, see the [Retrieve a video indexing task](/v1.3/api-reference/tasks/retrieve).

## Wait for a video indexing task to complete

**Description**: This method waits until a video indexing task is completed. It checks the task status at regular intervals by retrieving its details. If you provide a callback function, the method calls it after each status check with the current task object. This allows you to monitor the progress of the task.

**Function signature and example**:


  ```python Function signature
  def wait_for_done(
      self,
      *,
      sleep_interval: float = 5.0,
      callback: Optional[Callable[[Task], None]] = None,
      **kwargs,
  ) -> Task
  ```

  ```Python Python example
  def on_task_update(task: Task):
      print(f"  Status={task.status}")

  task.wait_for_done(sleep_interval=5, callback=on_task_update)
  ```


**Parameters**

| Name             | Type                               | Required | Description                                                                                                                                 |
| :--------------- | :--------------------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------ |
| `sleep_interval` | `float`                            | No       | Sets the time in seconds to wait between status checks. Must be greater than 0. Default is 5.0.                                             |
| `callback`       | `Optional[Callable[[Task], None]]` | No       | Provides an optional function to call after each status check. The function receives the current task object. Use this to monitor progress. |
| `**kwargs`       | `dict`                             | No       | Passes additional keyword arguments to the `retrieve` method when checking the task status.                                                 |

**Return value**: Returns the `Task` object after the task is completed.

## List video indexing tasks with direct pagination

**Description**: This method returns a list of the video indexing tasks in your account. By default, the platform returns your video indexing tasks sorted by creation date, with the newest at the top of the list. Choose this method mainly when the total number of items is manageable, or you must fetch a single page of results.

**Function signature and example**:


  ```python Function signature
  def list(
      self,
      *,
      id: Optional[str] = None,
      index_id: Optional[str] = None,
      filename: Optional[str] = None,
      duration: Optional[float] = None,
      width: Optional[int] = None,
      height: Optional[int] = None,
      created_at: Optional[Union[str, Dict[str, str]]] = None,
      updated_at: Optional[Union[str, Dict[str, str]]] = None,
      page: Optional[int] = None,
      page_limit: Optional[int] = None,
      sort_by: Optional[str] = None,
      sort_option: Optional[str] = None,
      **kwargs,
  ) -> RootModelList[models.Task]:
  ```

  ```python Python example
  tasks = client.task.list(
    id="",
    index_id="",
    filename="",
    duration=20,
    width=1920,
    height=1080,
    sort_by = "updated_at",
    sort_option="asc",
    created_at="2024-09-17T07:53:46.365Z",
    updated_at="2024-09-17T07:53:46.365Z",
    page=2,
    page_limit=5,
  )
  for task in tasks:
    print(f"Task ID: {task.id}")
    print(f"Index ID: {task.index_id}")
    print(f"Video ID: {task.video_id}")
    print(f"Estimated time: {task.estimated_time}")
    print(f"Status: {task.status}")
    print("System metadata:")
    for key, value in task.system_metadata.items():
        print(f"  {key}: {value}")

    if task.hls:
        print("HLS:")
        print(f"  Video URL: {task.hls.video_url}")
        print("  Thumbnail URLs:")
        for url in task.hls.thumbnail_urls or []:
            print(f"    {url}")
        print(f"  Status: {task.hls.status}")
        print(f"  Updated at: {task.hls.updated_at}")
        
    if task.process:
        print("Process:")
        print(f"  Percentage: {task.process.percentage}%")
        print(f"  Remaining Seconds: {task.process.remain_seconds}") 

    print(f"Created at: {task.created_at}")
    print(f"Updated at: {task.updated_at}")
  ```


**Parameters**:

| Name          | Type                                   | Required | Description                                                                                                                               |
| ------------- | -------------------------------------- | -------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| `id`          | `Optional[str]`                        | No       | Filter by the unique identifier of a video indexing task.                                                                                 |
| `index_id`    | `Optional[str]`                        | No       | Filter by the unique identifier of an index.                                                                                              |
| `filename`    | `Optional[str]`                        | No       | Filter by filename.                                                                                                                       |
| `duration`    | `Optional[float]`                      | No       | Filter by duration expressed in seconds.                                                                                                  |
| `width`       | `Optional[int]`                        | No       | Filter by width.                                                                                                                          |
| `height`      | `Optional[int]`                        | No       | Filter by height.                                                                                                                         |
| `created_at`  | `Optional[Union[str, Dict[str, str]]]` | No       | Filter by the creation date of the task. This parameter can be a string or a dictionary with string keys and values for range queries.    |
| `updated_at`  | `Optional[Union[str, Dict[str, str]]]` | No       | Filter by the last update date of the task. This parameter can be a string or a dictionary with string keys and values for range queries. |
| `page`        | `Optional[int]`                        | No       | Page number for pagination. Defaults to 1.                                                                                                |
| `page_limit`  | `Optional[int]`                        | No       | Number of items per page. Defaults to 10.                                                                                                 |
| `sort_by`     | `Optional[str]`                        | No       | Field to sort by ("created\_at" or "updated\_at"). Defaults to "created\_at".                                                             |
| `sort_option` | `Optional[str]`                        | No       | Sort order ("asc" or "desc"). Defaults to "desc".                                                                                         |
| `**kwargs`    | `dict`                                 | No       | Additional keyword arguments for the request.                                                                                             |

**Return value**:  Returns a [`RootModelList`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/_base.py)  containing  [`models.Task`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/task.py) objects representing the list of videos that match the specified criteria.

**API Reference**: For a description of each field in the request and response, see the [List video indexing tasks](/v1.3/api-reference/tasks/list) page.

## List video indexing tasks with iterative pagination

**Description**: This method returns a paginated list of the video indexing tasks in your account. Choose this method mainly when your application must retrieve a large number of items. By default, the platform returns your video indexing tasks sorted by creation date, with the newest at the top of the list.

**Function signature and example**:


  ```python Function signature
  def list_pagination(
      self,
      *,
      id: Optional[str] = None,
      index_id: Optional[str] = None,
      filename: Optional[str] = None,
      duration: Optional[float] = None,
      width: Optional[int] = None,
      height: Optional[int] = None,
      created_at: Optional[Union[str, Dict[str, str]]] = None,
      updated_at: Optional[Union[str, Dict[str, str]]] = None,
      page: Optional[int] = None,
      page_limit: Optional[int] = None,
      sort_by: Optional[str] = None,
      sort_option: Optional[str] = None,
      **kwargs,
  ) -> models.TaskListWithPagination
  ```

  ```python Python example
  def print_page(page):
      for task in page:
        print(f"Task ID: {task.id}")
        print(f"Index ID: {task.index_id}")
        print(f"Video ID: {task.video_id}")
        print(f"Status: {task.status}")
        print("System metadata:")
        for key, value in task.system_metadata.items():
            print(f"  {key}: {value}")

        if task.hls:
            print("HLS:")
            print(f"  Video URL: {task.hls.video_url}")
            print("  Thumbnail URLs:")
            for url in task.hls.thumbnail_urls or []:
                print(f"    {url}")
            print(f"  Status: {task.hls.status}")
            print(f"  Updated at: {task.hls.updated_at}")

        if task.process:
            print("Process:")
            print(f"  Percentage: {task.process.percentage}%")
            print(f"  Remaining Seconds: {task.process.remain_seconds}")

        print(f"Created at: {task.created_at}")
        print(f"Updated at: {task.updated_at}")

  # Fetch the initial page of results
  task_paginator = client.task.list_pagination(
    id="",
    index_id="",
    filename="",
    duration=20,
    width=1920,
    height=1080,
    sort_by = "updated_at",
    sort_option="asc",
    created_at="2024-09-17T07:53:46.365Z",
    updated_at="2024-09-17T07:53:46.365Z",
    page=2,
    page_limit=5,
  )

  # Print the first page of results
  print_page(task_paginator.data)

  # Iterate through subsequent pages
  while True:
      try:
          next_task_page = next(task_paginator)
          print_page(next_task_page)
      except StopIteration:
          break
  ```


**Parameters**:

| Name          | Type                                   | Required | Description                                                                                                                               |
| ------------- | -------------------------------------- | -------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| `id`          | `Optional[str]`                        | No       | Filter by the unique identifier of a video indexing task.                                                                                 |
| `index_id`    | `Optional[str]`                        | No       | Filter by the unique identifier of an index.                                                                                              |
| `filename`    | `Optional[str]`                        | No       | Filter by filename.                                                                                                                       |
| `duration`    | `Optional[float]`                      | No       | Filter by duration expressed in seconds.                                                                                                  |
| `width`       | `Optional[int]`                        | No       | Filter by width.                                                                                                                          |
| `height`      | `Optional[int]`                        | No       | Filter by height.                                                                                                                         |
| `created_at`  | `Optional[Union[str, Dict[str, str]]]` | No       | Filter by the creation date of the task. This parameter can be a string or a dictionary with string keys and values for range queries.    |
| `updated_at`  | `Optional[Union[str, Dict[str, str]]]` | No       | Filter by the last update date of the task. This parameter can be a string or a dictionary with string keys and values for range queries. |
| `page`        | `Optional[int]`                        | No       | Page number for pagination. Defaults to 1.                                                                                                |
| `page_limit`  | `Optional[int]`                        | No       | Number of items per page. Defaults to 10.                                                                                                 |
| `sort_by`     | `Optional[str]`                        | No       | Field to sort by ("created\_at" or "updated\_at"). Defaults to "created\_at".                                                             |
| `sort_option` | `Optional[str]`                        | No       | Sort order ("asc" or "desc"). Defaults to "desc".                                                                                         |
| `**kwargs`    | `dict`                                 | No       | Additional keyword arguments for the request.                                                                                             |

**Return value**: Returns a [`models.TaskListWithPagination`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/task.py)  object containing the list of videos that match the specified criteria and pagination information.


  To retrieve subsequent pages of results, use the iterator protocol:

  1. Invoke the `next` function, passing the `TaskListWithPagination` object as a parameter.
  2. Repeat this call until a `StopIteration` exception occurs, indicating no more pages exist.


**API Reference**: For a description of each field in the request and response, see the [List video indexing tasks](/v1.3/api-reference/tasks/list) page.

## Delete a video indexing task

**Description**: This method deletes an existing video indexing task.

**Function signature and example**:


  ```python Function signature
  def delete(self, id: str, **kwargs) -> None
  ```

  ```python Python example
  client.task.delete("");
  ```


**Parameters**:

| Name       | Type   | Required | Description                                                 |
| :--------- | :----- | :------- | :---------------------------------------------------------- |
| `id`       | `str`  | Yes      | The unique identifier of the video indexing task to delete. |
| `**kwargs` | `dict` | No       | Additional keyword arguments for the request.               |

**Return value**: `None`. This method doesn't return any data upon successful completion.

**API Reference**: [Delete a video indexing task](/v1.3/api-reference/tasks/delete).




## Import videos

**Description**: An import represents the process of uploading and indexing all videos from the specified integration. This method initiates an asynchronous import.


  The videos you wish to upload must meet the following requirements:

  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the [FFmpeg Formats Documentation](https://ffmpeg.org/ffmpeg-formats.html) page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).
  * **Duration**: For Marengo, it must be between 4 seconds and 2 hours (7,200s). For Pegasus, it must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration for Pegasus will be 2 hours (7,200 seconds).
  * **File size**: Must not exceed 2 GB.
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  If both Marengo and Pegasus are enabled for your index, the most restrictive prerequisites will apply.


**Function signature and example**:


  ```python Function signature
  def import_videos(
      self,
      integration_id: str,
      index_id: str,
      user_metadata: Optional[Dict[str, Any]] = None,
      incremental_import: Optional[bool] = None,
      retry_failed: Optional[bool] = None,
      **kwargs,
  ) -> models.TransferImportResponse
  ```

  ```python Python example
  res = client.task.transfers.import_videos(
      "",
      "",
  )
  for video in res.videos:
      print(f"video: {video.video_id} {video.filename}")
  if res.failed_files:
      for failed_file in res.failed_files:
          print(f"failed_file: {failed_file.filename} {failed_file.error_message}")
  ```


**Parameters**:

| Name                 | Type                              | Required | Description                                                                                                                         |
| :------------------- | :-------------------------------- | :------- | :---------------------------------------------------------------------------------------------------------------------------------- |
| `integration_id`     | `str`                             | Yes      | The unique identifier of the integration for which you want to import videos.                                                       |
| `index_id`           | `str`                             | Yes      | The unique identifier of the index to which the videos are being uploaded.                                                          |
| `user_metadata`      | `Optional[Dict[str, Any]] = None` | No       | Metadata that helps you categorize your videos.                                                                                     |
| `incremental_import` | `Optional[bool] = None`           | No       | Specifies whether or not incremental sync is enabled. If set to `false`, the platform will synchronize all the files in the bucket. |
| `**kwargs`           | `dict`                            | No       | Additional keyword arguments for the request.                                                                                       |

**Return value**: Returns a [`models.TransferImportResource`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/task.py)  object containing two lists:

* Videos that will be imported.
* Videos that will not be imported, typically due to unmet [prerequisites](/v1.3/docs/guides/search#prerequisites).

**API Reference**: [Import videos](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/create).

**Related guide**: [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations).

## Retrieve import status

**Description**: This method retrieves the current status for each video from a specified integration and index.

**Function signature and example**:


  ```python Function signature
  def import_status(
      self, integration_id: str, index_id: str, **kwargs
  ) -> models.TransferImportStatusResponse
  ```

  ```python Python example
  status = client.task.transfers.import_status(integration_id, index_id)
  for ready in status.ready:
      print(f"ready: {ready.video_id} {ready.filename} {ready.created_at}")
  for failed in status.failed:
      print(f"failed: {failed.filename} {failed.error_message}")
  ```


**Parameters**:

| Name             | Type   | Required | Description                                                                                                 |
| :--------------- | :----- | :------- | :---------------------------------------------------------------------------------------------------------- |
| `integration_id` | `str`  | Yes      | The unique identifier of the integration for which you want to retrieve the status of your imported videos. |
| `index_id`       | `str`  | Yes      | The unique identifier of the index for which you want to retrieve the status of your imported videos.       |
| `**kwargs`       | `dict` | No       | Additional keyword arguments for the request.                                                               |

**Return value**:  Returns a [`models.TransferImportStatusResponse`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/task.py)  object containing lists of videos grouped by status. See the [Task object](/v1.3/api-reference/tasks/the-task-object)  page for details on each status.

**API Reference**: [Retrieve import status](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/get-logs).

**Related guide**: [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations).

## Retrieve import logs

**Description**: This method returns a chronological list of import operations for the specified integration.

**Function signature and example**:


  ```python Function signature
  def import_logs(
      self, integration_id: str, **kwargs
  ) -> RootModelList[models.TransferImportLog]
  ```

  ```python Python example
  logs = client.task.transfers.import_logs("")
  for log in logs:
      print(
          f"index_id={log.index_id} index_name={log.index_name} created_at={log.created_at} ended_at={log.ended_at} video_status={log.video_status}"
      )
      if log.failed_files:
          for failed_file in log.failed_files:
              print(
                  f"failed_file: {failed_file.filename} {failed_file.error_message}"
              )
  ```


**Parameters**:

| Name             | Type   | Required | Description                                                                              |
| :--------------- | :----- | :------- | :--------------------------------------------------------------------------------------- |
| `integration_id` | `str`  | Yes      | The unique identifier of the integration for which you want to retrieve the import logs. |
| `kwargs`         | `dict` | No       | Additional keyword arguments for the request.                                            |

**Return value**: Returns a [`models.TransferImportLog`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/task.py)  object containing a chronological list of import operations for the specified integration. The list is sorted by creation date, with the oldest imports first. Each item in the list contains:

* The number of videos in each status
* Detailed error information for failed uploads, including filenames and error messages.

**API Reference**: [Retrieve import logs](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/get-logs).

**Related guide**: [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations).

# Error codes

This section lists the most common error messages you may encounter while uploading videos.

* `video_resolution_too_low`
  * The resolution of the video is too low. Please upload a video with resolution between 360x360 and 3840x2160. Current resolution is `{current_resolution}`.
* `video_resolution_too_high`
  * The resolution of the video is too high. Please upload a video with resolution between 360x360 and 3840x2160. Current resolution is `{current_resolution}`.
* `video_resolution_invalid_aspect_ratio`
  * The aspect ratio of the video is invalid. Please upload a video with aspect ratio between 1:1 and 2.4:1. Current resolution is `{current_resolution}`.
* `video_duration_too_short`
  * Video is too short. Please use video with duration between 10 seconds and 2 hours(7200 seconds). Current duration is `{current_duration}` seconds.
* `video_duration_too_long`
  * Video is too long. Please use video with duration between 10 seconds and 2 hours(7200 seconds). Current duration is `{current_duration}` seconds.
* `video_file_broken`
  * Cannot read video file. Please check the video file is valid and try again.
* `task_cannot_be_deleted`
  * (Returns raw error message)
* `usage_limit_exceeded`
  * Not enough free credit. Please register a payment method or contact [sales@twelvelabs.io](mailto:sales@twelvelabs.io).
* `video_filesize_too_large`
  * The video is too large. Please use a video with a size less than `{maximum_size}`. The current size is `{current_file_size}`.


# Manage videos

The [`resources.Video`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/video.py) class provides methods to manage the videos you've uploaded to the platform.

# Methods

## Retrieve video information

**Description**: This method retrieves information about the specified video.

**Function signature and example**:


  ```python Function signature
  def retrieve(
      self,
      index_id: str,
      id: str,
      *,
      embedding_option: Optional[List[str]] = None,
      **kwargs,
  ) -> models.Video
  ```

  ```python Python example
  def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
      for segment in segments:
          print(
              f"  embedding_scope={segment.embedding_scope} embedding_option={segment.embedding_option} start_offset_sec={segment.start_offset_sec} end_offset_sec={segment.end_offset_sec}"
          )
          print(f"  embeddings: {segment.embeddings_float[:max_elements]}")

  video = client.index.video.retrieve(index_id="", id="", embedding_option=["visual-text", "audio"])

  print(f"ID: {video.id}")
  print(f"Created at: {video.created_at}")
  print(f"Updated at: {video.updated_at}")
  print(f"Indexed at: {video.indexed_at}")
  print("System metadata:")
  print(f"  Filename: {video.system_metadata.filename}")
  print(f"  Duration: {video.system_metadata.duration}")
  print(f"  FPS: {video.system_metadata.fps}")
  print(f"  Width: {video.system_metadata.width}")
  print(f"  Height: {video.system_metadata.height}")
  print(f"  Size: {video.system_metadata.size}")
  if video.user_metadata:
      print("User metadata:")
      for key, value in video.user_metadata.items():
          print(f"{key}: {value}")
  if video.hls:
      print("HLS:")
      print(f"  Video URL: {video.hls.video_url}")
      print("  Thumbnail URLs:")
      for url in video.hls.thumbnail_urls or []:
          print(f"    {url}")
      print(f"  Status: {video.hls.status}")
      print(f"  Updated At: {video.hls.updated_at}")
  if video.source:
      print("Source:")
      print(f"  Type: {video.source.type}")
      print(f"  Name: {video.source.name}")
      print(f"  URL: {video.source.url}")
  if video.embedding:
      print(f"Engine_name={video.embedding.engine_name}")
      print("Embeddings:")
      print_segments(video.embedding.video_embedding.segments)

  ```


**Parameters**:

| Name               | Type        | Required | Description                                                                                                                                                                                                                |
| :----------------- | :---------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `**kwargs`         | `dict`      | No       | Additional keyword arguments for the request.                                                                                                                                                                              |
| `index_id`         | `str`       | Yes      | The unique identifier of the index to which the video has been uploaded.                                                                                                                                                   |
| `id`               | `str`       | Yes      | The unique identifier of the video to retrieve.                                                                                                                                                                            |
| `embedding_option` | `List[str]` | No       | Specifies which types of embeddings to retrieve. You can include one or more of the following values:
- `visual-text`: Returns visual embeddings optimized for text search.
- `audio`: Returns audio embeddings. |

**Return value**: Returns a [`models.Video`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/video.py) object representing the retrieved video.

**API Reference**: For a description of each field in the request and response, see the [Retrieve video information](/v1.3/api-reference/videos/retrieve) page.

## List videos with direct pagination

**Description**: This method returns a paginated list of the videos in the specified index based on the provided parameters. Choose this method mainly when the total number of items is manageable, or you must fetch a single page of results. By default, the platform returns your videos sorted by their upload date, with the newest at the top of the list.

**Function signature and example**:


  ```python Function signature
  def list(
      self,
      index_id: str,
      *,
      id: Optional[str] = None,
      filename: Optional[str] = None,
      size: Optional[Union[int, Dict[str, int]]] = None,
      width: Optional[Union[int, Dict[str, int]]] = None,
      height: Optional[Union[int, Dict[str, int]]] = None,
      duration: Optional[Union[int, Dict[str, int]]] = None,
      fps: Optional[Union[int, Dict[str, int]]] = None,
      user_metadata: Optional[Dict[str, Any]] = None,
      created_at: Optional[Union[str, Dict[str, str]]] = None,
      updated_at: Optional[Union[str, Dict[str, str]]] = None,
      page: Optional[int] = None,
      page_limit: Optional[int] = None,
      sort_by: Optional[str] = None,
      sort_option: Optional[str] = None,
      **kwargs
  ) -> RootModelList[models.Video]
  ```

  ```python Python example
  videos = client.index.video.list(
      index_id="",
      id="",
      filename="",
      size=1024,
      width=920,
      height=1080,
      duration=100,
      fps=30,
      user_metadata={"category": "nature"},
      user_created_at="2024-09-17T07:53:46.365Z",
      updated_at="2024-09-17T07:53:46.365Z",
      page=1,
      page_limit=5,
      sort_by="created_at",
      sort_option="desc"
  )
  for video in videos:
      print(f"ID: {video.id}")
      print(f"  Created at: {video.created_at}")
      print(f"  Updated at: {video.updated_at}")
      print("   System metadata:")
      print(f"    Filename: {video.system_metadata.filename}")
      print(f"    Duration: {video.system_metadata.duration}")
      print(f"    FPS: {video.system_metadata.fps}")
      print(f"    Width: {video.system_metadata.width}")
      print(f"    Height: {video.system_metadata.height}")
      print(f"  Size: {video.system_metadata.size}")
      if video.user_metadata:
          print("User metadata:")
          for key, value in video.user_metadata.items():
              print(f"{key}: {value}")
      if video.hls:
          print("  HLS:")
          print(f"    Video URL: {video.hls.video_url}")
          print("    Thumbnail URLs:")
          for url in video.hls.thumbnail_urls or []:
              print(f"      {url}")
          print(f"    Status: {video.hls.status}")
          print(f"    Updated At: {video.hls.updated_at}")
      if video.source:
          print("  Source:")
          print(f"    Type: {video.source.type}")
          print(f"    Name: {video.source.name}")
          print(f"    URL: {video.source.url}")
  ```


**Parameters**:

| Name            | Type                                   | Required | Description                                                                                                 |
| --------------- | -------------------------------------- | -------- | ----------------------------------------------------------------------------------------------------------- |
| `index_id`      | `str`                                  | Yes      | The unique identifier of the index for which the API will retrieve the videos.                              |
| `id`            | `Optional[str]`                        | No       | Filter by the unique identifier of a video.                                                                 |
| `filename`      | `Optional[str]`                        | No       | Filter by the name of the video file.                                                                       |
| `size`          | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the size of the video file. This parameter can be an integer or a dictionary for range queries.   |
| `width`         | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the width of the video. This parameter can be an integer or a dictionary for range queries.       |
| `height`        | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the height of the video. This parameter can be an integer or a dictionary for range queries.      |
| `duration`      | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the duration of the video. This parameter can be an integer or a dictionary for range queries.    |
| `fps`           | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the number frames per second. This parameter can be an integer or a dictionary for range queries. |
| `user_metadata` | `Optional[Dict[str, Any]]`             | No       | Filter by user metadata.                                                                                    |
| `created_at`    | `Optional[Union[str, Dict[str, str]]]` | No       | Filter by the creation date. This parameter can be a string or a dictionary for range queries.              |
| `updated_at`    | `Optional[Union[str, Dict[str, str]]]` | No       | Filter by the last update date. This parameter can be a string or a dictionary for range queries.           |
| `page`          | `Optional[int]`                        | No       | Page number for pagination.                                                                                 |
| `page_limit`    | `Optional[int]`                        | No       | Number of items per page.                                                                                   |
| `sort_by`       | `Optional[str]`                        | No       | Field to sort by.                                                                                           |
| `sort_option`   | `Optional[str]`                        | No       | Sort order. You can specify one of the following values: "asc" or "desc".                                   |
| `**kwargs`      | `dict`                                 | No       | Additional keyword arguments for the request.                                                               |

**Return value**: Returns a [`RootModelList`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/_base.py) containing [`models.Video`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/video.py) objects, representing the list of videos that match the specified criteria.

**API Reference**: For a description of each field in the request and response, see the [List videos](/v1.3/api-reference/videos/list) page.

## List videos with iterative pagination

**Description**: This method iterates through a paginated list of the videos in the specified index based on the provided parameters. Choose this method mainly when your application must retrieve a large number of items. By default, the API returns your videos sorted by creation date, with the newest at the top of the list.

**Function signature and example**:


  ```python Function signature
  def list_pagination(
      self,
      index_id: str,
      *,
      id: Optional[str] = None,
      filename: Optional[str] = None,
      size: Optional[Union[int, Dict[str, int]]] = None,
      width: Optional[Union[int, Dict[str, int]]] = None,
      height: Optional[Union[int, Dict[str, int]]] = None,
      duration: Optional[Union[int, Dict[str, int]]] = None,
      fps: Optional[Union[int, Dict[str, int]]] = None,
      user_metadata: Optional[Dict[str, Any]] = None,
      created_at: Optional[Union[str, Dict[str, str]]] = None,
      updated_at: Optional[Union[str, Dict[str, str]]] = None,
      page: Optional[int] = None,
      page_limit: Optional[int] = None,
      sort_by: Optional[str] = None,
      sort_option: Optional[str] = None,
      **kwargs
  ) -> models.VideoListWithPagination
  ```

  ```python Python example
  def print_page(page):
      for video in page:
          print(f"ID: {video.id}")
          print(f"  Created at: {video.created_at}")
          print(f"  Updated at: {video.updated_at}")
          print("  System metadata:")
          print(f"    Filename: {video.system_metadata.filename}")
          print(f"    Duration: {video.system_metadata.duration}")
          print(f"    FPS: {video.system_metadata.fps}")
          print(f"    Width: {video.system_metadata.width}")
          print(f"    Height: {video.system_metadata.height}")
          print(f"    Size: {video.system_metadata.size}")
          if video.user_metadata:
              print("User metadata:")
              for key, value in video.user_metadata.items():
                  print(f"{key}: {value}")
          if video.hls:
              print("  HLS:")
              print(f"    Video URL: {video.hls.video_url}")
              print("    Thumbnail URLs:")
              for url in video.hls.thumbnail_urls or []:
                  print(f"      {url}")
              print(f"    Status: {video.hls.status}")
              print(f"    Updated At: {video.hls.updated_at}")
          if video.source:
              print("  Source:")
              print(f"    Type: {video.source.type}")
              print(f"    Name: {video.source.name}")
              print(f"    URL: {video.source.url}")

  # Fetch the initial page of results
  video_paginator = client.index.video.list_pagination(
      index_id="",
      id="

**Parameters**:

| Name            | Type                                   | Required | Description                                                                                                    |
| --------------- | -------------------------------------- | -------- | -------------------------------------------------------------------------------------------------------------- |
| `index_id`      | `str`                                  | Yes      | The unique identifier of the index for which the API will retrieve the videos.                                 |
| `id`            | `Optional[str]`                        | No       | Filter by the unique identifier of a video.                                                                    |
| `filename`      | `Optional[str]`                        | No       | Filter by the name of the video file.                                                                          |
| `size`          | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the size of the video file. This parameter can be an integer or a dictionary for range queries.      |
| `width`         | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the width of the video. This parameter can be an integer or a dictionary for range queries.          |
| `height`        | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the height of the video. This parameter can be an integer or a dictionary for range queries.         |
| `duration`      | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the duration of the video. This parameter can be an integer or a dictionary for range queries.       |
| `fps`           | `Optional[Union[int, Dict[str, int]]]` | No       | Filter by the number of frames per second. This parameter can be an integer or a dictionary for range queries. |
| `user_metadata` | `Optional[Dict[str, Any]]`             | No       | Filter by user metadata.                                                                                       |
| `created_at`    | `Optional[Union[str, Dict[str, str]]]` | No       | Filter by the creation date. This parameter can be a string or a dictionary for range queries.                 |
| `updated_at`    | `Optional[Union[str, Dict[str, str]]]` | No       | Filter by the last update date. This parameter can be a string or a dictionary for range queries.              |
| `page`          | `Optional[int]`                        | No       | Page number for pagination.                                                                                    |
| `page_limit`    | `Optional[int]`                        | No       | Number of items per page.                                                                                      |
| `sort_by`       | `Optional[str]`                        | No       | Field to sort by.                                                                                              |
| `sort_option`   | `Optional[str]`                        | No       | Sort order. You can specify one of the following values: "asc" or "desc".                                      |
| `**kwargs`      | `dict`                                 | No       | Additional keyword arguments for the request.                                                                  |

**Return value**: Returns a [`models.VideoListWithPagination`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/video.py) object, containing the list of videos that match the specified criteria and pagination information.


  To retrieve subsequent pages of results, use the iterator protocol:

  1. Call the `next` function, passing the `VideoListWithPagination` object as a parameter.
  2. Repeat this call until a `StopIteration` exception occurs, indicating no more pages exist.


**API Reference**: For a description of each field in the request and response, see the [List videos](/v1.3/api-reference/videos/list) page.

## Update video information

**Description**: This method updates the title and metadata of a video.

**Function signature and example**:


  ```python Function signature
  def update(
      self,
      index_id: str,
      id: str,
      *,
      user_metadata: Optional[Dict[str, Any]] = None,
      **kwargs
  ) -> None
  ```

  ```python Python example
  client.index.video.update(
      index.id, video.id, user_metadata={"from_sdk": True}
  )
  ```


**Parameters**:

| Name            | Type                       | Required | Description                                                                                                                                                   |
| :-------------- | :------------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `index_id`      | `str`                      | Yes      | The unique identifier of the index to which the video has been uploaded.                                                                                      |
| `id`            | `str`                      | Yes      | The unique identifier of the video.                                                                                                                           |
| `user_metadata` | `Optional[Dict[str, Any]]` | No       | A dictionary containing the metadata you want to update or add. Note that only the provided properties are modified; the omitted properties remain unchanged. |
| `**kwargs`      | `dict`                     | No       | Additional keyword arguments for the request.                                                                                                                 |

**Return value**: `None`. The method doesn't return any value.

**API Reference**: For a description of each field in the request, see the [Update video information](/v1.3/api-reference/videos/update) page.

## Delete video information

**Description**: This method deletes all the information about the specified video. This action cannot be undone.

**Function signature and example**:


  ```python Function signature
  def delete(self, index_id: str, id: str, **kwargs) -> None
  ```

  ```python Python example
  client.task.delete(index_id="", id="")
  ```


**Parameters**:

| Name       | Type     | Required | Description                                                              |
| :--------- | :------- | :------- | :----------------------------------------------------------------------- |
| `index_id` | `string` | Yes      | The unique identifier of the index to which the video has been uploaded. |
| `id`       | `string` | Yes      | The unique identifier of the video to delete.                            |
| `**kwargs` | `dict`   | No       | Additional keyword arguments for the request.                            |

**Return value**: `None`. The method doesn't return any value.

**API Reference**: [Delete video information](/v1.3/api-reference/videos/delete).


# Search

The [`resources.Search`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/search.py) class provides methods to perform searches.

[**Related quickstart notebook**](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Search.ipynb)

# Methods

## Make a search request

**Description**: This method performs a search across a specific index based on the provided parameters and returns the first page of results.

If you wish to use images as queries, ensure that your images meet the following requirements:

* **Format**: JPEG and PNG.
* **Dimension**: Must be at least 64 x 64 pixels.
* **Size**: Must not exceed 5MB.

**Function signature and example**:


  ```python Function signature
  def query(
      self,
      index_id: str,
      options: List[Literal["visual", "audio"]],
      *,
      query_text: str = None,
      query_media_type: Literal["image"] = None,
      query_media_file: Union[str, BinaryIO, None] = None,
      query_media_url: str = None,
      group_by: Optional[Literal["video", "clip"]] = None,
      threshold: Optional[Literal["high", "medium", "low", "none"]] = None,
      operator: Optional[Literal["or", "and"]] = None,
      filter: Optional[Dict[str, Any]] = None,
      page_limit: Optional[int] = None,
      sort_option: Optional[Literal["score", "clip_count"]] = None,
      adjust_confidence_level: Optional[float] = None,
      **kwargs
  ) -> models.SearchResult
  ```

  ```python Python example
  from twelvelabs.models.search import SearchData, GroupByVideoSearchData

  def print_search_data(data: SearchData):
      print(f"  Score: {data.score}")
      print(f"  Start: {data.start}")
      print(f"  End: {data.end}")
      print(f"  Video ID: {data.video_id}")
      print(f"  Confidence: {data.confidence}")
      print(f"  Thumbnail URL: {data.thumbnail_url}")

  result = client.search.query(
      index_id="",
      options=["visual", "audio"],
      query_text="",
      group_by="clip",
      # threshold="medium",
      operator="or",
      # filter={"category": "nature"},
      page_limit=5,
      sort_option="score",
      # adjust_confidence_level=0.5
  )

  # Print the search pool information
  print("Search pool:")
  print(f"  Total count: {result.pool.total_count}")
  print(f"  Total duration: {result.pool.total_duration}")
  print(f"  Index ID: {result.pool.index_id}")

  # Print the search results
  print("Search Results:")
  for item in result.data:
      if isinstance(item, GroupByVideoSearchData):
          print(f"Video ID: {item.id}")
          if item.clips:
              for clip in item.clips:
                  print_search_data(clip)
      else:
          print_search_data(item)

  # Print the page information
  print("Page information:")
  print(f"  Limit per page: {result.page_info.limit_per_page}")
  print(f"  Total results: {result.page_info.total_results}")
  print(f"  Page expires at: {result.page_info.page_expires_at}")
  print(f"  Next page token: {result.page_info.next_page_token}")
  print(f"  Previous page token: {result.page_info.prev_page_token}")
   
  ```


**Parameters**:

| Name                      | Type                                                 | Required | Description                                                                                                                                                    |
| ------------------------- | ---------------------------------------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `index_id`                | `str`                                                | Yes      | The unique identifier of the index to search.                                                                                                                  |
| `options`                 | `List[Literal["visual", "audio"]]`                   | Yes      | Specifies the [sources of information](https://docs.twelvelabs.io/v1.3/docs/concepts/search-options) the platform uses when performing a search.               |
| `query_text`              | `str`                                                | No       | The text query to search for. This parameter is required for text queries.                                                                                     |
| `query_media_type`        | `Literal["image"]`                                   | No       | The type of media you wish to use. This parameter is required for media queries. For example, to perform an image-based search, set this parameter to `image`. |
| `query_media_file`        | `Union[str, BinaryIO, None]`                         | No       | The path to a media file or a file-like object to use as a query. This parameter is required for media queries if `query_media_url` is not provided.           |
| `query_media_url`         | `str`                                                | No       | The publicly accessible URL of a media file to use as a query. This parameter is required for media queries if `query_media_file` is not provided.             |
| `group_by`                | `Optional[Literal["video", "clip"]]`                 | No       | Use this parameter to group or ungroup items in a response.                                                                                                    |
| `threshold`               | `Optional[Literal["high", "medium", "low", "none"]]` | No       | Filter on the level of confidence that the results match your query.                                                                                           |
| `operator`                | `Optional[Literal["or", "and"]]`                     | No       | Logical operator for combining search options.                                                                                                                 |
| `filter`                  | `Optional[Dict[str, Any]]`                           | No       | Additional filters for the search.                                                                                                                             |
| `page_limit`              | `Optional[int]`                                      | No       | The maximum number of results per page.                                                                                                                        |
| `sort_option`             | `Optional[Literal["score", "clip_count"]]`           | No       | The sort order for the response.                                                                                                                               |
| `adjust_confidence_level` | `Optional[float]`                                    | No       | The strictness of the thresholds for assigning the high, medium, or low confidence levels to search results.                                                   |
| `**kwargs`                | `dict`                                               | No       | Additional keyword arguments for the request.                                                                                                                  |

**Return value**: Returns a [`models.SearchResult`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/search.py) object containing the search results.

**API Reference**: For a detailed description of each field in the request and response, see the [Any-to-video search](/v1.3/api-reference/any-to-video-search/make-search-request) page.

**Related guides**:

* [Search](/v1.3/docs/guides/search)
* [Search > Pagination](/v1.3/docs/guides/search/pagination)
* [Search > Sorting](/v1.3/docs/guides/search/sorting)
* [Search > Filtering](/v1.3/docs/guides/search/filtering)
* [Grouping](/v1.3/docs/guides/search/grouping)

## Retrieve a specific page of search results

**Description**: This method retrieves a specific page of search results. This method provides direct pagination. Choose it mainly when the total number of items is manageable, or you must fetch a single page of results.

**Function signature and example**:


  ```python Function signature
  def by_page_token(self, page_token: str, **kwargs) -> models.SearchResult
  ```

  ```python Python example
  from twelvelabs.models.search import SearchData, GroupByVideoSearchData

  def print_search_data(data: SearchData):
      print(f"  Score: {data.score}")
      print(f"  Start: {data.start}")
      print(f"  End: {data.end}")
      print(f"  Video ID: {data.video_id}")
      print(f"  Confidence: {data.confidence}")
      print(f"  Thumbnail URL: {data.thumbnail_url}")

  result = client.search.by_page_token("")

  # Print the search pool information
  print("Search pool:")
  print(f"  Total count: {result.pool.total_count}")
  print(f"  Total duration: {result.pool.total_duration}")
  print(f"  Index ID: {result.pool.index_id}")

  # Print the search results
  print("Search Results:")
  for item in result.data:
      if isinstance(item, GroupByVideoSearchData):
          print(f"Video ID: {item.id}")
          if item.clips:
              for clip in item.clips:
                  print_search_data(clip)
      else:
          print_search_data(item)

  # Print the page information
  print("Page information:")
  print(f"  Limit per page: {result.page_info.limit_per_page}")
  print(f"  Total results: {result.page_info.total_results}")
  print(f"  Page expires at: {result.page_info.page_expires_at}")
  print(f"  Next page token: {result.page_info.next_page_token}")
  print(f"  Previous page token: {result.page_info.prev_page_token}")
  ```


**Parameters**:

| Name        | Type   | Required | Description                                    |
| :---------- | :----- | :------- | :--------------------------------------------- |
| `pageToken` | `str`  | Yes      | A token that identifies the page to retrieve.  |
| `**kwargs`  | `dict` | No       | Additional keyword arguments for the request.. |

**Return value**: Returns a [`models.SearchResult`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/search.py) object containing the search results.

**API Reference**: For a description of each field in the request and response, see the [Retrieve a specific page of search results](/v1.3/api-reference/any-to-video-search/retrieve-page) page.

**Related guides**:

* [Search](/v1.3/docs/guides/search)
* [Pagination](/v1.3/docs/guides/search/pagination)
* [Sorting](/v1.3/docs/guides/search/sorting)
* [Filtering](/v1.3/docs/guides/search/filtering)
* [Grouping](/v1.3/docs/guides/search/grouping)

# Iterative pagination

If your application must retrieve a large number of items, use iterative pagination. To retrieve the first page of results, invoke the `query` method of the `search` object. To retrieve subsequent pages of results, use the iterator protocol.

```python Python
from twelvelabs.models.search import SearchData, GroupByVideoSearchData

def print_search_data(data: SearchData):
    print(f"  Score: {data.score}")
    print(f"  Start: {data.start}")
    print(f"  End: {data.end}")
    print(f"  Video ID: {data.video_id}")
    print(f"  Confidence: {data.confidence}")
    print(f"  Thumbnail URL: {data.thumbnail_url}")

def print_page(result):
    # Print the search results
    print("Search Results:")
    for item in getattr(result, 'data', result):
        if isinstance(item, GroupByVideoSearchData):
            print(f"Video ID: {item.id}")
            if item.clips:
                for clip in item.clips:
                    print_search_data(clip)
        else:
            print_search_data(item)


# Initial query
search_results = client.search.query(
    index_id="",
    options=["visual", "audio"],
    query_text="man laughing",
    group_by="clip",
    # threshold="medium",
    operator="or",
    # filter={"category": "nature"},
    page_limit=2,
    sort_option="score",
    # adjust_confidence_level=0.5
)

# Print the search pool information
print("Search pool:")
print(f"  Total count: {search_results.pool.total_count}")
print(f"  Total duration: {search_results.pool.total_duration}")
print(f"  Index ID: {search_results.pool.index_id}")

# Print the first page
print_page(search_results)

# Print subsequent pages using the iterator protocol
page_number = 2
while True:
    try:
        print(f"Page {page_number}")
        print_page(next(search_results))
        page_number += 1
    except StopIteration:
        break

print("No more results.")
```

# Error codes

This section lists the most common error messages you may encounter while performing search requests.

* `search_option_not_supported`
  * Search option `{search_option}` is not supported for index `{index_id}`. Please use one of the following search options: `{supported_search_option}`.
* `search_option_combination_not_supported`
  * Search option `{search_option}` is not supported with `{other_combination}`.
* `search_filter_invalid`
  * Filter used in search is invalid. Please use the valid filter syntax by following filtering documentation.
* `search_page_token_expired`
  * The token that identifies the page to be retrieved is expired or invalid. You must make a new search request. Token: `{next_page_token}`.
* `index_not_supported_for_search`:
  * You can only perform search requests on indexes with an engine from the Marengo family enabled.

For a list of errors specific to this endpoint and general errors that apply to all endpoints, see the [Error codes]() page.


# Create text, image, and audio embeddings

The [`resources.Embed`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/embed.py) class provides methods to create text, image, and audio embeddings.

# Create text, image, and audio embeddings

**Description**: This method creates a new embedding.

Note that you must specify at least the following parameters:

* `model_name`: The name of the video understanding model to use.

* One or more of the following input types:
  * `text`: For text embeddings
  * `audio_url` or `audio_file`: For audio embeddings. If you specify both, the `audio_url` parameter takes precedence.
  * `image_url` or `image_file`: For image embeddings. If you specify both, the `image_url` parameter takes precedence.

You must provide at least one input type, but you can include multiple types in a single function call.

**Function signature and example**:


  ```python Function signature
  def create(
      self,
      model_name: Literal["Marengo-retrieval-2.7"],
      *,
      # text params
      text: str = None,
      text_truncate: Literal["none", "start", "end"] = None,
      # audio params
      audio_url: str = None,
      audio_file: Union[str, BinaryIO, None] = None,
      # image params
      image_url: str = None,
      image_file: Union[str, BinaryIO, None] = None,
      **kwargs,
  ) -> models.CreateEmbeddingsResult
  ```

  ```python Python example
  def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
      for segment in segments:
          print(
              f"  embedding_scope={segment.embedding_scope} start_offset_sec={segment.start_offset_sec} end_offset_sec={segment.end_offset_sec}"
          )
          print(f"  embeddings: {segment.embeddings_float[:max_elements]}")
              
            
  res = client.embed.create(
    model_name="Marengo-retrieval-2.7",
    text_truncate="start",
    text="",
    audio_url="",
    image_url=""
  )

  print(f" Model: {res.model_name}")
  if res.text_embedding is not None and res.text_embedding.segments is not None:
    print("Created text embeddings:")
    print_segments(res.text_embedding.segments)
  if res.image_embedding is not None and res.image_embedding.segments is not None:
    print("Created image embeddings:")
    print_segments(res.image_embedding.segments)
  if res.audio_embedding is not None and res.audio_embedding.segments is not None:
    print("Created audio embeddings:")
    print_segments(res.audio_embedding.segments)
  ```


**Parameters**:

| Name            | Type                              | Required | Description                                                                         |
| --------------- | --------------------------------- | -------- | ----------------------------------------------------------------------------------- |
| `model_name`    | `Literal["Marengo-retrieval-2.7"` | Yes      | The name of the video understanding model to use. Example: "Marengo-retrieval-2.7". |
| `text`          | `str`                             | Yes      | The text for which you want to create an embedding.                                 |
| `text_truncate` | `Literal["none", "start", "end"]` | Yes      | Specifies how to truncate the text if it exceeds the maximum length of 77 tokens.   |
| `audio_url`     | `str`                             |          | A publicly accessible URL of your audio file.                                       |
| `audio_file`    | `Union[str, BinaryIO, None]`      |          | A local audio file.                                                                 |
| `image_url`     | `str`                             |          | A publicly accessible URL of your image file.                                       |
| `image_file`    | `Union[str, BinaryIO, None]`      |          | A local image file.                                                                 |
| `**kwargs`      | `dict`                            | No       | Additional keyword arguments for the request.                                       |

**Return value**: Returns a [`models.CreateEmbeddingsResult`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/embed.py) object containing the embedding results.

**API Reference**: For a description of each field in the request and response, see the [Create text, audio, and image embeddings](/v1.3/api-reference/text-image-audio-embeddings/create-text-image-audio-embeddings) page.

**Related guides**:

* [Create text embeddings](/v1.3/docs/guides/create-embeddings/text)
* [Create audio embeddings](/v1.3/docs/guides/create-embeddings/audio)
* [Create image embeddings](/v1.3/docs/guides/create-embeddings/image)

# Error codes

This section lists the most common error messages you may encounter while creating text, image, and audio embeddings.

* `parameter_invalid`
  * The `text` parameter is invalid. The text token length should be less than or equal to 77.
  * The `text_truncate` parameter is invalid. You should use one of the following values: `none`, `start`, `end`.


# Create video embeddings

The [`resources.EmbedTask`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/resources/embed.py) class provides methods to create embeddings for your videos.

To create video embeddings:

1. Create a video embedding task that uploads and processes a video.
2. Monitor the status of your task.
3. Retrieve the embeddings once the task is completed.

[**Related quickstart notebook**](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Embeddings.ipynb)

# Methods

## Create a video embedding task

**Description**: This method creates a new video embedding task.


  The videos you wish to upload must meet the following requirements:

  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the [FFmpeg Formats Documentation](https://ffmpeg.org/ffmpeg-formats.html) page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).
  * **Duration**: Must be between 4 seconds and 2 hours (7,200s).
  * **File size**: Must not exceed 2 GB.
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).


**Function signature and example**:


  ```python Function signature
  def create(
      self,
      model_name: str,
      *,
      video_file: Union[str, BinaryIO, None] = None,
      video_url: Optional[str] = None,
      video_start_offset_sec: Optional[float] = None,
      video_end_offset_sec: Optional[float] = None,
      video_clip_length: Optional[int] = None,
      video_embedding_scopes: Optional[List[Literal["clip", "video"]]] = None,
      **kwargs
  ) -> models.EmbeddingsTask
  ```

  ```python Python example
  task = client.embed.task.create(
      model_name="Marengo-retrieval-2.7",
      # video_file="",
      video_url="",
      video_start_offset_sec=0,
      video_end_offset_sec=10,
      video_clip_length=5,
      video_embedding_scopes=["clip", "video"]
  )

  print(f"Task ID: {task.id}")
  print(f"Model Name: {task.model_name}")
  print(f"Status: {task.status}")
  ```


**Parameters**:

| Name                     | Type                                       | Required | Description                                                                                         |
| ------------------------ | ------------------------------------------ | -------- | --------------------------------------------------------------------------------------------------- |
| `model_name`             | `Literal["Marengo-retrieval-2.7"]`         | Yes      | The name of the video understanding model to use. Example: "Marengo-retrieval-2.7".                 |
| `video_file`             | `Union[str, BinaryIO, None]`               | No       | Path to the video file or a file-like object.                                                       |
| `video_url`              | `Optional[str]`                            | No       | The publicly accessible URL of the video.                                                           |
| `video_start_offset_sec` | `Optional[float]`                          | No       | The start offset in seconds from the beginning of the video where processing should begin.          |
| `video_end_offset_sec`   | `Optional[float]`                          | No       | The end offset in seconds from the beginning of the video where processing should stop.             |
| `video_clip_length`      | `Optional[int]`                            | No       | The desired duration in seconds for each clip for which the platform generates an embedding.        |
| `video_embedding_scopes` | `Optional[List[Literal["clip", "video"]]]` | No       | Specifies the embedding scope. Valid values are:

- `["clip"]`
- `["clip", "video"]` |
| `**kwargs`               | `dict`                                     | No       | Additional keyword arguments for the request.                                                       |

**Return value**: Returns a [`models.EmbeddingsTask`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/embed.py) object representing the new video embedding task.

**API Reference**: For a description of each field in the request and response, see the [Create a video embedding task](/v1.3/api-reference/video-embeddings/create-video-embedding-task) page.

**Related guide**: [Create video embeddings](/v1.3/docs/guides/create-embeddings/video).

## Retrieve the status of a video embedding task

**Description**: This method retrieves the status of a video embedding task.

**Function signature and example**:


  ```python Function signature
  def status(self, task_id: str, **kwargs) -> models.EmbeddingsTaskStatus
  ```

  ```python Python example
  task = client.embed.task.status(task_id="")

  print(f"Task ID: {task.id}")
  print(f"Model Name: {task.model_name}")
  print(f"Status: {task.status}")
  ```


**Parameters**:

| Name       | Type     | Required | Description                                        |
| :--------- | :------- | :------- | :------------------------------------------------- |
| `task_id`  | `string` | Yes      | The unique identifier of the video embedding task. |
| `**kwargs` | `dict`   | No       | Additional keyword arguments for the request.      |

**Return value**: Returns a [`models.EmbeddingsTaskStatus`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/embed.py) object containing the current status of the embedding task.

**API Reference**: For a description of each field in the response, see the [Retrieve the status of a video embedding task](/v1.3/api-reference/tasks/retrieve) page.

**Related guide**: [Create video embeddings](/v1.3/docs/guides/create-embeddings/video).

## Wait for a video embedding task to complete

**Description**: This method waits until a video embedding task is completed by periodically checking its status. If you provide a callback function, it calls the function after each status update with the current task object, allowing you to monitor progress.


  ```python Function signature
  def wait_for_done(
      self,
      *,
      sleep_interval: float = 5.0,
      callback: Optional[Callable[[EmbeddingsTask], None]] = None,
      **kwargs,
  ) -> str
  ```

  ```python Python example
  def on_task_update(task: EmbeddingsTask):

      print(f"  Status={task.status}")

  status = task.wait_for_done(sleep_interval=5, callback=on_task_update)

  print(f"Embedding done: {status}")
  ```


**Parameters**

| Name             | Type                                         | Required | Description                                                                                                                                 |
| :--------------- | :------------------------------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------ |
| `sleep_interval` | `float`                                      | No       | Sets the time in seconds to wait between status checks. Must be greater than 0. Default is 5.0.                                             |
| `callback`       | `Optional[Callable[[EmbeddingsTask], None]]` | No       | Provides an optional function to call after each status check. The function receives the current task object. Use this to monitor progress. |
| `**kwargs`       | `dict`                                       | No       | Passes additional keyword arguments to the `update_status` method when checking the task status.                                            |

**Return value**: Returns a string representing the status of the task.

## Retrieve video embeddings

**Description**: This method retrieves embeddings for a specific video embedding task. Ensure the task status is `ready` before retrieving your embeddings.

**Function signature and example**:


  ```python Function signature
  def retrieve(self, **kwargs) -> EmbeddingsTask
  ```

  ```python Python example
  def print_segments(segments: List[SegmentEmbedding], max_elements: int = 5):
      for segment in segments:
          print(
              f"  embedding_scope={segment.embedding_scope} embedding_option={segment.embedding_option} start_offset_sec={segment.start_offset_sec} end_offset_sec={segment.end_offset_sec}"
          )
          print(f"  embeddings: {segment.embeddings_float[:max_elements]}")
     
  task = client.embed.task.retrieve(embedding_option=["visual-text"])
  if task.video_embedding is not None and task.video_embedding.segments is not None:
      print_segments(task.video_embedding.segments)
  ```


**Parameters**:

| Name       | Type   | Required | Description                                   |
| :--------- | :----- | :------- | :-------------------------------------------- |
| `**kwargs` | `dict` | No       | Additional keyword arguments for the request. |

**Return value**: Returns a [`models.EmbeddingsTask`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/embed.py) object containing the details of the embedding task, including the embeddings if available. The `video_embeddings` property of the returned object is a [`RootModelList`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/_base.py) of [`VideoEmbedding`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/embed.py) objects when the task is completed, or `None` if the embeddings are not yet available.

**API Reference**: For a description of each field in the request and response, see the [Retrieve video embeddings](/v1.3/api-reference/video-embeddings/retrieve-video-embeddings) page.

**Related guide**: [Create video embeddings](/v1.3/docs/guides/create-embeddings/video).

# Error codes

This section lists the most common error messages you may encounter while creating video embeddings.

* `parameter_invalid`
  * The `video_clip_length` parameter is invalid. `video_clip_length` should be within 2-10 seconds long
  * The `video_end_offset_sec` parameter is invalid. `video_end_offset_sec` should be greater than `video_start_offset_sec`


# Analyze videos

The TwelveLabs Python SDK provides methods to analyze videos to generate text from their content.

[**Related quickstart notebook**](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Analyze.ipynb)

# Titles, topics, and hashtags


  This method method has been flattened and is now called 

  `client.gist`

   instead of 

  `client.generate.gist`

  . The 

  `client.generate.gist`

   method will remain available until July 30, 2025; after this date, it will be deprecated. Update your code to use 

  `client.gist`

   to ensure uninterrupted service.


**Description**: This method analyzes a specific video and generates titles, topics, and hashtags based on its content. It uses predefined formats and doesn't require a custom prompt, and it's best for generating immediate and straightforward text representations without specific customization.

**Function signature and example**:


  ```python Function signature
  def gist(
      self,
      video_id: str,
      types: List[Union[str, Literal["topic", "hashtag", "title"]]],
      **kwargs
  ) -> models.GenerateGistResult
  ```

  ```python Python example

  result = client.gist(
      video_id="

**Parameters**:

| Name       | Type                                                     | Required | Description                                                                               |
| :--------- | :------------------------------------------------------- | :------- | :---------------------------------------------------------------------------------------- |
| `video_id` | `str`                                                    | Yes      | The unique identifier of the video for which you want to generate text.                   |
| `types`    | `List[Union[str, Literal["topic", "hashtag", "title"]]]` | Yes      | The types of text you want to generate. Available values: `topics`, `titles`, `hashtags`. |
| `**kwargs` | `dict`                                                   | No       | Additional keyword arguments for the request.                                             |

**Return value**: Returns a [`models.GenerateGistResult`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/generate.py) object containing the generated gist elements.

**API Reference**:  For a description of each field in the request and response, see the [Titles, topics, and hashtags](/v1.3/api-reference/analyze-videos/gist) page.

**Related guide**: [Titles, topics, and hashtags](/v1.3/docs/guides/analyze-videos/generate-titles-topics-and-hashtags).

# Summaries, chapters, and highlights


  This method method has been flattened and is now called 

  `client.summarize`

   instead of 

  `client.generate.summarize`

  . The 

  `client.generate.summarize`

   method will remain available until July 30, 2025; after this date, it will be deprecated. Update your code to use 

  `client.summarize`

   to ensure uninterrupted service.


**Description**: This method analyzes a video and generates summaries, chapters, or highlights based on its content. Optionally, you can provide a prompt to customize the output.

**Function signature and example**:


  ```python Function signature
  def summarize(
      self,
      video_id: str,
      type: Union[str, Literal["summary", "chapter", "highlight"]],
      *,
      prompt: Optional[str] = None,
      temperature: Optional[float] = None,
      **kwargs
  ) -> models.GenerateSummarizeResult
  ```

  ```python Python example
  result = client.summarize(
      video_id="",
      prompt="",
      temperature=0.7,
      type="summary"
  )

  print(f"Result ID: {result.id}")

  if result.summary is not None:
      print(f"Summary: {result.summary}")

  if result.chapters is not None:
      print("Chapters:")
      for chapter in result.chapters:
          print(f"  Chapter {chapter.chapter_number}:")
          print(f"    Start: {chapter.start}")
          print(f"    End: {chapter.end}")
          print(f"    Title: {chapter.chapter_title}")
          print(f"    Summary: {chapter.chapter_summary}")

  if result.highlights is not None:
      print("Highlights:")
      for highlight in result.highlights:
          print(f"  Start: {highlight.start}")
          print(f"  End: {highlight.end}")
          print(f"  Highlight: {highlight.highlight}")

  if result.usage is not None:
      print(f"Output tokens: {result.usage.output_tokens}")
  ```


**Parameters**:

| Name          | Type                                                                                     | Required | Description                                                                                     |
| :------------ | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------------------------------------------- |
| `video_id`    | `string`                                                                                 | Yes      | The unique identifier of the video for which you want to generate text.                         |
| `type`        | `Union[str, Literal["summary", "chapter", "highlight"]`                                  | Yes      | The type of text you want to generate. Available values: `summaries`, `chapters`, `highlights`. |
| `prompt`      | `Optional[str]`                                                                          | No       | The prompt to customize the output.                                                             |
| `temperature` | `Optional[float]`                                                                        | No       | The temperature to use in the generation.                                                       |
| `**kwargs`    | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional keyword arguments for the request.                                                   |

**Return value**: Returns a [`models.GenerateSummarizeResult`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/generate.py) object containing the generated content.

**API Reference**: For a description of each field in the request and response, see the [Summaries, chapters, and highlights](/v1.3/api-reference/analyze-videos/summarize) page.

**Related guide**: [Summaries, chapters, and highlights](/v1.3/docs/guides/analyze-videos/generate-summaries-chapters-and-highlights).

# Open-ended analysis

**Description**: This method analyzes a video and generates text based on its content.

**Function signature and example**:


  ```python Function signature
  def analyze(
      self,
      video_id: str,
      prompt: str,
      *,
      temperature: Optional[float] = None,
      **kwargs
  ) -> models.GenerateOpenEndedTextResult
  ```

  ```python Python example
  result = client.analyze(
    video_id="",
    prompt="",
    temperature=0.7
  )

  print("Result ID:", result.id)
  print(f"Generated Text: {result.data}")
  if result.usage is not None:
      print(f"Output tokens: {result.usage.output_tokens}")
  ```


**Parameters**:

| Name          | Type              | Required | Description                                                                   |
| :------------ | :---------------- | :------- | :---------------------------------------------------------------------------- |
| `video_id`    | `str`             | Yes      | The unique identifier of the video you wish to analyze and generate text for. |
| `prompt`      | `str`             | Yes      | The prompt to customize the output.                                           |
| `temperature` | `Optional[float]` | No       | The temperature to use in the generation.                                     |
| `**kwargs`    | `dict`            | No       | Additional keyword arguments for the request.                                 |

**Return value**: Returns a [`models.GenerateOpenEndedTextResult`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/generate.py) object containing the generated text.

**API Reference**: For a description of each field in the request and response, see the [Open-ended analysis](/v1.3/api-reference/analyze-videos/analyze) page.

**Related guide**: [Open-ended analysis](/v1.3/docs/guides/analyze-videos/open-ended-analysis).

# Open-ended analysis with streaming responses

**Description**: This method generates open-ended texts and supports streaming responses.

**Function signature and example**:


  ```python Function signature
  def analyze_stream(
      self,
      video_id: str,
      prompt: str,
      *,
      temperature: Optional[float] = None,
      **kwargs
  ) -> models.GenerateOpenEndedTextStreamResult
  ```

  ```python Python example
  result = client.analyze_stream(
      video_id="",
      prompt="",
      temperature=0.7
  )
  for text in result:
      print(text)

  print(f"Aggregated text: {result.aggregated_text}")

  if result.usage is not None:
      print(f"Output tokens: {result.usage.output_tokens}")
  ```


**Parameters**:

| Name          | Type              | Required | Description                                                                   |
| :------------ | :---------------- | :------- | :---------------------------------------------------------------------------- |
| `video_id`    | `str`             | Yes      | The unique identifier of the video you wish to analyze and generate text for. |
| `prompt`      | `str`             | No       | The prompt to customize the output.                                           |
| `temperature` | `Optional[float]` | No       | The temperature to use in the generation.                                     |
| `**kwargs`    | `dict`            | No       | Additional keyword arguments for the request.                                 |

**Return value**: Returns a [`models.GenerateOpenEndedTextStreamResult`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/generate.py) object.

**API Reference**: For a description of each field in the request and response, see the [Open-ended analysis](/v1.3/api-reference/analyze-videos/analyze) page.

**Related guide**: [Open-ended analysis](/v1.3/docs/guides/analyze-videos/open-ended-analysis).

# Open-ended text


  This method will be deprecated on 

  **July 30, 2025**

  . Transition to the 

  [`analyze`](/v1.3/sdk-reference/python/analyze-videos#open-ended-analysis)

   method, which provides identical functionality. Ensure you've updated your function calls before the deprecation date to ensure uninterrupted service.


**Description**: This method generates open-ended texts based on your videos.

**Function signature and example**:


  ```python Function signature
  def text(
      self,
      video_id: str,
      prompt: str,
      *,
      temperature: Optional[float] = None,
      **kwargs
  ) -> models.GenerateOpenEndedTextResult
  ```

  ```python Python example
  result = client.generate.text(
    video_id="",
    prompt="",
    temperature=0.7
  )

  print("Result ID:", result.id)
  print(f"Generated Text: {result.data}")
  if result.usage is not None:
      print(f"Output tokens: {result.usage.output_tokens}")
  ```


**Parameters**:

| Name          | Type              | Required | Description                                                             |
| :------------ | :---------------- | :------- | :---------------------------------------------------------------------- |
| `video_id`    | `str`             | Yes      | The unique identifier of the video for which you want to generate text. |
| `prompt`      | `str`             | Yes      | The prompt to customize the output.                                     |
| `temperature` | `Optional[float]` | No       | The temperature to use in the generation.                               |
| `**kwargs`    | `dict`            | No       | Additional keyword arguments for the request.                           |

**Return value**: Returns a [`models.GenerateOpenEndedTextResult`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/generate.py) object containing the generated text.

**API Reference**: For a description of each field in the request and response, see the [Open-ended text](/v1.3/api-reference/analyze-videos/open-ended) page.

# Open-ended text with streaming responses


  This method will be deprecated on 

  **July 30, 2025**

  . Transition to the 

  [`analyze_stream`](/v1.3/sdk-reference/python/analyze-videos#open-ended-analysis-with-streaming-responses)

   method, which provides identical functionality. Ensure you've updated your function calls before the deprecation date to ensure uninterrupted service.


**Description**: This method generates open-ended texts and supports streaming responses.

**Function signature and example**:


  ```python Function signature
  def text_stream(
      self,
      video_id: str,
      prompt: str,
      *,
      temperature: Optional[float] = None,
      **kwargs
  ) -> models.GenerateOpenEndedTextStreamResult
  ```

  ```python Python example
  result = client.generate.text_stream(
      video_id="",
      prompt="",
      temperature=0.7
  )
  for text in result:
      print(text)

  print(f"Aggregated text: {result.aggregated_text}")

  if result.usage is not None:
      print(f"Output tokens: {result.usage.output_tokens}")
  ```


**Parameters**:

| Name          | Type              | Required | Description                                                             |
| :------------ | :---------------- | :------- | :---------------------------------------------------------------------- |
| `video_id`    | `str`             | Yes      | The unique identifier of the video for which you want to generate text. |
| `prompt`      | `str`             | No       | The prompt to customize the output.                                     |
| `temperature` | `Optional[float]` | No       | The temperature to use in the generation.                               |
| `**kwargs`    | `dict`            | No       | Additional keyword arguments for the request.                           |

**Return value**: Returns a [`models.GenerateOpenEndedTextStreamResult`](https://github.com/twelvelabs-io/twelvelabs-python/blob/main/twelvelabs/models/generate.py) object.

**API Reference**: For a description of each field in the request and response, see the [Open-ended text](/v1.3/api-reference/analyze-videos/open-ended) page.

# Error codes

This section lists the most common error messages you may encounter while analyzing videos.

* `token_limit_exceeded`
  * Your request could not be processed due to exceeding maximum token limit. Please try with another request or another video with shorter duration.
* `index_not_supported_for_generate`
  * You can only summarize videos uploaded to an index with an engine from the Pegasus family enabled.


# Node.js SDK

{/* 

The 1.3 version of the API includes significant improvements and introduces breaking changes. If you are using v1.2, refer to the [Migration guide](/v1.3/docs/resources/migration-guide) page for a detailed list of changes, migration instructions, and code examples.

You must use SDK version 0.4.x or later to access API version 1.3.
 */}

The [TwelveLabs Node.js SDK](https://github.com/twelvelabs-io/twelvelabs-js) provides a robust interface for interacting with the TwelveLabs Video Understanding Platform. It simplifies authentication and efficiently processes asynchronous tasks.


  * This section provides an overview of the key methods and their usage. However, it is a partial list of all available methods and fields in the SDK.
  * The SDK is designed to closely follow the structure of the TwelveLabs API. In most cases, you'll find a corresponding method in the SDK for each method in the API.
  * The documentation uses `params` as a generic name for the first parameter of methods that accept multiple properties in an object.
  * While the API uses snake case for function and parameter names, the Node.js SDK adopts camel case. For example, an API parameter like `video_id` is `videoId` in the SDK.


# Sample applications

To get started quickly, see the sample applications below that demonstrate the capabilities of the TwelveLabs Node.js SDK. These applications show how to implement common use cases and the best practices for using the SDK.

* [Building a Shade Finder App: Using TwelveLabs' API to Pinpoint Specific Colors in Videos](https://www.twelvelabs.io/blog/shade-finder)


# Typical workflows

This page provides an overview of common workflows for using the TwelveLabs Node.js SDK. Each workflow consists of a series of steps, with links to detailed documentation for each step.

All workflows involving uploading video content to the platform require asynchronous processing. You must wait for the video processing to complete before proceeding with the subsequent steps.

# Prerequisites

Ensure that the following prerequisites are met before using the Node.js SDK:

* [Node.js](https://nodejs.org/) or [Bun](https://bun.sh) must be installed on your machine.
* To use the platform, you need an API key:
  
    
      If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
    

    
      Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
    

    
      Select the **Copy** icon next to your key.
    
  

# Install the SDK

Install the `twelvelabs-js` package:

```sh Shell
yarn add twelvelabs-js # or npm i twelvelabs-js
```

# Initialize the SDK

1. Import the required packages:

```javascript Node.js
import { TwelveLabs } from "twelvelabs-js";
```

2. Instantiate the SDK client with your API key.:

```javascript Node.js
const client = new TwelveLabs({ apiKey: "" });
```

# Search

Follow the steps in this section to search through your video content and find specific moments, scenes, or information.

**Steps**:

1. [Create an index](/v1.3/sdk-reference/node-js/manage-indexes#create-an-index), enabling the Marengo video understanding model.
2. [Upload videos](/v1.3/sdk-reference/node-js/upload-videos#create-a-video-indexing-task) and monitor the processing.
3. [Perform a search request](/v1.3/sdk-reference/node-js/search#make-a-search-request),  using text or images as queries.


  * The search scope is an individual index.
  * Results support pagination, filtering, sorting, and grouping.


# Create text, image, and audio embeddings

This workflow guides you through creating embeddings for text.

**Steps**:

1. [Create text, image, and audio embeddings](/v1.3/sdk-reference/node-js/create-text-image-and-audio-embeddings)


  Creating text, image, and audio embeddings is a synchronous process.


# Create video embeddings

This workflow guides you through creating embeddings for videos..

**Steps**:

1. [Upload a video](/v1.3/sdk-reference/node-js/create-video-embeddings#create-a-video-embedding-task) and monitor the processing.
2. [Retrieve the embeddings](/v1.3/sdk-reference/node-js/create-video-embeddings#retrieve-video-embeddings).


  Creating video embeddings is an asynchronous process.


# Analyze videos

Follow the steps in this section to analyze your videos and generate text based on their content.

**Steps**:

1. [Create an index](/v1.3/sdk-reference/node-js/manage-indexes#create-an-index), enabling the Pegasus video understanding model.
2. [Upload videos](/v1.3/sdk-reference/node-js/upload-videos#create-a-video-indexing-task) and monitor the processing.
3. Depending on your use case:

* [Generate titles, topics, and hashtags](/v1.3/sdk-reference/node-js/analyze-videos#titles-topics-and-hashtags)
* [Generate summaries, chapters, and highlights](/v1.3/sdk-reference/node-js/analyze-videos#summaries-chapters-and-highlights)
* [Perform open-ended analysis](/v1.3/sdk-reference/node-js/analyze-videos#open-ended-analysis).


# The TwelveLabs class

The [`TwelveLabs`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/index.ts) class is the main entry point for the SDK. It initializes the client and provides access to all the resources.

# Properties

| Name     | Type                                                                                                         | Description                                 |
| -------- | ------------------------------------------------------------------------------------------------------------ | ------------------------------------------- |
| `index`  | [`Resources.Index`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/index/index.ts)   | Use this object to manage your indexes.     |
| `task`   | [`Resources.Task`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/task/index.ts)     | Use this object to upload videos.           |
| `search` | [`Resources.Search`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/search/index.ts) | Use this object to perform search requests. |
| `embed`  | [`Resources.Embed`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/embed/index.ts)   | Use this object to create embeddings.       |

# Methods

## The constructor

**Description**: The constructor creates a new instance of the [TwelveLabs class](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/index.ts).

**Function signature and example**:


  ```javascript Function signature
  constructor({ apiKey, version = DEFAULT_API_VERSION }: ClientOptions) 
  ```

  ```javascript Node.js example
  const client = new TwelveLabs({ apiKey: "" });
  ```


**Parameters**:

| Name      | Type     | Required | Description                                             |
| :-------- | :------- | :------- | :------------------------------------------------------ |
| `apiKey`  | `string` | Yes      | Your TwelveLabs API key.                                |
| `version` | `string` | No       | The API version to use. Defaults to the latest version. |

**Return value**: Implicitly returns the newly created instance.


# Manage indexes

An index is a basic unit for organizing and storing video data consisting of video embeddings and metadata. Indexes facilitate information retrieval and processing. The [`Resources.Index`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/index/index.ts) class provides methods to manage your indexes.

# Properties

| Name    | Type                                                                                                  | Description                                                    |
| :------ | :---------------------------------------------------------------------------------------------------- | :------------------------------------------------------------- |
| `video` | [`VideoResource`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/video/index.ts) | Use this property to manage the videos uploaded to this index. |




# Methods

## Create an index

**Description**: This method creates a new index based on the provided parameters.

**Function signature and example**:


  ```javascript Function signature
  async create(
    { name, models, addons }: CreateIndexParams,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const createdIndex = await client.index.create({
    name: "",
    models: [
      {
        name: "marengo2.7",
        options: ["visual", "audio"],
      },
    ],
    addons: ["thumbnail"],
  });
  console.log(`ID: ${createdIndex.id}`);
  console.log(`Name: ${createdIndex.name}`);
  console.log("Models:");
  createdIndex.models.forEach((model, index) => {
    console.log(`  Model ${index + 1}:`);
    console.log(`    Name: ${model.name}`);
    console.log(`    Options: ${JSON.stringify(model.options)}`);
  });
  console.log(`Video count: ${createdIndex.videoCount}`);
  console.log(`Total turation: ${createdIndex.totalDuration} seconds`);
  console.log(`Created at: ${createdIndex.createdAt}`);
  if (createdIndex.updatedAt) {
    console.log(`Updated at: ${createdIndex.updatedAt}`);
  }
  ```


**Parameters**

| Name      | Type                                                                                                              | Required | Description                                           |
| :-------- | :---------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `params`  | [`CreateIndexParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/index/interfaces.ts) | Yes      | Parameters for creating the index.                    |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                          | No       | Additional options for the request. Defaults to `{}`. |

The `CreateIndexParams` interface has the following properties:

| Name     | Type                                                                       | Required | Description                                                                                                                                                                           |
| :------- | :------------------------------------------------------------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `name`   | `string`                                                                   | Yes      | The name of the new index.                                                                                                                                                            |
| `models` | `name: 'marengo2.7' \| 'pegasus1.2'; options: ('visual' \| 'audio')[] }[]` | Yes      | A list of objects specifying the video understanding models and the model options you want to enable for this index. Each object is a dictionary with two keys: `name' and 'options`. |
| `addons` | `string[]`                                                                 | No       | A list specifying which add-ons you want to enable.                                                                                                                                   |

**Return value**: Returns a `Promise` that resolves to a [`Models.Index`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/index/index.ts)  instance representing the newly created index.

**API Reference**: For a description of each field in the request and response, see the [Create an index](/v1.3/docs/concepts/indexes#create-an-index) page.

**Related guide**: [Create an index](/v1.3/docs/concepts/indexes#create-an-index).

## Retrieve an index

**Description**: This method retrieves details of a specific index.

**Function signature and example**:


  ```javascript Function signature
  async retrieve(id: string, options: RequestOptions = {}): Promise
  ```

  ```javascript Node.js example
  const retrievedIndex = await client.index.retrieve("");
  console.log(`ID: ${retrievedIndex.id}`);
  console.log(`Name: ${retrievedIndex.name}`);
  console.log("Models:");
  retrievedIndex.modles.forEach((model, index) => {
    console.log(`  Model ${index + 1}:`);
    console.log(`    Name: ${model.name}`);
    console.log(`    Options: ${JSON.stringify(model.options)}`);
  });
  console.log(`Video count: ${retrievedIndex.videoCount}`);
  console.log(`Total turation: ${retrievedIndex.totalDuration} seconds`);
  console.log(`Created at: ${retrievedIndex.createdAt}`);
  if (retrievedIndex.updatedAt) {
    console.log(`Updated at: ${retrievedIndex.updatedAt}`);
  ```


**Parameters**

| Name      | Type                                                                                     | Required | Description                                              |
| :-------- | :--------------------------------------------------------------------------------------- | :------- | :------------------------------------------------------- |
| `id`      | `string`                                                                                 | Yes      | The unique identifier of the index you want to retrieve. |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.    |

**Return value**: Returns a `Promise` that resolves to a [`Models.Index`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/index/index.ts) instance.

**API Reference**: For a description of each field in the request and response, see the [Retrieve an index](/v1.3/api-reference/indexes/retrieve) page

## List indexes with direct pagination

**Description**: The list method retrieves a paginated list of indexes based on the provided parameters. Choose this method mainly when the total number of items is manageable, or you must fetch a single page of results. By default, the platform returns your indexes sorted by creation date, with the newest at the top of the list.

**Function signature and example**:


  ```javascript Function signature
  async list(
    { id, name, createdAt, updatedAt, ...restParams }: ListIndexParams = {},
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const listParams = {
    id: "",
    name: "",
    modelOptions: ["audio", "visual"],
    modelFamily: "marengo",
    page: 1,
    pageLimit: 5,
    sortBy: "updated_at",
    sortOption: "asc"
  };

  const indexes = await client.index.list(listParams);
  indexes.forEach(index => {
    console.log(`ID: ${index.id}`);
    console.log(`  Name: ${index.name}`);
    console.log("  Models:");
    index.models.forEach((model, index) => {
      console.log(`    Model ${index + 1}:`);
      console.log(`      Name: ${model.name}`);
      console.log(`      Options: ${JSON.stringify(model.options)}`);
    });
    console.log(`  Video count: ${index.videoCount}`);
    console.log(`  Total duration: ${index.totalDuration} seconds`);
    console.log(`  Created at: ${index.createdAt}`);
    if (index.updatedAt) {
      console.log(`  Updated at: ${index.updatedAt}`);
    }
  });
  ```


**Parameters**

| Name      | Type                                                                                                            | Required | Description                                                      |
| :-------- | :-------------------------------------------------------------------------------------------------------------- | :------- | :--------------------------------------------------------------- |
| `params`  | [`ListIndexParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/index/interfaces.ts) | No       | Parameters for retrieving the list of indexes. Defaults to `{}`. |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                        | No       | Additional options for the request. Defaults to `{}`.            |

The `ListIndexParams` interface extends the `PageOptions` interface and defines the parameters for listing indexes:

| Name           | Type                               | Required | Description                                                                                                                       |
| -------------- | ---------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------- |
| `id`           | `string`                           | No       | Filter by the unique identifier of an index.                                                                                      |
| `name`         | `string`                           | No       | Filter by the name of an index.                                                                                                   |
| `modelOptions` | `string[]`                         | No       | Filter by model options.                                                                                                          |
| `modelFamily`  | `'marengo' \| 'pegasus'`           | No       | Filter by the model family. It must be either 'marengo' or 'pegasus'.                                                             |
| `createdAt`    | `string \| Record` | No       | Filter by the creation date. This parameter can be a string or an object with string keys and string values for range queries.    |
| `updatedAt`    | `string \| Record` | No       | Filter by the last update date. This parameter can be a string or an object with string keys and string values for range queries. |

The following properties are inherited from `PageOptions`:

| Name         | Type                         | Required | Description                                                             |
| ------------ | ---------------------------- | -------- | ----------------------------------------------------------------------- |
| `page`       | `number`                     | No       | Page number for pagination. Defaults to 1.                              |
| `pageLimit`  | `number`                     | No       | Number of items per page. Defaults to 10.                               |
| `sortBy`     | `'createdAt' \| 'updatedAt'` | No       | Field to sort by ("createdAt" or "updatedAt"). Defaults to "createdAt". |
| `sortOption` | `'asc' \| 'desc'`            | No       | Sort order ("asc" or "desc"). Defaults to "desc".                       |

**Return value**: Returns a `Promise` that resolves to an array of [`Models.Index`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/index/index.ts) instances.

**API Reference**: For a description of each field in the request and response, see the [List indexes](/v1.3/api-reference/indexes/list) page.

## List indexes with iterative pagination

**Description**: This method iterates through a paginated list of indexes based on the provided parameters.  Choose this method mainly when your application must retrieve a large number of items. By default, the platform returns your indexes sorted by creation date, with the newest at the top of the list.

**Function signature and example**:


  ```javascript Function signature
    async listPagination(
      { id, name, createdAt, updatedAt, ...restParams }: ListIndexParams = {},
      options: RequestOptions = {},
    ): Promise
  ```

  ```javascript Node.js example
  function printIndexPage(indexPage) {
    indexPage.forEach(index => {
      console.log(`ID: ${index.id}`);
      console.log(`  Name: ${index.name}`);
      console.log("  Models:");
      index.models.forEach((model, index) => {
        console.log(`    Model ${index + 1}:`);
        console.log(`      Name: ${model.name}`);
        console.log(`      Options: ${JSON.stringify(model.options)}`);
      });
      console.log(`  Video count: ${index.videoCount}`);
      console.log(`  Total duration: ${index.totalDuration} seconds`);
      console.log(`  Created at: ${index.createdAt}`);
      if (index.updatedAt) {
        console.log(`  Updated at: ${index.updatedAt}`);
      }
    });
  }

  const listParams = {
    id: "",
    name: "",
    modelOptions: ["audio", "visual"],
    modelFamily: "marengo",
    page: 1,
    pageLimit: 5,
    sortBy: "updated_at",
    sortOption: "asc"
  };

  // Fetch the initial page of results
  const indexPaginator = await client.index.listPagination(listParams);

  // Print the first page of results
  printIndexPage(indexPaginator.data);

  // Iterate through subsequent pages
  while (true) {
    const nextIndexPage = await indexPaginator.next();
    if (!nextIndexPage) {
      console.log('No more pages of index results available');
      break;
    }
    printIndexPage(nextIndexPage);
  }
  ```


**Parameters**

| Name      | Type                                                                                                            | Required | Description                                                |
| :-------- | :-------------------------------------------------------------------------------------------------------------- | :------- | :--------------------------------------------------------- |
| `params`  | [`ListIndexParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/index/interfaces.ts) | No       | Parameters retrieving the list of indexes. Defaults to {}. |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                        | No       | Additional options for the request. Defaults to `{}`.      |

The `ListIndexParams` interface extends the `PageOptions` interface and defines the parameters for listing indexes:

| Name           | Type                               | Required | Description                                                                                                                       |
| -------------- | ---------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------- |
| `id`           | `string`                           | No       | Filter by the unique identifier of an index.                                                                                      |
| `name`         | `string`                           | No       | Filter by the name of an index.                                                                                                   |
| `modelOptions` | `string[]`                         | No       | Filter by the model options.                                                                                                      |
| `modelFamily`  | `'marengo' \| 'pegasus'`           | No       | Filter by the model family. It must be either 'marengo' or 'pegasus'.                                                             |
| `createdAt`    | `string \| Record` | No       | Filter by the creation date. This parameter can be a string or an object with string keys and string values for range queries.    |
| `updatedAt`    | `string \| Record` | No       | Filter by the last update date. This parameter can be a string or an object with string keys and string values for range queries. |

The following properties are inherited from `PageOptions`:

| Name         | Type                         | Required | Description                                                              |
| ------------ | ---------------------------- | -------- | ------------------------------------------------------------------------ |
| `page`       | `number`                     | No       | Page number for pagination. Defaults to 1.                               |
| `pageLimit`  | `number`                     | No       | Number of items per page. Defaults to 10.                                |
| `sortBy`     | `'createdAt' \| 'updatedAt'` | No       | Field to sort by ("createdAt" or "updatedAt").  Defaults to "createdAt". |
| `sortOption` | `'asc' \| 'desc'`            | No       | Sort order ("asc" or "desc"). Defaults to "desc".                        |

**Return value**: Returns a `Promise` that resolves to a [`Models.IndexListWithPagination`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/index/index.ts) instance.


  To retrieve subsequent pages of results, use the async iterator protocol:

  1. Invoke the `next`method of the `IndexListWithPagination` object.
  2. Repeat this call until the method returns  a promise that resolves to`null`, indicating no more pages exist.


**API Reference**: For a description of each field in the request and response, see the [List indexes](/v1.3/api-reference/indexes/list) page.

## Update an index

**Description**: This method updates the name of an existing index.

**Function signature and example**:


  ```javascript Function signature
  async update(id: string, name: string, options: RequestOptions = {}): Promise
  ```

  ```javascript Node.js example
  await client.index.update("", "");
  ```


**Parameters**:

| Name      | Type                                                                                     | Required | Description                                            |
| :-------- | :--------------------------------------------------------------------------------------- | :------- | :----------------------------------------------------- |
| `id`      | `string`                                                                                 | Yes      | The unique identifier of the index you want to update. |
| `name`    | `string`                                                                                 | Yes      | The new name of the index.                             |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.  |

**Return value**: Returns a `Promise` that resolves to `void`. This method doesn't return any data upon successful completion.

**API Reference**: [Update an index](/v1.3/api-reference/indexes/update).

## Delete an index

**Description**: This method deletes an existing index.

**Function signature and example**:


  ```javascript Function signature
  async delete(id: string, options: RequestOptions = {}): Promise
  ```

  ```javascript Node.js example
  await client.index.delete("YOUR_INDEX_ID>");
  ```


**Parameters**:

| Name      | Type                                                                                     | Required | Description                                           |
| :-------- | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `id`      | `string`                                                                                 | Yes      | The unique identifier of the index to delete.         |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`. |

**Return value**: Returns a `Promise` that resolves to `void`. This method doesn't return any data upon successful completion.

**API Reference**: [Delete an index](/v1.3/api-reference/indexes/delete).

# Error codes

This section lists the most common error messages you may encounter while managing indexes.

* `index_option_cannot_be_changed`
  * Index option cannot be changed. Please remove index\_options parameter and try again. If you want to change index option, please create new index.
* `index_engine_cannot_be_changed`
  * Index engine cannot be changed. Please remove engine\_id parameter and try again. If you want to change engine, please create new index.
* `index_name_already_exists`
  * Index name `{index_name}` already exists. Please use another unique name and try again.


# Upload videos

A video indexing task represents a request to upload and index a video. The [`Resources.Task`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/task/index.ts) class provides methods to manage your video indexing tasks.

# Methods

## Create a video indexing task

**Description**: This method creates a new video indexing task that uploads and indexes a video.


  The videos you wish to upload must meet the following requirements:

  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the [FFmpeg Formats Documentation](https://ffmpeg.org/ffmpeg-formats.html) page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).
  * **Duration**: For Marengo, it must be between 4 seconds and 2 hours (7,200s). For Pegasus, it must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration for Pegasus will be 2 hours (7,200 seconds).
  * **File size**: Must not exceed 2 GB.
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  If both Marengo and Pegasus are enabled for your index, the most restrictive prerequisites will apply.


**Function signature and example**:


  ```javascript Function signature
  async create(body: CreateTaskParams, options: RequestOptions = {}): Promise 
  ```

  ```javascript Node.js example
  import { Task } from "twelvelabs-js";

  const task = await client.task.create({
    indexId: "",
    file: ""
  });
  console.log(`Task ID:${task.id}`);
  await task.waitForDone(5000, (task: Task) => {
    console.log(`  Status=${task.status}`);
  });

  if (task.status !== "ready") {
    throw new Error(`Indexing failed with status ${task.status}`);
  }
  console.log(`Video ID: ${task.videoId}`);
  ```


**Parameters**:

| Name      | Type                                                                                                            | Required | Description                                           |
| :-------- | :-------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `body`    | [`CreateTaskParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/task/interfaces.ts) | Yes      | Parameters for creating the new video indexing task.  |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                        | No       | Additional options for the request. Defaults to `{}`. |

The `CreateTaskParams` interface defines the parameters for creating a new video indexing task:

| Name                | Type                                        | Required | Description                                                                                                                       |
| ------------------- | ------------------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------- |
| `indexId`           | `string`                                    | Yes      | The unique identifier of the index to which the video will be uploaded.                                                           |
| `file`              | `Buffer \| NodeJS.ReadableStream \| string` | No       | The file you want to upload. This parameter can be a `Buffer`, a `ReadableStream`, or a string representing the path to the file. |
| `url`               | `string`                                    | No       | The publicly accessible URL of the video you want to upload.                                                                      |
| `enableVideoStream` | `boolean`                                   | No       | Indicates if the platform stores the video for streaming.                                                                         |

**Return value**: Returns a `Promise` that resolves to a [`Models.Task`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/task/index.ts) instance representing the newly created video indexing task.

**API Reference**: For a description of each field in the request and response, see the [Create a video indexing task](/v1.3/api-reference/tasks/create) page.

**Related guide**: [Upload single videos](/v1.3/docs/guides/upload-videos/single).

## Retrieve a video indexing task

**Description**: This method retrieves the details of a specific video indexing task.

**Function signature and example**:


  ```javascript Function signature
  async retrieve(id: string, options: RequestOptions = {}): Promise 
  ```

  ```javascript Node.js Example
  const retrievedTask = await client.task.retrieve("");
  console.log(`Task ID=${retrievedTask.id}`);
  console.log(`Index ID: ${retrievedTask.indexId}`);
  console.log(`Video ID: ${retrievedTask.videoId}`);
  console.log(`Status: ${retrievedTask.status}`);

  console.log("System metadata:");
  for (const [key, value] of Object.entries(retrievedTask.systemMetadata)) {
    console.log(`  ${key}: ${value}`);
  }

  if (retrievedTask.hls) {
    console.log("HLS:");
    console.log(`  Video URL: ${retrievedTask.hls.videoUrl}`);
    console.log("  Thumbnail URLs:");
    for (const url of retrievedTask.hls.thumbnailUrls || []) {
      console.log(`    ${url}`);
    }
    console.log(`  Status: ${retrievedTask.hls.status}`);
    console.log(`  Updated at: ${retrievedTask.hls.updatedAt}`);
  }

  if (retrievedTask.process) {
    console.log("Process:");
    console.log(`  Percentage: ${retrievedTask.process.percentage}%`);
    console.log(`  Remaining Seconds: ${retrievedTask.process.remainSeconds}`);
  }

  console.log(`Created at: ${retrievedTask.createdAt}`);
  console.log(`Updated at: ${retrievedTask.updatedAt}`);
  ```


**Parameters**

| Name      | Type                                                                                     | Required | Description                                           |
| :-------- | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `id`      | `string`                                                                                 | Yes      | The unique identifier of the task to retrieve.        |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`. |

**Return value**: Returns a `Promise` that resolves to a [`Models.Task`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/task/index.ts) instance.

**API Reference**: For a description of each field in the response, see the [Retrieve a video indexing task](/v1.3/api-reference/tasks/retrieve).

## Wait for a video indexing task to complete

**Description**: This method waits until a video indexing task is completed. It checks the task status at regular intervals by retrieving its details. If you provide a callback function, the method calls it after each status check with the current task object. This allows you to monitor the progress of the task.


  ```javascript Function signature
  async waitForDone(sleepInterval: number = 5000, callback?: (task: Task) => void): Promise
  ```

  ```javascript Node.js example
  await task.waitForDone(5000, (task) => {
    console.log(`  Status=${task.status}`);
  });

  if (task.status !== "ready") {
    throw new Error(`Indexing failed with status ${task.status}`);

  }
  ```


**Parameters**

| Name            | Type                   | Required | Description                                                                                                              |
| :-------------- | :--------------------- | :------- | :----------------------------------------------------------------------------------------------------------------------- |
| `sleepInterval` | `number`               | No       | The time in milliseconds to wait between status checks. Must be greater than 0. Default is 5000.                         |
| `callback`      | `(task: Task) => void` | No       | An optional function to call after each status check. It receives the current task object. Use this to monitor progress. |

**Return value**: A `Promise` that resolves to the `Task` object once the task is "ready" or "failed".

## List video indexing tasks with direct pagination

**Description**: This method returns a paginated list of the video indexing tasks in your account based on the provided parameters. By default, the platform returns your video indexing tasks sorted by creation date, with the newest at the top of the list. Choose this method mainly when the total number of items is manageable, or you must fetch a single page of results.

**Function signature and example**:


  ```javascript Function signature
  async list(
    { id, createdAt, updatedAt, ...restParams }: ListTaskParams = {},
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const listParams = {
    id: "",
    indexId: "",
    filename: "",
    duration: 20,
    width:1920,
    height: 1080,
    sortBy: "updated_at",
    sortOption: "asc",
    estimatedTime: "2024-09-17T07:55:22.125Z",
    page: 2,
    pageLimit: 5,
  };
  const tasks = await client.task.list(listParams)
  tasks.forEach(task => {
    console.log(`Task ID=${task.id}`);
    console.log(`Index ID: ${task.indexId}`);
    console.log(`Video ID: ${task.videoId}`);
    console.log(`Status: ${task.status}`);
    
    console.log("System metadata:");
    for (const [key, value] of Object.entries(task.systemMetadata)) {
      console.log(`  ${key}: ${value}`);
    }

    if (task.hls) {
      console.log("HLS:");
      console.log(`  Video URL: ${task.hls.videoUrl}`);
      console.log("  Thumbnail URLs:");
      for (const url of task.hls.thumbnailUrls || []) {
        console.log(`    ${url}`);
      }
      console.log(`  Status: ${task.hls.status}`);
      console.log(`  Updated at: ${task.hls.updatedAt}`);
    }

    if (task.process) {
      console.log("Process:");
      console.log(`  Percentage: ${task.process.percentage}%`);
      console.log(`  Remaining Seconds: ${task.process.remainSeconds}`);
    }

    console.log(`Created at: ${task.createdAt}`);
    console.log(`Updated at: ${task.updatedAt}`);
  });
  ```


**Parameters**:

| Name      | Type                                                                                                          | Required | Description                                                   |
| :-------- | :------------------------------------------------------------------------------------------------------------ | :------- | :------------------------------------------------------------ |
| `params`  | [`ListTaskParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/task/interfaces.ts) | No       | Parameters for filtering the list of tasks. Defaults to `{}`. |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                      | No       | Additional options for the request. Defaults to `{}`.         |

The `ListTaskParams` interface extends the `PageOptions` interface and defines the parameters for listing video indexing tasks:

| Name        | Type                               | Required | Description                                                                                                                                         |
| ----------- | ---------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| `id`        | `string`                           | No       | Filter by the unique identifier of a video indexing task.                                                                                           |
| `indexId`   | `string`                           | No       | Filter by the unique identifier of an index.                                                                                                        |
| `filename`  | `string`                           | No       | Filter by the name of a video file.                                                                                                                 |
| `duration`  | `number`                           | No       | Filter by the duration of a video expressed in seconds.                                                                                             |
| `width`     | `number`                           | No       | Filter by the width of a video.                                                                                                                     |
| `height`    | `number`                           | No       | Filter by the height of a video.                                                                                                                    |
| `createdAt` | `string \| Record` | No       | Filter by the creation date of a video indexing task. This parameter can be a string or an object with string keys and values for range queries.    |
| `updatedAt` | `string \| Record` | No       | Filter by the last update date of a video indexing task. This parameter can be a string or an object with string keys and values for range queries. |

The following properties are inherited from `PageOptions`:

| Name         | Type                         | Required | Description                                                            |
| ------------ | ---------------------------- | -------- | ---------------------------------------------------------------------- |
| `page`       | `number`                     | No       | Page number for pagination. Defaults to 1.                             |
| `pageLimit`  | `number`                     | No       | Number of items per page. Defaults to 10.                              |
| `sortBy`     | `'createdAt' \| 'updatedAt'` | No       | Field to sort by ("createdAt" or "updatedAt"). Defaults to "createAt". |
| `sortOption` | `'asc' \| 'desc'`            | No       | Sort order ("asc" or "desc"). Defaults to "desc".                      |

**Return value**: Returns a `Promise` that resolves to an array of [`Models.Task`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/task/index.ts)  instances.

**API Reference**: For a description of each field in the request and response, see the [List video indexing tasks](/v1.3/api-reference/tasks/list) page.

**Related guides**:

* [Direct pagination](/v1.3/docs/guides/pagination/indexes-videos-and-tasks#direct-pagination).
* [Sorting > Indexes, videos, and tasks](/v1.3/docs/guides/sorting/indexes-videos-and-tasks).
* [Filtering > Indexes, videos, and tasks](/v1.3/docs/guides/filtering/indexes-videos-and-tasks).

## List video indexing tasks with iterative pagination

**Description**: This method returns a paginated list of the video indexing tasks in your account based on the provided parameters. Choose this method mainly when your application must retrieve a large number of items. By default, the platform returns your video indexing tasks sorted by creation date, with the newest at the top of the list.

**Function signature and example**:


  ```javascript Function signature
  async listPagination(
    { id, createdAt, updatedAt, ...restParams }: ListTaskParams = {},
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  function printPage(page) {
    page.forEach(task => {
      console.log(`Task ID=${task.id}`);
      console.log(`Index ID: ${task.indexId}`);
      console.log(`Video ID: ${task.videoId}`);
      console.log(`Status: ${task.status}`);
      
      console.log("System metadata:");
      for (const [key, value] of Object.entries(task.systemMetadata)) {
        console.log(`  ${key}: ${value}`);
      }
    
      if (task.hls) {
        console.log("HLS:");
        console.log(`  Video URL: ${task.hls.videoUrl}`);
        console.log("  Thumbnail URLs:");
        for (const url of task.hls.thumbnailUrls || []) {
          console.log(`    ${url}`);
        }
        console.log(`  Status: ${task.hls.status}`);
        console.log(`  Updated at: ${task.hls.updatedAt}`);
      }
    
      if (task.process) {
        console.log("Process:");
        console.log(`  Percentage: ${task.process.percentage}%`);
        console.log(`  Remaining Seconds: ${task.process.remainSeconds}`);
      }
    
      console.log(`Created at: ${task.createdAt}`);
      console.log(`Updated at: ${task.updatedAt}`);
    });
  }
  const listParams = {
      id: "",
      indexId: "",
      filename: "",
      duration: 20,
      width:1920,
      height: 1080,
      sortBy: "updated_at",
      sortOption: "asc",
      createdAt: "2024-09-17T07:53:46.365Z",
      updatedAt: "2024-09-17T07:53:46.365Z",
      page: 2,
      pageLimit: 5,
  };

  // Fetch the initial page of results
  const taskPaginator = await client.task.listPagination(listParams);

  // Print the first page of results
  printPage(taskPaginator.data);

  // Iterate through subsequent pages
  while (true) {
  const nextTaskPage = await taskPaginator.next();
  if (!nextTaskPage) {
      console.log('No more pages of index results available');
      break;
  }
  printPage(nextTaskPage);
  }
  ```


**Parameters**

| Name      | Type                                                                                                          | Required | Description                                                                  |
| :-------- | :------------------------------------------------------------------------------------------------------------ | :------- | :--------------------------------------------------------------------------- |
| `params`  | [`ListTaskParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/task/interfaces.ts) | No       | Parameters for filtering and paginating the list of tasks. Defaults to `{}`. |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                      | No       | Additional options for the request. Defaults to `{}`.                        |

The `ListTaskParams` interface extends the `PageOptions` interface and defines the parameters for listing video indexing tasks:

| Name        | Type                               | Required | Description                                                                                                                                         |
| ----------- | ---------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| `id`        | `string`                           | No       | Filter by the unique identifier of a video indexing task.                                                                                           |
| `indexId`   | `string`                           | No       | Filter by the unique identifier of an index.                                                                                                        |
| `filename`  | `string`                           | No       | Filter by the name of a video file.                                                                                                                 |
| `duration`  | `number`                           | No       | Filter by the duration of a video expressed in seconds.                                                                                             |
| `width`     | `number`                           | No       | Filter by the width of a video.                                                                                                                     |
| `height`    | `number`                           | No       | Filter by the height of a video.                                                                                                                    |
| `createdAt` | `string \| Record` | No       | Filter by the creation date of a video indexing task. This parameter can be a string or an object with string keys and values for range queries.    |
| `updatedAt` | `string \| Record` | No       | Filter by the last update date of a video indexing task. This parameter can be a string or an object with string keys and values for range queries. |

The following properties are inherited from `PageOptions`:

| Name         | Type                         | Required | Description                                                             |
| ------------ | ---------------------------- | -------- | ----------------------------------------------------------------------- |
| `page`       | `number`                     | No       | Page number for pagination. Defaults to 1.                              |
| `pageLimit`  | `number`                     | No       | Number of items per page. Defaults to 10.                               |
| `sortBy`     | `'createdAt' \| 'updatedAt'` | No       | Field to sort by ("createdAt" or "updatedAt"). Defaults to "createdAt". |
| `sortOption` | `'asc' \| 'desc'`            | No       | Sort order ("asc" or "desc"). Defaults to "desc".                       |

**Return value**: Returns a `Promise` that resolves to a [`Models.TaskListWithPagination`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/task/index.ts) instance.


  To retrieve subsequent pages of results, use the async iterator protocol:

  1. Invoke the `next`method of the `TaskListWithPagination` object.
  2. Repeat this call until the method returns  a promise that resolves to`null`, indicating no more pages exist.


**API Reference**: For a description of each field in the request and response, see the [List video indexing tasks](/v1.3/api-reference/tasks/list) page.

**Related guides**:

* [Iterative pagination](/v1.3/docs/guides/pagination/indexes-videos-and-tasks#iterative-pagination)
* [Sorting > Indexes, videos, and tasks](/v1.3/docs/guides/sorting/indexes-videos-and-tasks).
* [Filtering > Indexes, videos, and tasks](/v1.3/docs/guides/filtering/indexes-videos-and-tasks) .

## Delete a video indexing task

**Description**: This method deletes an existing video indexing task.

**Function signature and example**:


  ```javascript Function signature
  async delete(id: string, options: RequestOptions = {}): Promise
  ```

  ```javascript Node.js example
  await client.task.delete("");
  ```


**Parameters**:

| Name      | Type                                                                                     | Required | Description                                                 |
| :-------- | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------- |
| `id`      | `string`                                                                                 | Yes      | The unique identifier of the video indexing task to delete. |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.       |

**Return value**: Returns a `Promise` that resolves to `void`. This method doesn't return any data upon successful completion.

**API Reference**: [Delete a video indexing task](/v1.3/api-reference/tasks/delete).

## Import videos

**Description**: An import represents the process of uploading and indexing all videos from the specified integration. This method initiates an asynchronous import.


  The videos you wish to upload must meet the following requirements:

  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the [FFmpeg Formats Documentation](https://ffmpeg.org/ffmpeg-formats.html) page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).
  * **Duration**: For Marengo, it must be between 4 seconds and 2 hours (7,200s). For Pegasus, it must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration for Pegasus will be 2 hours (7,200 seconds).
  * **File size**: Must not exceed 2 GB.
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).

  If both Marengo and Pegasus are enabled for your index, the most restrictive prerequisites will apply.


**Function signature and example**:


  ```javascript Function signature
  async importVideos(
    { integrationId, ...restParams }: TransferImportParams,
    options: RequestOptions = {},
  ): Promise 
  ```

  ```javascript Node.js example
  const res = await client.task.transfers.importVideos({"", "" });
  res.videos.forEach((v) => {
    console.log(`video: ${v.videoId} ${v.filename}`);
  });
  res.failedFiles?.forEach((f) => {
    console.log(`failed file: ${f.filename} ${f.errorMessage}`);
  });
  ```


**Parameters**:

| Name            | Type                                                                                     | Required | Description                                                                   |
| :-------------- | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------------------------- |
| `integrationId` | `string`                                                                                 | Yes      | The unique identifier of the integration for which you want to import videos. |
| `indexId`       | `string`                                                                                 | Yes      | The unique identifier of the index to which the videos are being uploaded.    |
| `options`       | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.                         |

**Return value**: Returns a [`Models.TransferImportResponse`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/task/index.ts)  object containing two lists:

* Videos that will be imported.
* Videos that will not be imported, typically due to unmet [prerequisites](/v1.3/docs/guides/search#prerequisites).

**API Reference**: [Import videos](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/create).

**Related guide**: [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations).

## Retrieve import status

**Description**: This method retrieves the current status for each video from a specified integration and index.

**Function signature and example**:


  ```javascript Function signature
  async importStatus(
    integrationId: string,
    indexId: string,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const status = await client.task.transfers.importStatus("", "");
  status.ready.forEach((v) => {
    console.log(`ready: ${v.videoId} ${v.filename} ${v.createdAt}`);
  });
  status.failed.forEach((f) => {
    console.log(`failed: ${f.filename} ${f.errorMessage}`);
  });
  ```


**Parameters**:

| Name            | Type                                                                                     | Required | Description                                                                                                 |
| :-------------- | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------------------------------------------------------- |
| `integrationId` | `string`                                                                                 | Yes      | The unique identifier of the integration for which you want to retrieve the status of your imported videos. |
| `indexId`       | `string`                                                                                 | Yes      | The unique identifier of the index for which you want to retrieve the status of your imported videos.       |
| `options`       | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.                                                       |

**Return value**:  Returns a [`Models.TransferImportStatusResponse`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/task/index.ts)  object containing lists of videos grouped by status. See the [Task object](/v1.3/api-reference/tasks/the-task-object)  page for details on each status.

**API Reference**: [Retrieve import status](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/get-status).

**Related guide**: [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations).

## Retrieve import logs

**Description**: This method returns a chronological list of import operations for the specified integration.

**Function signature and example**:


  ```javascript Function signature
  async importLogs(integrationId: string, options: RequestOptions = {}): Promise
  ```

  ```javascript Node.js example
  const logs = await client.task.transfers.importLogs("");
  logs.forEach((l) => {
    console.log(
      `indexId: ${l.indexId} indexName: ${l.indexName} createdAt: ${l.createdAt} endedAt: ${l.endedAt} videoStatus: ${l.videoStatus}`,
    );
    l.failedFiles?.forEach((f) => {
      console.log(`failed file: ${f.filename} ${f.errorMessage}`);
    });
  });
  ```


**Parameters**:

| Name            | Type                                                                                     | Required | Description                                                                              |
| :-------------- | :--------------------------------------------------------------------------------------- | :------- | :--------------------------------------------------------------------------------------- |
| `integrationId` | `string`                                                                                 | Yes      | The unique identifier of the integration for which you want to retrieve the import logs. |
| `options`       | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.                                    |

**Return value**: Returns a [`Models.TransferImportLog`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/task/index.ts)  object containing a chronological list of import operations for the specified integration. The list is sorted by creation date, with the oldest imports first. Each item in the list contains:

* The number of videos in each status
* Detailed error information for failed uploads, including filenames and error messages.

**API Reference**: [Retrieve import logs](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/get-logs).

**Related guide**: [Cloud-to-cloud integrations](/v1.3/docs/advanced/cloud-to-cloud-integrations).

# Error codes

This section lists the most common error messages you may encounter while uploading videos.

* `video_resolution_too_low`
  * The resolution of the video is too low. Please upload a video with resolution between 360x360 and 3840x2160. Current resolution is `{current_resolution}`.
* `video_resolution_too_high`
  * The resolution of the video is too high. Please upload a video with resolution between 360x360 and 3840x2160. Current resolution is `{current_resolution}`.
* `video_resolution_invalid_aspect_ratio`
  * The aspect ratio of the video is invalid. Please upload a video with aspect ratio between 1:1 and 2.4:1. Current resolution is `{current_resolution}`.
* `video_duration_too_short`
  * Video is too short. Please use video with duration between 10 seconds and 2 hours(7200 seconds). Current duration is `{current_duration}` seconds.
* `video_duration_too_long`
  * Video is too long. Please use video with duration between 10 seconds and 2 hours(7200 seconds). Current duration is `{current_duration}` seconds.
* `video_file_broken`
  * Cannot read video file. Please check the video file is valid and try again.
* `task_cannot_be_deleted`
  * (Returns raw error message)
* `usage_limit_exceeded`
  * Not enough free credit. Please register a payment method or contact [sales@twelvelabs.io](mailto:sales@twelvelabs.io).
* `video_filesize_too_large`
  * The video is too large. Please use a video with a size less than `{maximum_size}`. The current size is `{current_file_size}`.


# Manage videos

The [`Resources.Video`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/video/index.ts) class provides methods to manage the videos you've uploaded to the platform.

# Methods

## Retrieve video information

**Description**: This method retrieves information about the specified video.

**Function signature and example**:


  ```javascript Function signature
  async retrieve(
    indexId: string,
    id: string,
    { embeddingOption }: RetrieveVideoParams = {},
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const printSegments = (segments: SegmentEmbedding[], maxElements = 5) => {
    segments.forEach((segment) => {
      console.log(
        `  embeddingScope=${segment.embeddingScope} embeddingOption=${segment.embeddingOption} startOffsetSec=${segment.startOffsetSec} endOffsetSec=${segment.endOffsetSec}`
      );
      console.log(
        "  embeddings: ",
        segment.embeddingsFloat.slice(0, maxElements)
      );
    });
  };

  const video = await client.index.video.retrieve(
    "",
    "",
    { embeddingOption: ['visual-text', 'audio'] }
  );
  console.log(`ID: ${video.id}`);
  console.log(`Created At: ${video.createdAt}`);
  console.log(`Updated At: ${video.updatedAt || "N/A"}`);
  console.log(`Indexed At: ${video.indexedAt || "N/A"}`);
  console.log("System metadata:");
  console.log(`  Filename: ${video.systemMetadata.filename}`);
  console.log(`  Duration: ${video.systemMetadata.duration}`);
  console.log(`  FPS: ${video.systemMetadata.fps}`);
  console.log(`  Width: ${video.systemMetadata.width}`);
  console.log(`  Height: ${video.systemMetadata.height}`);
  console.log(`  Size: ${video.systemMetadata.size}`);
  if (!video.userMetadata) {
    console.log("User metadata:");
      Object.entries(video.userMetadata).forEach(([key, value]) => {
          console.log(`${key}: ${value}`);
      });
  }
  if (video.hls) {
  console.log("HLS:");
  console.log(`  Video URL: ${video.hls.videoUrl || "N/A"}`);
  console.log("  Thumbnail URLs:");
  (video.hls.thumbnailUrls || []).forEach((url) => {
      console.log(`    ${url}`);
  });
  console.log(`  Status: ${video.hls.status || "N/A"}`);
  console.log(`  Updated At: ${video.hls.updatedAt}`);
  }
  if (video.source) {
  console.log("Source:");
  console.log(`  Type: ${video.source.type}`);
  console.log(`  Name: ${video.source.name}`);
  console.log(`  URL: ${video.source.url || "N/A"}`);
  }
  if (video.embedding) {
      console.log(`Engine name: ${video.embedding.engineName}`);
      console.log("Embeddings:");
      printSegments(video.embedding.videoEmbedding.segments);
  }
  ```


**Parameters**:

| Name              | Type                                                                                                                | Required | Description                                                                                                                                                                                                                |
| :---------------- | :------------------------------------------------------------------------------------------------------------------ | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `indexId`         | `string`                                                                                                            | Yes      | The unique identifier of the index to which the video has been uploaded.                                                                                                                                                   |
| `id`              | `string`                                                                                                            | Yes      | The unique identifier of the video to retrieve.                                                                                                                                                                            |
| `embeddingOption` | [`RetrieveVideoParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/video/interfaces.ts) | No       | Specifies which types of embeddings to retrieve. You can include one or more of the following values:
- `visual-text`: Returns visual embeddings optimized for text search.
- `audio`: Returns audio embeddings. |
| `options`         | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                            | No       | Additional options for the request. Defaults to `{}`.                                                                                                                                                                      |

**Return value**: Returns a `Promise` that resolves to a [`Models.Video`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/video/index.ts) instance.

**API Reference**: For a description of each field in the request and response, see the [Retrieve video information](/v1.3/api-reference/videos/retrieve) page.

## List videos with direct pagination

**Description**: This method returns a paginated list of the videos in the specified index based on the provided parameters. Choose this method mainly when the total number of items is manageable, or you must fetch a single page of results. By default, the platform returns your videos sorted by their upload date, with the newest at the top of the list.

**Function signature and example**:


  ```javascript Function signature
  async list(
    indexId: string,
    {
      id,
      size,
      width,
      height,
      duration,
      fps,
      createdAt,
      updatedAt,
      ...restParams
    }: ListVideoParams = {},
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const listParams = {
    id: "66ea61c40679760c1fc6a114",
    filename: "example_video.mp4",
    size: 1024,
    width: 1280,
    height: 720,  // Example: 720p to 1080p
    duration: 20,  // Example: 1 minute to 5 minutes
    fps: 24,
    userMetadata: {
        category: "nature"
    },
    createdAt: "2024-09-17T07:53:46.365Z",
    updatedAt: "2024-09-17T07:53:46.365Z",
    page: 1,
    pageLimit: 5,
    sortBy: "created_at",
    sortOption: "desc"
  };

  const videos = await client.index.video.list("66e9358a808d95368f6f7a7c", listParams)
  videos.forEach(video => {
  console.log(`ID: ${video.id}`);
  console.log(`  Created at: ${video.createdAt}`);
  console.log(`  Updated at: ${video.updatedAt || "N/A"}`);
  console.log("  System metadata:");
  console.log(`    Filename: ${video.systemMetadata.filename}`);
  console.log(`    Duration: ${video.systemMetadata.duration}`);
  console.log(`    FPS: ${video.systemMetadata.fps}`);
  console.log(`    Width: ${video.systemMetadata.width}`);
  console.log(`    Height: ${video.systemMetadata.height}`);
  console.log(`    Size: ${video.systemMetadata.size}`);
  if (!video.userMetadata) {
    console.log("User Metadata:");
      Object.entries(video.userMetadata).forEach(([key, value]) => {
          console.log(`${key}: ${value}`);
      });
  };
  if (video.hls) {
      console.log("  HLS:");
      console.log(`    Video URL: ${video.hls.videoUrl || "N/A"}`);
      console.log("    Thumbnail URLs:");
      (video.hls.thumbnailUrls || []).forEach((url) => {
      console.log(`      ${url}`);
      });
      console.log(`    Status: ${video.hls.status || "N/A"}`);
      console.log(`    Updated At: ${video.hls.updatedAt}`);
  }
  if (video.source) {
      console.log("  Source:");
      console.log(`    Type: ${video.source.type}`);
      console.log(`    Name: ${video.source.name}`);
      console.log(`    URL: ${video.source.url || "N/A"}`);
  }
  });
  ```


**Parameters**:

| Name      | Type                                                                                                            | Required | Description                                                                                                                                                 |
| :-------- | :-------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `indexId` | `string`                                                                                                        | Yes      | The unique identifier of the index to which the video has been uploaded.                                                                                    |
| `params`  | [`ListVideoParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/video/interfaces.ts) | No       | Parameters for retrieving the list of videos. Defaults to `{}`.  If you don't specify the `page` parameter, the platform returns the first page of results. |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                        | No       | Additional options for the request. Defaults to `{}`.                                                                                                       |

The `ListVideoParams` interface extends the `PageOptions` interface and defines the parameters for listing videos:

| Name           | Type                               | Required | Description                                                                                                       |
| -------------- | ---------------------------------- | -------- | ----------------------------------------------------------------------------------------------------------------- |
| `id`           | `string`                           | No       | Filter by the unique identifier of a video.                                                                       |
| `filename`     | `string`                           | No       | Filter by filename.                                                                                               |
| `size`         | `number \| Record` | No       | Filter by the size of the video file. This field can be a specific number or an object for range queries.         |
| `width`        | `number \| Record` | No       | Filter by the width of the video. This field can be a specific number or an object for range queries.             |
| `height`       | `number \| Record` | No       | Filter by the height of the video. This field can be a specific number or an object for range queries.            |
| `duration`     | `number \| Record` | No       | Filter by the duration of the video. This field can be a specific number or an object for range queries.          |
| `fps`          | `number \| Record` | No       | Filter by the frames per second of the video. This field can be a specific number or an object for range queries. |
| `userMetadata` | `Record`              | No       | Filter by metadata associated with the video. This field can contain any key-value pairs.                         |
| `createdAt`    | `string \| Record` | No       | Filter by the creation date of the video. This field can be a string or an object for range queries.              |
| `updatedAt`    | `string \| Record` | No       | Filter by the last update date of the video. This field can be a string or an object for range queries.           |

The following properties are inherited from the `PageOptions` interface:

| Name         | Type                           | Required | Description                                                                                       |
| ------------ | ------------------------------ | -------- | ------------------------------------------------------------------------------------------------- |
| `page`       | `number`                       | No       | The page number for pagination.                                                                   |
| `pageLimit`  | `number`                       | No       | The number of items per page.                                                                     |
| `sortBy`     | `'created_at' \| 'updated_at'` | No       | Specifies the field to sort by. This parameter must be either 'created\_at' or 'updated\_at'.     |
| `sortOption` | `'asc' \| 'desc'`              | No       | Specifies the sort order. This parameter must be either 'asc' (ascending) or 'desc' (descending). |

**Return value**: Returns a `Promise` that resolves to an array of `Models.Video` instances.

**API Reference**: For a description of each field in the request and response, see the [List videos](/v1.3/api-reference/videos/list) page.

## List videos with iterative pagination

**Description**: This method iterates through a paginated list of the videos in the specified index based on the provided parameters. Choose this method mainly when your application must retrieve a large number of items. By default, the API returns your videos sorted by creation date, with the newest at the top of the list.

**Function signature and example**:


  ```javascript Function signature
  async listPagination(
    indexId: string,
    {
      id,
      size,
      width,
      height,
      duration,
      fps,
      createdAt,
      updatedAt,
      ...restParams
    }: ListVideoParams = {},
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  function printPage(page) {
    page.forEach(video => {
      console.log(video)
      console.log(`ID: ${video.id}`);
      console.log(`  Created at: ${video.createdAt}`);
      console.log(`  Updated at: ${video.updatedAt || "N/A"}`);
      console.log("  System metadata:");
      console.log(`    Filename: ${video.systemMetadata.filename}`);
      console.log(`    Duration: ${video.systemMetadata.duration}`);
      console.log(`    FPS: ${video.systemMetadata.fps}`);
      console.log(`    Width: ${video.systemMetadata.width}`);
      console.log(`    Height: ${video.systemMetadata.height}`);
      console.log(`    Size: ${video.systemMetadata.size}`);
      if (!video.userMetadata) {
      console.log("User metadata:");
        Object.entries(video.userMetadata).forEach(([key, value]) => {
            console.log(`${key}: ${value}`);
        });
    }
      if (video.hls) {
        console.log("  HLS:");
        console.log(`    Video URL: ${video.hls.videoUrl || "N/A"}`);
        console.log("    Thumbnail URLs:");
        (video.hls.thumbnailUrls || []).forEach((url) => {
          console.log(`      ${url}`);
        });
        console.log(`    Status: ${video.hls.status || "N/A"}`);
        console.log(`    Updated At: ${video.hls.updatedAt}`);
      }
      if (video.source) {
        console.log("  Source:");
        console.log(`    Type: ${video.source.type}`);
        console.log(`    Name: ${video.source.name}`);
        console.log(`    URL: ${video.source.url || "N/A"}`);
      }
    });
  }

  const listParams = {
  id: "66ea61c40679760c1fc6a114",
  filename: "example_video.mp4",
  size: 1024,
  width: 1280,
  height: 720,  // Example: 720p to 1080p
  duration: 20,  // Example: 1 minute to 5 minutes
  fps: 24,
  userMetadata: {
      category: "nature"
  },
  createdAt: "2024-09-17T07:53:46.365Z",
  updatedAt: "2024-09-17T07:53:46.365Z",
  page: 1,
  pageLimit: 5,
  sortBy: "created_at",
  sortOption: "desc"
  };

  // Fetch the initial page of results
  const videoPaginator = await client.index.video.listPagination("66e9358a808d95368f6f7a7c", listParams);

  // Print the first page of results
  printPage(videoPaginator.data);

  // Iterate through subsequent pages
  while (true) {
  const nextPage = await videoPaginator.next();
  if (!nextPage) {
      break;
  }
  printPage(nextPage);
  }
  ```


**Parameters**:

| Name      | Type                                                                                                            | Required | Description                                                                                                                                                 |
| :-------- | :-------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `params`  | [`ListVideoParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/video/interfaces.ts) | No       | Parameters for retrieving the list of videos. Defaults to `{}`.  If you don't specify the `page` parameter, the platform returns the first page of results. |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                        | No       | Additional options for the request. Defaults to `{}`.                                                                                                       |

The `ListVideoParams` interface extends the `PageOptions` interface and defines the parameters for listing videos:

| Name           | Type                               | Required | Description                                                                                                       |
| -------------- | ---------------------------------- | -------- | ----------------------------------------------------------------------------------------------------------------- |
| `id`           | `string`                           | No       | Filter by the unique identifier of a video.                                                                       |
| `filename`     | `string`                           | No       | Filter by filename.                                                                                               |
| `size`         | `number \| Record` | No       | Filter by the size of the video file. This field can be a specific number or an object for range queries.         |
| `width`        | `number \| Record` | No       | Filter by the width of the video. This field can be a specific number or an object for range queries.             |
| `height`       | `number \| Record` | No       | Filter by the height of the video. This field can be a specific number or an object for range queries.            |
| `duration`     | `number \| Record` | No       | Filter by the duration of the video. This field can be a specific number or an object for range queries.          |
| `fps`          | `number \| Record` | No       | Filter by the frames per second of the video. This field can be a specific number or an object for range queries. |
| `userMetadata` | `Record`              | No       | Filter by metadata associated with the video. This field can contain any key-value pairs.                         |
| `createdAt`    | `string \| Record` | No       | Filter by the creation date of the video. This field can be a string or an object for range queries.              |
| `updatedAt`    | `string \| Record` | No       | Filter by the last update date of the video. This field can be a string or an object for range queries.           |

The following properties are inherited from the `PageOptions` interface:

| Name         | Type                           | Required | Description                                                                                       |
| ------------ | ------------------------------ | -------- | ------------------------------------------------------------------------------------------------- |
| `page`       | `number`                       | No       | The page number for pagination.                                                                   |
| `pageLimit`  | `number`                       | No       | The number of items per page.                                                                     |
| `sortBy`     | `'created_at' \| 'updated_at'` | No       | Specifies the field to sort by. This parameter must be either 'created\_at' or 'updated\_at'.     |
| `sortOption` | `'asc' \| 'desc'`              | No       | Specifies the sort order. This parameter must be either 'asc' (ascending) or 'desc' (descending). |

**Return value**: Returns a `Promise` that resolves to an array of [`VideoListWithPagination`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/video/index.ts) instances.


  To retrieve subsequent pages of results, use the async iterator protocol:

  1. Invoke the `next` method of the `VideoListWithPagination` object.
  2. Repeat this call until the method returns  a promise that resolves to`null`, indicating no more pages exist.


**API Reference**: For a description of each field in the request and response, see the [List videos](/v1.3/api-reference/videos/list) page.

## Update video information

**Description**: This method updates the title and the metadata of a video.

**Function signature and example**:


  ```javascript Function signature
  async update(
    indexId: string,
    id: string,
    { userMetadata }: UpdateVideoParams,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  await client.index.video.update("", "", { userMetadata: { from_sdk: true } });
  ```


**Parameters**:

| Name      | Type                                                                                                              | Required | Description                                                              |
| :-------- | :---------------------------------------------------------------------------------------------------------------- | :------- | :----------------------------------------------------------------------- |
| `indexId` | `string`                                                                                                          | Yes      | The unique identifier of the index to which the video has been uploaded. |
| `id`      | `string`                                                                                                          | Yes      | The unique identifier of the video.                                      |
| `params`  | [`UpdateVideoParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/video/interfaces.ts) | Yes      | Parameters for updating the video information.                           |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                          | No       | Additional options for the request. Defaults to `{}`.                    |

The `UpdateVideoParams` interface defines the parameters for updating a video's information:

| Name           | Type                  | Required | Description                                                                                                                                                                            |
| -------------- | --------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `userMetadata` | `Record` | No       | The metadata you want to update or add.. This parameter can contain any key-value pairs. Note that only the provided properties are modified; the omitted properties remain unchanged. |

**Return value**: Returns a `Promise` that resolves to `void`.

**API Reference**: For a description of each field in the request, see the [Update video information](/v1.3/api-reference/videos/update) page.

## Delete video information

**Description**: This method deletes all the information about the specified video. This action cannot be undone.

**Function signature and example**:


  ```javascript Function signature
  async delete(indexId: string, id: string, options: RequestOptions = {}): Promise 
  ```

  ```javascript Node.js example
  await client.index.video.delete("YOUR_INDEX_ID");
  ```


**Parameters**:

| Name      | Type                                                                                     | Required | Description                                                              |
| :-------- | :--------------------------------------------------------------------------------------- | :------- | :----------------------------------------------------------------------- |
| `indexId` | `string`                                                                                 | Yes      | The unique identifier of the index to which the video has been uploaded. |
| `id`      | `string`                                                                                 | Yes      | The unique identifier of the video to delete.                            |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.                    |

**Return value**: Returns a `Promise` that resolves to `void`.

**API Reference**: [Delete video information](/v1.3/api-reference/videos/delete).


# Search

The [`Resources.Search`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/search/index.ts) class provides methods to perform searches.

# Methods

## Make a search request

**Description**: This method performs a search across a specific index based on the provided parameters and returns the first page of results.

If you wish to use images as queries, ensure that your images meet the following requirements:

* **Format**: JPEG and PNG.
* **Dimension**: Must be at least 64 x 64 pixels.
* **Size**: Must not exceed 5MB.

**Function signature and example**:


  ```javascript Function signature
  async query(
    {
      indexId,
      query,
      queryText,
      queryMediaType,
      queryMediaFile,
      queryMediaUrl,
      options: searchOptions,
      groupBy,
      threshold,
      operator,
      conversationOption,
      filter,
      pageLimit,
      sortOption,
      adjustConfidenceLevel,
    }: SearchOptions,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  function printSearchData(data) {
    console.log(`  Score: ${data.score}`);
    console.log(`  Start: ${data.start}`);
    console.log(`  End: ${data.end}`);
    console.log(`  Video ID: ${data.videoId}`);
    console.log(`  Confidence: ${data.confidence}`);
    console.log(`  Thumbnail URL: ${data.thumbnailUrl}`);
  }

  const result = await client.search.query({
    indexId: "",
    queryText: "",
    options: ["visual", "audio"],
    groupBy: "clip",
    threshold: "medium",
    operator: "or",
    filter: {
       "metadata.language": "en",
    },
    sortOption: "score",
    adjustConfidenceLevel: 0.5,
    pageLimit: 5
  });

  // Print the search pool information
  console.log("Search pool:");
  console.log(`  Total count: ${result.pool.totalCount}`);
  console.log(`  Total duration: ${result.pool.totalDuration}`);
  console.log(`  Index ID: ${result.pool.indexId}`);

  // Print the search results
  console.log("Search Results:");
  result.data.forEach(item => {
    if ('clips' in item) {  // This is equivalent to isinstance(item, GroupByVideoSearchData)
      console.log(`Video ID: ${item.id}`);
      if (item.clips) {
        item.clips.forEach(clip => {
          printSearchData(clip);
        });
      }
    } else {
      printSearchData(item);
    }
  });

  // Print the page information
  console.log("Page information:");
  console.log(`  Limit per page: ${result.pageInfo.limitPerPage}`);
  console.log(`  Total results: ${result.pageInfo.totalResults}`);
  console.log(`  Page expired at: ${result.pageInfo.pageExpiresAt}`);
  console.log(`  Next page token: ${result.pageInfo.nextPageToken}`);
  console.log(`  Previous page token: ${result.pageInfo.prevPageToken}`);
  ```


**Parameters**:

| Name      | Type                                                                                                           | Required | Description                                           |
| :-------- | :------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `params`  | [`SearchOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/search/interfaces.ts) | Yes      | Parameters for performing the search.                 |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                       | No       | Additional options for the request. Defaults to `{}`. |

The `SearchOptions` interface defines the parameters for performing a search request:

| Name                    | Type                                        | Required | Description                                                                                                                                                    |
| ----------------------- | ------------------------------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `indexId`               | `string`                                    | Yes      | The unique identifier of the index to search.                                                                                                                  |
| `queryText`             | `string`                                    | No       | The text query to search for. This parameter is required for text queries.                                                                                     |
| `queryMediaType`        | `'image'`                                   | No       | The type of media you wish to use. This parameter is required for media queries. For example, to perform an image-based search, set this parameter to `image`. |
| `queryMediaFile`        | `Buffer \| NodeJS.ReadableStream \| string` | No       | The media file to be used as a query. This parameter can be a `Buffer`, a `ReadableStream`, or a string representing the path to the file.                     |
| `queryMediaUrl`         | `string`                                    | No       | The publicly accessible URL of a media file to use as a query. This parameter is required for media queries if `query_media_file` is not provided.             |
| `options`               | `('visual' \| 'audio')[]`                   | No       | Specifies the [sources of information](/v1.3/docs/concepts/search-options) the platform uses when performing a search                                          |
| `groupBy`               | `'video' \| 'clip'`                         | No       | Use this parameter to group or ungroup items in a response.                                                                                                    |
| `threshold`             | `'high' \| 'medium' \| 'low'`               | No       | Filter on the level of confidence that the results match your query.                                                                                           |
| `operator`              | `'or' \| 'and'`                             | No       | TLogical operator for combining search options.                                                                                                                |
| `filter`                | `Record`                       | No       | Additional filters for the search. This parameter can contain any key-value pairs.                                                                             |
| `pageLimit`             | `number`                                    | No       | The maximum number of results per page.                                                                                                                        |
| `sortOption`            | `'score' \| 'clip_count'`                   | No       | The sort order for the response.                                                                                                                               |
| `adjustConfidenceLevel` | `number`                                    | No       | The strictness of the thresholds for assigning the high, medium, or low confidence levels to search results.                                                   |

**Return value**: Returns a `Promise` that resolves to a [`Models.SearchResult`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/search/index.ts)  object containing the search results.

**API Reference**: For a description of each field in the request and response, see the [Any-to-video search](/v1.3/api-reference/any-to-video-search/make-search-request) page.

**Related guides**:

* [Search](/v1.3/docs/guides/search)
* [Pagination](/v1.3/docs/guides/search/pagination)
* [Sorting](/v1.3/docs/guides/search/sorting)
* [Filtering](/v1.3/docs/guides/search/filtering)
* [Grouping](/v1.3/docs/guides/search/grouping)

## Retrieve a specific page of search results

**Description**: This method retrieves a specific page of search results.


  This method provides direct pagination. Choose it mainly when the total number of items is manageable, or you must fetch a single page of results. When your application must retrieve a large number of items, choose iterative pagination. For details, see the [Iterative pagination](/v1.3/docs/guides/search/pagination#iterative-pagination)  section.


**Function signature and example**:


  ```javascript Function signature
  async byPageToken(pageToken: string, options: RequestOptions = {}): Promise
  ```

  ```javascript Node.js example
  function printSearchData(data) {
    console.log(`  Score: ${data.score}`);
    console.log(`  Start: ${data.start}`);
    console.log(`  End: ${data.end}`);
    console.log(`  Video ID: ${data.videoId}`);
    console.log(`  Confidence: ${data.confidence}`);
    console.log(`  Thumbnail URL: ${data.thumbnailUrl}`);
  }

  const result = await client.search..by_page_token("");

  // Print the search pool information
  console.log("Search pool:");
  console.log(`  Total count: ${result.pool.totalCount}`);
  console.log(`  Total duration: ${result.pool.totalDuration}`);
  console.log(`  Index ID: ${result.pool.indexId}`);

  // Print the search results
  console.log("Search Results:");
  result.data.forEach(item => {
    if ('clips' in item) { 
      console.log(`Video ID: ${item.id}`);
      if (item.clips) {
        item.clips.forEach(clip => {
          printSearchData(clip);
        });
      }
    } else {
      printSearchData(item);
    }
  });

  // Print the page information
  console.log("Page information:");
  console.log(`  Limit per page: ${result.pageInfo.limitPerPage}`);
  console.log(`  Total results: ${result.pageInfo.totalResults}`);
  console.log(`  Page expired at: ${result.pageInfo.pageExpiresAt}`);
  console.log(`  Next page token: ${result.pageInfo.nextPageToken}`);
  console.log(`  Previous page token: ${result.pageInfo.prevPageToken}`);
  ```


**Parameters**:

| Name        | Type                                                                                     | Required | Description                                           |
| :---------- | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `pageToken` | `string`                                                                                 | Yes      | A token that identifies the page to retrieve.         |
| `options`   | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`. |

**Return value**: Returns a `Promise` that resolves to a [`Models.SearchResult`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/search/index.ts)  object containing the search results.

**API Reference**: For a description of each field in the request and response, see the [Retrieve a specific page of search results](/v1.3/api-reference/any-to-video-search/retrieve-page) page.

**Related guides**:

* [Search](/v1.3/docs/guides/search)
* [Pagination](/v1.3/docs/guides/search/pagination)
* [Sorting](/v1.3/docs/guides/search/sorting)
* [Filtering](/v1.3/docs/guides/search/filtering)
* [Grouping](/v1.3/docs/guides/search/grouping)

# Iterative pagination

If your application must retrieve a large number of items, use iterative pagination. To retrieve the first page of results, invoke the `query` method of the `search` object. To retrieve subsequent pages of results, use the async iterator protocol.

```javascript Node.js
function printSearchData(data) {
  console.log(`  Score: ${data.score}`);
  console.log(`  Start: ${data.start}`);
  console.log(`  End: ${data.end}`);
  console.log(`  Video ID: ${data.videoId}`);
  console.log(`  Confidence: ${data.confidence}`);
  console.log(`  Thumbnail URL: ${data.thumbnailUrl}`);
}

function printPage(result, pageNumber) {
  console.log(`Page ${pageNumber}`);
  
  // Print the search results
  console.log("Search Results:");
  const data = result.data || result;
  data.forEach(item => {
    if ('clips' in item) {
      console.log(`Video ID: ${item.id}`);
      if (item.clips) {
        item.clips.forEach(clip => {
          printSearchData(clip);
        });
      }
    } else {
      printSearchData(item);
    }
  });
}

let searchResults = await client.search.query({
  indexId: "",
  queryText: "",
  options: ["visual", "audio"],
  groupBy: "clip",
  threshold: "medium",
  operator: "or",
  filter: {
     "metadata.language": "en",
  },
  sortOption: "score",
  adjustConfidenceLevel: 0.5,
  pageLimit: 5
});

// Print the search pool information
console.log("Search pool:");
console.log(`  Total count: ${searchResults.pool.totalCount}`);
console.log(`  Total duration: ${searchResults.pool.totalDuration}`);
console.log(`  Index ID: ${searchResults.pool.indexId}`);

let pageNumber = 1;
printPage(searchResults, pageNumber);

while (true) {
  const nextPage = await searchResults.next();
  if (nextPage === null) break;
  pageNumber++;
  printPage(nextPage, pageNumber);
}

console.log("No more results.");
```

# Error codes

This section lists the most common error messages you may encounter while performing search requests.

* `search_option_not_supported`
  * Search option `{search_option}` is not supported for index `{index_id}`. Please use one of the following search options: `{supported_search_option}`.
* `search_option_combination_not_supported`
  * Search option `{search_option}` is not supported with `{other_combination}`.
* `search_filter_invalid`
  * Filter used in search is invalid. Please use the valid filter syntax by following filtering documentation.
* `search_page_token_expired`
  * The token that identifies the page to be retrieved is expired or invalid. You must make a new search request. Token: `{next_page_token}`.
* `index_not_supported_for_search`:
  * You can only perform search requests on indexes with an engine from the Marengo family enabled.


# Create text, image, and audio embeddings

The [`Resources.Embed`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/embed/index.ts) class provides methods to create text, image, and audio embeddings.

# Create text, image, and audio embeddings

**Description**: This method creates a new embedding.

Note that you must specify at least the following parameters:

* `modelName`: The name of the video understanding model to use.
* One or more of the following input types:
  * `text`: For text embeddings
  * `audioUrl `or `audioFile`: For audio embeddings. If you specify both, the audioUrl parameter takes precedence.
  * `imageUrl` or `imageFile`: For image embeddings. If you specify both, the `imageUrl` parameter takes precedence.

You must provide at least one input type, but you can include multiple types in a single function call.

**Function signature and example**:


  ```javascript Function signature
  async create(
    { modelName, text, textTruncate, audioUrl, audioFile, imageUrl, imageFile }: CreateEmbedParams,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const printSegments = (segments: SegmentEmbedding[], maxElements = 5) => {
    segments.forEach((segment) => {
      console.log(
        `  embedding_scope=${segment.embeddingScope} start_offset_sec=${segment.startOffsetSec} end_offset_sec=${segment.endOffsetSec}`
      );
      console.log(
        "  embeddings: ",
        segment.embeddingsFloat.slice(0, maxElements)
      );
    });
  };

  cont res = await client.embed.create({
    modelName: "Marengo-retrieval-2.7",
    text: "",
  });

  console.log(`Created text embedding: modelName=${res.modelName}`);
  if (res.textEmbedding?.segments) {
    printSegments(res.textEmbedding.segments);
  }
  ```


**Parameters**:

| Name      | Type                                                                                                              | Required | Description                                           |
| :-------- | :---------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `params`  | [`CreateEmbedParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/embed/interfaces.ts) | Yes      | Parameters for creating the text embedding.           |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                          | No       | Additional options for the request. Defaults to `{}`. |

The `CreateEmbedParams` interface defines the parameters for creating a text embedding.

| Name           | Type                         | Required | Description                                                                         |
| -------------- | ---------------------------- | -------- | ----------------------------------------------------------------------------------- |
| `modelName`    | `string`                     | Yes      | The name of the video understanding model to use. Example: "Marengo-retrieval-2.7". |
| `text`         | `string`                     | Yes      | The text for which you want to create an embedding.                                 |
| `textTruncate` | `'none' \| 'start' \| 'end'` | No       | Specifies how to truncate the text if it exceeds the maximum length of 77 tokens.   |
| `audioUrl`     |                              |          |                                                                                     |

**Return value**: Returns a `Promise` that resolves to a [`Models.CreateEmbeddingsResult`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/embed/index.ts) instance.

**API Reference**: For a description of each field in the request and response, see the [Create text, image, and audio embeddings](/v1.3/api-reference/text-image-audio-embeddings/create-text-image-audio-embeddings) page.

**Related guide**:

* [Create text embeddings](/v1.3/docs/guides/create-embeddings/text)
* [Create audio embeddings](/v1.3/docs/guides/create-embeddings/audio)
* [Create image embeddings](/v1.3/docs/guides/create-embeddings/image)

# Error codes

This section lists the most common error messages you may encounter while creating text, image, and audio embeddings.

* `parameter_invalid`
  * The `text` parameter is invalid. The text token length should be less than or equal to 77.
  * The `text_truncate` parameter is invalid. You should use one of the following values: `none`, `start`, `end`.


# Create video embeddings

The [`Resources.EmbedTask`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/embed/index.ts) class provides methods to create embeddings for your videos.

To create video embeddings:

1. Create a video embedding task that uploads and processes a video.
2. Monitor the status of your task.
3. Retrieve the embeddings once the task is completed.

# Methods

## Create a video embedding task

**Description**: This method creates a new video embedding task.


  The videos you wish to upload must meet the following requirements:

  * **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
  * **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  * **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the [FFmpeg Formats Documentation](https://ffmpeg.org/ffmpeg-formats.html) page. For videos in other formats, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).
  * **Duration**: Must be between 4 seconds and 2 hours (7,200s).
  * **File size**: Must not exceed 2 GB.
    If you require different options, contact us at [support@twelvelabs.io](mailto:support@twelvelabs.io).


**Function signature and example**:


  ```javascript Function signature
  async create(
    modelName: string,
    { file, url, startOffsetSec, endOffsetSec, clipLength, scopes }: CreateEmbeddingsTaskVideoParams,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const params = {
    file: "",
    url: "",
    startOffsetSec: 5,
    endOffsetSec: 15,
    clipLength: 5,
    scopes: ["clip", "video"],
  };
  const task = await client.embed.task.create("Marengo-retrieval-2.7", params);

  console.log(`Task ID: ${task.id}`);
  console.log(`Model Name: ${task.modelName}`);
  console.log(`Status: ${task.status}`);
  ```


**Parameters**:

| Name        | Type                                                                                                                            | Required | Description                                                              |
| :---------- | :------------------------------------------------------------------------------------------------------------------------------ | :------- | :----------------------------------------------------------------------- |
| `modelName` | `string`                                                                                                                        | Yes      | The name of the model you want to use (example: "Marengo-retrieval-2.7". |
| `params`    | [`CreateEmbeddingsTaskVideoParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/embed/interfaces.ts) | Yes      | Parameters for creating the video embedding task.                        |
| `options`   | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                                        | No       | Additional options for the request. Defaults to `{}`.                    |

The `CreateEmbeddingsTaskVideoParams` interface defines the parameters for creating a video embedding task:

| Name             | Type                                        | Required | Description                                                                                                                                   |
| ---------------- | ------------------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| `file`           | `Buffer \| NodeJS.ReadableStream \| string` | No       | The video file you want to upload. This parameter can be a `Buffer`, a `ReadableStream`, or a string representing the path to the video file. |
| `url`            | `string`                                    | No       | The publicly accessible URL of the video file.                                                                                                |
| `startOffsetSec` | `number`                                    | No       | The start offset in seconds from the beginning of the video where processing should begin.                                                    |
| `endOffsetSec`   | `number`                                    | No       | The end offset in seconds from the beginning of the video where processing should stop.                                                       |
| `clipLength`     | `number`                                    | No       | The desired duration in seconds for each clip for which the platform generates an embedding.                                                  |
| `scopes`         | `Array<'clip' \| 'video'>`                  | No       | Specifies the embedding scope. Valid values are:

- `['clip']`
- `['clip', 'video']`                                           |

**Return value**: Returns a `Promise` that resolves to a [`Models.EmbeddingsTask`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/embed/index.ts) instance.

**API Reference**: For a description of each field in the request and response, see the [Create a video embedding task](/api-reference/video-embeddings/create-video-embedding-task) page.

**Related guide**: [Create video embeddings](/v1.3/docs/guides/create-embeddings/video).

## Retrieve the status of a video embedding task

**Description**: This method retrieves the status of a video embedding task.

**Function signature and example**:


  ```javascript Function signature
  async status(taskId: string, options: RequestOptions = {}): Promise
  ```

  ```javascript Node.js example
  const task = await client.embed.task.status("")

  console.log(`Task ID: ${task.id}`);
  console.log(`Model name: ${task.modelName}`);
  console.log(`Status: ${task.status}`);
  ```


**Parameters**:

| Name      | Type                                                                                     | Required | Description                                           |
| :-------- | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `taskId`  | `string`                                                                                 | Yes      | The unique identifier of a video embedding task.      |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`. |

**Return value**: Returns a `Promise` that resolves to a [`Models.EmbeddingsTaskStatus`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/embed/index.ts) instance.

**API Reference**: For a description of each field in the response, see the [Retrieve the status of a video embedding task](/v1.3/api-reference/video-embeddings/retrieve-video-embedding-task-status) page.

**Related guide**: [Create video embeddings](/v1.3/docs/guides/create-embeddings/video).

## Wait for a video embedding task to complete

**Description**: This method waits until a video embedding task is completed by periodically checking its status. If you provide a callback function, it calls the function after each status update with the current task object, allowing you to monitor progress.

**Function signature and example**:


  ```javascript Function signature
  async waitForDone(
    sleepInterval: number = 5000,
    callback?: (task: EmbeddingsTask) => void,
  ): Promise
  ```

  ```javascript Node.js example
  const status = await task.waitForDone(5000, (task) => { console.log(`  Status=${task.status}`);});
  console.log(`Embedding done: ${status}`);
  ```


**Parameters**

| Name            | Type                             | Required | Description                                                                                                              |
| :-------------- | :------------------------------- | :------- | :----------------------------------------------------------------------------------------------------------------------- |
| `sleepInterval` | `number`                         | No       | The time in milliseconds to wait between status checks. Must be greater than 0. Default is 5000.                         |
| `callback`      | `(task: EmbeddingsTask) => void` | No       | An optional function to call after each status check. It receives the current task object. Use this to monitor progress. |

**Return value**: A `Promise` that resolves to a string representing the final status of the task ("ready" or "failed").

## Retrieve video embeddings

**Description**: This method retrieves embeddings for a specific video embedding task. Ensure the task status is `ready` before retrieving your embeddings.

**Function signature and example**:


  ```javascript Function signature
  async retrieve(params: Resources.RetrieveEmbeddingsTaskParams = {}, options: RequestOptions = {}): Promise
  ```

  ```javascript Node.js example
  const printSegments = (segments: SegmentEmbedding[], maxElements = 5) => {
    segments.forEach((segment) => {
      console.log(
        `  embeddingScope=${segment.embeddingScope} embeddingOption=${segment.embeddingOption} start_offset_sec=${segment.startOffsetSec} end_offset_sec=${segment.endOffsetSec}`
      );
      console.log(
        "  embeddings: ",
        segment.embeddingsFloat.slice(0, maxElements)
      );
    });
  };

  let task = await client.embed.task.retrieve("");

  console.log(`Task ID: ${task.id}`);
  console.log(`Model name: ${task.modelName}`);
  console.log(`Status: ${task.status}`);


  task = await task.retrieve({ embeddingOption: ["visual-text", "audio"] });
  if (task.videoEmbedding) {
    if (task.videoEmbedding.segments) {
      printSegments(task.videoEmbedding.segments);
    }
  }
  ```


**Parameters**:

| Name      | Type                                                                                                                                   | Required | Description                                           |
| :-------- | :------------------------------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `params`  | [`Resources.RetrieveEmbeddingsTaskParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/embed/interfaces.ts) | Yes      | Parameters for retrieving the video embedding task.   |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                                               | No       | Additional options for the request. Defaults to `{}`. |

The [`Resources.RetrieveEmbeddingsTaskParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/embed/interfaces.ts) interface contains the following fields:

| Name              | Type                           | Required | Description                                                                                                                          |
| :---------------- | :----------------------------- | :------- | :----------------------------------------------------------------------------------------------------------------------------------- |
| `embeddingOption` | `('visual-text' \| 'audio')[]` | No       | An optional array specifying the types of embeddings to retrieve. Each element in the array must be either `visual-text` or `audio`. |

**Return value**: Returns a `Promise` that resolves to a [`EmbeddingsTask`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/embed/index.ts) instance.

**API Reference**: For a description of each field in the response, see the [Retrieve video embeddings](/v1.3/api-reference/video-embeddings/retrieve-video-embeddings) page.

**Related guide**: [Create video embeddings](/v1.3/docs/guides/create-embeddings/video).

# Error codes

This section lists the most common error messages you may encounter while creating video embeddings.

* `parameter_invalid`
  * The `video_clip_length` parameter is invalid. `video_clip_length` should be within 2-10 seconds long
  * The `video_end_offset_sec` parameter is invalid. `video_end_offset_sec` should be greater than `video_start_offset_sec`


# Analyze videos

The TwelveLabs Node.js SDK provides methods to analyze videos to generate text from their content.

# Titles, topics, and hashtags


  This method method has been flattened and is now called 

  `client.gist`

   instead of 

  `client.generate.gist`

  . The 

  `client.generate.gist`

   method will remain available until July 30, 2025; after this date, it will be deprecated. Update your code to use 

  `client.gist`

   to ensure uninterrupted service.


**Description**: This method analyzes a specific video and generates titles, topics, and hashtags based on its content. It uses predefined formats and doesn’t require a custom prompt, and it’s best for generating immediate and straightforward text representations without specific customization.

**Function signature and example**:


  ```javascript Function signature
  async gist(
    videoId: string,
    types: GenerateGistType[],
    options: RequestOptions = {},
  ): Promise 
  ```

  ```javascript Node.js example
  const result = await client.gist(
    "",
    ["title", "topic", "hashtag"]
  );

  console.log("Result ID:", result.id);

  if (result.title !== undefined) {
    console.log("Title:", result.title);
  }

  if (result.topics !== undefined) {
    console.log("Topics:");
    result.topics.forEach(topic => {
      console.log(`  - ${topic}`);
    });
  }

  if (result.hashtags !== undefined) {
    console.log("Hashtags:");
    result.hashtags.forEach(hashtag => {
      console.log(`  - ${hashtag}`);
    });
  }

  if (result.usage !== undefined) {
      console.log(`Output tokens: ${result.usage.outputTokens}`)
  };
  ```


**Parameters**:

| Name      | Type                                                                                                                  | Required | Description                                                                               |
| :-------- | :-------------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------------------------------------- |
| `videoId` | `string`                                                                                                              | Yes      | The unique identifier of the video for which you want to generate text.                   |
| `types`   | [`GenerateGistType[]`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/generate/interfaces.ts) | Yes      | The types of text you want to generate. Available values: `topics`, `titles`, `hashtags`. |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                              | No       | Additional options for the request. Defaults to `{}`.                                     |

**Return value**: Returns a `Promise` that resolves to a [`Models.GenerateGistResult`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/generate/index.ts) instance.

**API Reference**:  For a description of each field in the request and response, see the [Titles, topics, and hashtags](/v1.3/api-reference/analyze-videos/gist) page.

**Related guide**: [Titles, topics, and hashtags](/v1.3/docs/guides/analyze-videos/generate-titles-topics-and-hashtags).

# Summaries, chapters, and highlights


  This method method has been flattened and is now called 

  `client.summarize`

   instead of 

  `client.generate.summarize`

  . The 

  `client.generate.summarize`

   method will remain available until July 30, 2025; after this date, it will be deprecated. Update your code to use 

  `client.summarize`

   to ensure uninterrupted service.


**Description**: This method analyzes a video and generates summaries, chapters, or highlights based on its content. Optionally, you can provide a prompt to customize the output.

**Function signature and example**:


  ```javascript Function signature
  async summarize(
    videoId: string,
    type: GenerateSummarizeType,
    prompt?: string,
    temperature?: number,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const result = await client.summarize(
    "",
    "summary",
    "",
    0.7
    
  )

  console.log(`Result ID: ${result.id}`);

  if (result.summary !== undefined) {
    console.log(`Summary: ${result.summary}`);
  }

  if (result.chapters !== undefined && result.chapters.length > 0) {
    console.log("Chapters:");
    result.chapters.forEach(chapter => {
      console.log(`  Chapter ${chapter.chapterNumber}:`);
      console.log(`    Start: ${chapter.start}`);
      console.log(`    End: ${chapter.end}`);
      console.log(`    Title: ${chapter.chapterTitle}`);
      console.log(`    Summary: ${chapter.chapterSummary}`);
    });
  }

  if (result.highlights !== undefined && result.highlights.length > 0) {
    console.log("Highlights:");
    result.highlights.forEach(highlight => {
      console.log(`  Start: ${highlight.start}`);
      console.log(`  End: ${highlight.end}`);
      console.log(`  Highlight: ${highlight.highlight}`);
    });
  }

  if (result.usage !== undefined) {
      console.log(`Output tokens: ${result.usage.outputTokens}`)
  };
  ```


**Parameters**:

| Name          | Type                                                                                     | Required | Description                                                                                     |
| :------------ | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------------------------------------------- |
| `videoId`     | `string`                                                                                 | Yes      | The unique identifier of the video for which you want to generate text.                         |
| `type`        | `string`                                                                                 | Yes      | The type of text you want to generate. Available values: `summaries`, `chapters`, `highlights`. |
| `prompt`      | `string`                                                                                 | No       | The prompt to customize the output.                                                             |
| `temperature` | `number`                                                                                 | No       | The temperature to use in the generation.                                                       |
| `options`     | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.                                           |

**Return value**: Returns a `Promise` that resolves to a [`Models.GenerateSummarizeResult`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/generate/index.ts) instance.

**API Reference**: For a description of each field in the request and response, see the [Summaries, chapters, and highlights](/v1.3/api-reference/analyze-videos/summarize) page.

**Related guide**: [Summaries, chapters, and highlights](/v1.3/docs/guides/analyze-videos/generate-summaries-chapters-and-highlights).

# Open-ended analysis

**Description**: This method analyzes a video and generates text based on its content.

**Function signature and example**:


  ```javascript Function signature
  async analyze(
    videoId: string,
    prompt: string,
    temperature?: number,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const result = await client.analyze(
    "",
    "Be concise"
    0.7,
  );
  console.log(`Result ID: ${result.id}`);
  console.log(`Generated text: ${result.data}`);
  if (result.usage !== undefined) {
      console.log(`Output tokens: ${result.usage.outputTokens}`)
  };
  ```


**Parameters**:

| Name          | Type                                                                                     | Required | Description                                                                   |
| :------------ | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------------------------- |
| `videoId`     | `string`                                                                                 | Yes      | The unique identifier of the video you wish to analyze and generate text for. |
| `prompt`      | `string`                                                                                 | Yes      | The prompt to customize the output.                                           |
| `temperature` | `number`                                                                                 | No       | The temperature to use in the generation.                                     |
| `options`     | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.                         |

**Return value**: Returns a `Promise` that resolves to a [`Models.GenerateOpenEndedTextResult`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/generate/index.ts) instance.

**API Reference**: For a description of each field in the request and response, see the [Open-ended analysis](/v1.3/api-reference/analyze-videos/analyze) page.

**Related guide**: [Open-ended analysis](/v1.3/docs/guides/analyze-videos/open-ended-analysis).

# Open-ended analysis with streaming responses

**Description**: This method analyzes a video and generates text based on its content. It supports streaming responses.

**Function signature and example**:


  ```javascript Function signature
  async analyzStream(
    { videoId, prompt, temperature }: GenerateTextStreamParams,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
    const textStream = await client.analyzeStream({
      videoId: "",
      prompt: '',
      temperature: 0.7
    });

    for await (const text of textStream) {
      console.log(text);
    }

    console.log(`Aggregated text: ${textStream.aggregatedText}`);
  })();
  ```


**Parameters**:

| Name      | Type                                                                                                                        | Required | Description                                           |
| :-------- | :-------------------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `params`  | [`GenerateTextStreamParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/generate/interfaces.ts) | Yes      | Parameters for generating open-ended text.            |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                                    | No       | Additional options for the request. Defaults to `{}`. |

The `GenerateTextStreamParams` interface defines the parameters for streaming text generated based on video content:

| Name          | Type     | Required | Description                                                                   |
| ------------- | -------- | -------- | ----------------------------------------------------------------------------- |
| `videoId`     | `string` | Yes      | The unique identifier of the video you wish to analyze and generate text for. |
| `prompt`      | `string` | Yes      | The prompt to customize the output.                                           |
| `temperature` | `number` | No       | The temperature to use in the generation.                                     |

**API Reference**: For a description of each field in the request and response, see the [Open-ended analysis](/v1.3/api-reference/analyze-videos/analyze) page.

**Related guide**: [Open-ended analysis](/v1.3/docs/guides/analyze-videos/open-ended-analysis).

# Open-ended text


  This method will be deprecated on 

  **July 30, 2025**

  . Transition to the 

  [`analyze`](/v1.3/sdk-reference/node-js/analyze-videos#open-ended-analysis)

   method, which provides identical functionality. Ensure you've updated your function calls before the deprecation date to ensure uninterrupted service.


**Description**: This method generates open-ended texts based on your videos.

**Function signature and example**:


  ```javascript Function signature
  async text(
    videoId: string,
    prompt: string,
    temperature?: number,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
  const result = await client.generate.text(
    "",
    "Be concise"
    0.7,
  );
  console.log(`Result ID: ${result.id}`);
  console.log(`Generated text: ${result.data}`);
  if (result.usage !== undefined) {
      console.log(`Output tokens: ${result.usage.outputTokens}`)
  };
  ```


**Parameters**:

| Name          | Type                                                                                     | Required | Description                                                             |
| :------------ | :--------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------------------------- |
| `videoId`     | `string`                                                                                 | Yes      | The unique identifier of the video for which you want to generate text. |
| `prompt`      | `string`                                                                                 | Yes      | The prompt to customize the output.                                     |
| `temperature` | `number`                                                                                 | No       | The temperature to use in the generation.                               |
| `options`     | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts) | No       | Additional options for the request. Defaults to `{}`.                   |

**Return value**: Returns a `Promise` that resolves to a [`Models.GenerateOpenEndedTextResult`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/models/generate/index.ts) instance.

**API Reference**: For a description of each field in the request and response, see the [Open-ended text](/v1.3/api-reference/analyze-videos/open-ended) page.

# Open-ended text with streaming responses


  This method will be deprecated on 

  **July 30, 2025**

  . Transition to the 

  [`analyze`](/v1.3/sdk-reference/node-js/analyze-videos#open-ended-analysis-with-streaming-responses)

   method, which provides identical functionality. Ensure you've updated your function calls before the deprecation date to ensure uninterrupted service.


**Description**: This method generates open-ended texts and supports streaming responses.

**Function signature and example**:


  ```javascript Function signature
  async textStream(
    { videoId, prompt, temperature }: GenerateTextStreamParams,
    options: RequestOptions = {},
  ): Promise
  ```

  ```javascript Node.js example
    const textStream = await client.generate.textStream({
      videoId: "",
      prompt: "",
      temperature: 0.7
    });

    for await (const text of textStream) {
      console.log(text);
    }

    console.log(`Aggregated text: ${textStream.aggregatedText}`);
  })();
  ```


**Parameters**:

| Name      | Type                                                                                                                        | Required | Description                                           |
| :-------- | :-------------------------------------------------------------------------------------------------------------------------- | :------- | :---------------------------------------------------- |
| `params`  | [`GenerateTextStreamParams`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/resources/generate/interfaces.ts) | Yes      | Parameters for generating open-ended text.            |
| `options` | [`RequestOptions`](https://github.com/twelvelabs-io/twelvelabs-js/blob/main/src/core.ts)                                    | No       | Additional options for the request. Defaults to `{}`. |

The `GenerateTextStreamParams` interface defines the parameters for streaming text generated based on video content:

| Name          | Type     | Required | Description                                                             |
| ------------- | -------- | -------- | ----------------------------------------------------------------------- |
| `videoId`     | `string` | Yes      | The unique identifier of the video for which you want to generate text. |
| `prompt`      | `string` | Yes      | The prompt to customize the output.                                     |
| `temperature` | `number` | No       | The temperature to use in the generation.                               |

**API Reference**: For a description of each field in the request and response, see the [Open-ended text](/v1.3/api-reference/analyze-videos/open-ended) page.

# Error codes

This section lists the most common error messages you may encounter while analyzing videos.

* `token_limit_exceeded`
  * Your request could not be processed due to exceeding maximum token limit. Please try with another request or another video with shorter duration.
* `index_not_supported_for_generate`
  * You can only summarize videos uploaded to an index with an engine from the Pegasus family enabled.


# Introduction

Use the TwelveLabs Video Understanding API to extract information from your videos and make it available to your applications. The API is organized around REST and returns responses in JSON format. It is compatible with most programming languages, and you can use one of the [available SDKs](/v1.3/docs/resources/twelve-labs-sd-ks), Postman, or other REST clients to interact with the API.

# Call an endpoint

To call an endpoint, you must construct a URL similar to the following one:

`{Method} {BaseURL}/{version}/{resource}/{path_parameters}?{query_parameters}`

The list below describes each component of a request:

* **Method**: The API supports the following methods:
  * `GET`: Reads data.
  * `POST`: Creates a new object or performs an action.
  * `PUT`: Updates an object.
  * `DELETE`: Deletes an object.\
    Note that the `POST` and `PUT` methods require you to pass a request body containing additional parameters.
* **Base URL**: The base URL of the API is `https://api.twelvelabs.io`.
* **Version**: To use this version of the API, it must be set to `v1.3`.
* **Resource**: The name of the resource you want to interact with.
* **Path Parameters**: Allow you to indicate a specific object. For example, you can retrieve details about an engine or index.
* **Query Parameters**: Any parameters that an endpoint accepts. For example, you can filter] or sort a response using query parameters.

Note that the API requires you to pass a header parameter containing your API key to authenticate each request. For details, see the [Authentication](/api-reference/authentication) page.

# Responses

TwelveLabs Video Understanding API follows the RFC 9110 standard to indicate the success or failure of a request. Each response contains a field named `X-Api-Version` that indicates the version of the API against which the operation was performed.

## HTTP status codes

The following list is a summary of the HTTP status codes returned by the API:

* `200`: The request was successful.
* `201`: The request was successful and a new resource was created.
* `400`: The API service cannot process the request. See the `code` and `message` fields in the response for more details about the error.
* `401`: The API key you provided is not valid. Note that, for security reasons, your API key automatically expires every two months. When your key has expired, you must generate a new one to continue using the API.
* `404`: The requested resource was not found.
* `429`: Indicates that a rate limit has been reached.

## Errors

HTTP status codes in the `4xx` range indicate an error caused by the parameters you provided in the request. For each error, the API service returns the following fields in the body of the response:

* `code`: A string representing the error code.
* `message`: A human-readable string describing the error, intended to be suitable for display in a user interface.
* *(Optional)* `docs_url`: The URL of the relevant documentation page.

For more details, see the [Error codes](/v1.3/api-reference/error-codes) page.


# Authentication

To make HTTP requests, you must include the API key in the header of each request.

# Prerequisites

To use the platform, you need an API key:


  
    If you don’t have an account, [sign up](https://playground.twelvelabs.io/) for a free account.
  

  
    Go to the [API Key](https://playground.twelvelabs.io/dashboard/api-key) page.
  

  
    Select the **Copy** icon next to your key.
  


# Procedure


  
    Verify that the required packages are installed on your system. If necessary, install the following packages:

    
      
        Install the `requests` package by entering the following command:

        ```shell
        python -m pip install requests
        ```
      

      
        Install the `axios` and `form-data` packages by entering the following command:

        ```shell
        npm install axios form-data
        ```
      
    
  

  
    Define the URL of the API and the specific endpoint for your request.
  

  
    Create the necessary headers for authentication.
  

  
    Prepare the data payload for your API request.
  

  
    Send the API request and process the response.
  


Below are complete code examples for Python and Node.js, integrating all the steps outlined above:


  
    ```Python Python
    import requests

    # Step 2: Define the API URL and the specific endpoint
    API_URL = "https://api.twelvelabs.io/v1.3"
    INDEXES_URL = f"{API_URL}/indexes"

    # Step 3: Create the necessary headers for authentication
    headers = {
        "x-api-key": ""
    }

    # Step 4: Prepare the data payload for your API request
    INDEX_NAME = ""
    data = {
        "models": [
            {
                "model_name": "marengo2.7",
                "model_options": ["visual", "audio"]
            }
        ],
        "index_name": INDEX_NAME
    }

    # Step 5: Send the API request and process the response
    response = requests.post(INDEXES_URL, headers=headers, json=data)
    print(f"Status code: {response.status_code}")
    if response.status_code == 201:
        print(response.json())
    else:
        print("Error:", response.json())
    ```
  

  
    ```Javascript Node.js
    const axios = require('axios');

    // Step 2: Define the API URL and the specific endpoint
    const API_URL = 'https://api.twelvelabs.io/v1.3';
    const INDEXES_URL = `${API_URL}/indexes`;

    // Step 3: Create the necessary headers for authentication
    const headers = {
        'x-api-key': ''
    };

    // Step 4: Prepare the data payload for your API request
    const INDEX_NAME = '';
    const data = {
        models: [
            {
                model_name: 'marengo2.7',
                model_options: ['visual', 'audio']
            }
        ],
        index_name: INDEX_NAME
    };

    // Step 5: Send the API request and process the response
    axios.post(INDEXES_URL, data, { headers })
        .then(resp => {
            console.log(`Status code: ${resp.status}`);
            console.log(resp.data);
        })
        .catch(error => {
            console.error(`Error: ${error.response.status} - ${error.response.data.message}`);
        });
    ```
  



# Typical workflows

This page provides an overview of common workflows for interacting with the TwelveLabs Video Understanding Platform using an HTTP client. Each workflow consists of a series of steps, with links to detailed documentation for each step.

All workflows involving uploading video content to the platform require asynchronous processing. You must wait for the video processing to complete before proceeding with the subsequent steps.

# Authentication

The API uses keys for authentication. For details, see the [Authentication](/v1.3/api-reference/authentication) page.

# Search

Follow the steps in this section to search through your video content and find specific moments, scenes, or information.

**Steps**:

1. [Create an index](/v1.3/api-reference/indexes/create), enabling the Marengo video understanding model.
2. [Upload videos](/v1.3/api-reference/tasks/create) and [monitor the processing](/v1.3/api-reference/tasks/retrieve).
3. [Perform a search request](/v1.3/api-reference/any-to-video-search/make-search-request), using text or images as queries.


  * The search scope is an individual index.
  * Results support pagination, filtering, sorting, and grouping.


For an interactive implementation using the [Python SDK](https://github.com/twelvelabs-io/twelvelabs-python), see the [Quickstart Search](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Search.ipynb) quickstart notebook.

# Analyze videos

Follow the steps in this section to analyze videos and generate text based their content.

**Steps**:

1. [Create an index](/v1.3/api-reference/indexes/create), enabling the Pegasus video understanding model.
2. [Upload videos](/v1.3/api-reference/tasks/create) and [monitor the processing](/v1.3/api-reference/tasks/retrieve).
3. Depending on your use case, analyze videos and generate one of the following types of text:
   * [Titles, topics, and hashtags](/v1.3/api-reference/analyze-videos/gist)
   * [Summaries, chapters, and highlights](/v1.3/api-reference/analyze-videos/summarize)
   * [Open-ended analysis](/api-reference/analyze-videos/analyze).

For an interactive implementation using the [Python SDK](https://github.com/twelvelabs-io/twelvelabs-python), see the [Analyze](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Analyze.ipynb) quickstart notebook.

# Create text, image, and audio embeddings

This workflow guides you through creating embeddings for text.

**Steps**:

1. [Create text, image, and audio embeddings](/v1.3/api-reference/text-image-audio-embeddings).


  Creating text, image, and audio embeddings is a synchronous process.


# Create video embeddings

This workflow guides you through creating embeddings for videos.

**Steps**:

1. [Upload a video](/v1.3/api-reference/video-embeddings/create-video-embedding-task) and [monitor the processing](/v1.3/api-reference/video-embeddings/retrieve-video-embedding-task-status).
2. [Retrieve the embeddings](/v1.3/api-reference/video-embeddings/retrieve-video-embeddings).


  Creating video embeddings is a synchronous process.


For an interactive implementation using the [Python SDK](https://github.com/twelvelabs-io/twelvelabs-python), see the [Quickstart Embed](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Embeddings.ipynb) quickstart notebook.


# Manage indexes

An index is a basic unit for organizing and storing video data consisting of ideo embeddings and metadata. Indexes facilitate information retrieval and processing. You can use this endpoint to manage your indexes.

**Related guide**: [Create an index](/v1.3/docs/concepts/indexes#create-an-index).


# The index object

The `index` object is composed of the following fields:

* `_id`: A string representing the unique identifier of the index. It is assigned by the API when an index is created.
* `index_name`: A string representing the name of the index.
* `models`: An array that specifies the [video understanding models](/v1.3/docs/concepts/models) and the [model options](/v1.3/docs/concepts/modalities#model-options) that are enabled for this index. This determines how the platform processes your videos.
* `created_at`: A string representing the date and time, in the RFC 3339 format, that the index was created.
* `updated_at`: A string representing the date and time, in the RFC 3339 format, that the index was updated.
* `expires_at`: A string representing the date and time, in the RFC 3339 format, when your index will expire.
* `video_count`: An integer representing the number of videos in the index.
* `total_duration`: An integer representing the total duration of the videos in the index.
* `addons`: The list of add-ons that are enabled for this index.


# Create an index

```http
POST https://api.twelvelabs.io/v1.3/indexes
Content-Type: application/json
```

This method creates an index.




## Response Body

- 201: An index has successfully been created
- 400: The request has failed.

## Examples

```shell
curl -X POST https://api.twelvelabs.io/v1.3/indexes \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "index_name": "myIndex",
  "models": [
    {
      "model_name": "marengo2.7",
      "model_options": [
        "visual",
        "audio"
      ]
    },
    {
      "model_name": "pegasus1.2",
      "model_options": [
        "visual",
        "audio"
      ]
    }
  ],
  "addons": [
    "thumbnail"
  ]
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes"

payload = {
    "index_name": "myIndex",
    "models": [
        {
            "model_name": "marengo2.7",
            "model_options": ["visual", "audio"]
        },
        {
            "model_name": "pegasus1.2",
            "model_options": ["visual", "audio"]
        }
    ],
    "addons": ["thumbnail"]
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"index_name":"myIndex","models":[{"model_name":"marengo2.7","model_options":["visual","audio"]},{"model_name":"pegasus1.2","model_options":["visual","audio"]}],"addons":["thumbnail"]}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes"

	payload := strings.NewReader("{\n  \"index_name\": \"myIndex\",\n  \"models\": [\n    {\n      \"model_name\": \"marengo2.7\",\n      \"model_options\": [\n        \"visual\",\n        \"audio\"\n      ]\n    },\n    {\n      \"model_name\": \"pegasus1.2\",\n      \"model_options\": [\n        \"visual\",\n        \"audio\"\n      ]\n    }\n  ],\n  \"addons\": [\n    \"thumbnail\"\n  ]\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"index_name\": \"myIndex\",\n  \"models\": [\n    {\n      \"model_name\": \"marengo2.7\",\n      \"model_options\": [\n        \"visual\",\n        \"audio\"\n      ]\n    },\n    {\n      \"model_name\": \"pegasus1.2\",\n      \"model_options\": [\n        \"visual\",\n        \"audio\"\n      ]\n    }\n  ],\n  \"addons\": [\n    \"thumbnail\"\n  ]\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/indexes")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"index_name\": \"myIndex\",\n  \"models\": [\n    {\n      \"model_name\": \"marengo2.7\",\n      \"model_options\": [\n        \"visual\",\n        \"audio\"\n      ]\n    },\n    {\n      \"model_name\": \"pegasus1.2\",\n      \"model_options\": [\n        \"visual\",\n        \"audio\"\n      ]\n    }\n  ],\n  \"addons\": [\n    \"thumbnail\"\n  ]\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/indexes', [
  'body' => '{
  "index_name": "myIndex",
  "models": [
    {
      "model_name": "marengo2.7",
      "model_options": [
        "visual",
        "audio"
      ]
    },
    {
      "model_name": "pegasus1.2",
      "model_options": [
        "visual",
        "audio"
      ]
    }
  ],
  "addons": [
    "thumbnail"
  ]
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"index_name\": \"myIndex\",\n  \"models\": [\n    {\n      \"model_name\": \"marengo2.7\",\n      \"model_options\": [\n        \"visual\",\n        \"audio\"\n      ]\n    },\n    {\n      \"model_name\": \"pegasus1.2\",\n      \"model_options\": [\n        \"visual\",\n        \"audio\"\n      ]\n    }\n  ],\n  \"addons\": [\n    \"thumbnail\"\n  ]\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "index_name": "myIndex",
  "models": [
    [
      "model_name": "marengo2.7",
      "model_options": ["visual", "audio"]
    ],
    [
      "model_name": "pegasus1.2",
      "model_options": ["visual", "audio"]
    ]
  ],
  "addons": ["thumbnail"]
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/indexes \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "index_name": "string",
  "models": [
    {
      "model_name": "string",
      "model_options": [
        "string"
      ]
    }
  ]
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes"

payload = {
    "index_name": "string",
    "models": [
        {
            "model_name": "string",
            "model_options": ["string"]
        }
    ]
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"index_name":"string","models":[{"model_name":"string","model_options":["string"]}]}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes"

	payload := strings.NewReader("{\n  \"index_name\": \"string\",\n  \"models\": [\n    {\n      \"model_name\": \"string\",\n      \"model_options\": [\n        \"string\"\n      ]\n    }\n  ]\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"index_name\": \"string\",\n  \"models\": [\n    {\n      \"model_name\": \"string\",\n      \"model_options\": [\n        \"string\"\n      ]\n    }\n  ]\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/indexes")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"index_name\": \"string\",\n  \"models\": [\n    {\n      \"model_name\": \"string\",\n      \"model_options\": [\n        \"string\"\n      ]\n    }\n  ]\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/indexes', [
  'body' => '{
  "index_name": "string",
  "models": [
    {
      "model_name": "string",
      "model_options": [
        "string"
      ]
    }
  ]
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"index_name\": \"string\",\n  \"models\": [\n    {\n      \"model_name\": \"string\",\n      \"model_options\": [\n        \"string\"\n      ]\n    }\n  ]\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "index_name": "string",
  "models": [
    [
      "model_name": "string",
      "model_options": ["string"]
    ]
  ]
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# List indexes

```http
GET https://api.twelvelabs.io/v1.3/indexes
```

This method returns a list of the indexes in your account. The API returns indexes sorted by creation date, with the oldest indexes at the top of the list.




## Query Parameters

- page (optional): A number that identifies the page to retrieve.

**Default**: `1`.

- page_limit (optional): The number of items to return on each page.

**Default**: `10`.
**Max**: `50`.

- sort_by (optional): The field to sort on. The following options are available:
- `updated_at`: Sorts by the time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"), when the item was updated.
- `created_at`: Sorts by the time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"), when the item was created.

**Default**: `created_at`.

- sort_option (optional): The sorting direction. The following options are available:
- `asc`
- `desc`

**Default**: `desc`.

- index_name (optional): Filter by the name of an index.
- model_options (optional): Filter by the model options. When filtering by multiple model options, the values must be comma-separated.

- model_family (optional): Filter by the model family. This parameter can take one of the following values: `marengo` or `pegasus`. You can specify a single value.

- created_at (optional): Filter indexes by the creation date and time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"). The platform returns the indexes that were created on the specified date at or after the given time.

- updated_at (optional): Filter indexes by the last update date and time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"). The platform returns the indexes that were last updated on the specified date at or after the given time.


## Response Body

- 200: The indexes have successfully been retrieved.
- 400: The request has failed.

## Examples

```shell
curl -G https://api.twelvelabs.io/v1.3/indexes \
     -H "x-api-key: " \
     -d index_name=myIndex \
     --data-urlencode model_options=visual,audio \
     -d model_family=marengo \
     --data-urlencode created_at=2024-08-16T16:53:59Z \
     --data-urlencode updated_at=2024-08-16T16:55:59Z
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes"

querystring = {"index_name":"myIndex","model_options":"visual,audio","model_family":"marengo","created_at":"2024-08-16T16:53:59Z","updated_at":"2024-08-16T16:55:59Z"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes?index_name=myIndex&model_options=visual%2Caudio&model_family=marengo&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A55%3A59Z';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes?index_name=myIndex&model_options=visual%2Caudio&model_family=marengo&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A55%3A59Z"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes?index_name=myIndex&model_options=visual%2Caudio&model_family=marengo&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A55%3A59Z")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/indexes?index_name=myIndex&model_options=visual%2Caudio&model_family=marengo&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A55%3A59Z")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/indexes?index_name=myIndex&model_options=visual%2Caudio&model_family=marengo&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A55%3A59Z', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes?index_name=myIndex&model_options=visual%2Caudio&model_family=marengo&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A55%3A59Z");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes?index_name=myIndex&model_options=visual%2Caudio&model_family=marengo&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A55%3A59Z")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -G https://api.twelvelabs.io/v1.3/indexes \
     -H "x-api-key: " \
     -d page=0 \
     -d page_limit=0
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes"

querystring = {"page":"0","page_limit":"0"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes?page=0&page_limit=0';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes?page=0&page_limit=0"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes?page=0&page_limit=0")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/indexes?page=0&page_limit=0")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/indexes?page=0&page_limit=0', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes?page=0&page_limit=0");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes?page=0&page_limit=0")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Retrieve an index

```http
GET https://api.twelvelabs.io/v1.3/indexes/{index-id}
```

This method retrieves details about the specified index.




## Path Parameters

- index-id (required): Unique identifier of the index to retrieve.


## Response Body

- 200: The specified index has successfully been retrieved.
- 400: The request has failed.

## Examples

```shell
curl https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl https://api.twelvelabs.io/v1.3/indexes/:index-id \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Update an index

```http
PUT https://api.twelvelabs.io/v1.3/indexes/{index-id}
Content-Type: application/json
```

This method updates the name of the specified index.




## Path Parameters

- index-id (required): Unique identifier of the index to update.


## Response Body


- 400: The request has failed.

## Examples

```shell
curl -X PUT https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "index_name": "myIndex"
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c"

payload = { "index_name": "myIndex" }
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.put(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c';
const options = {
  method: 'PUT',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"index_name":"myIndex"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c"

	payload := strings.NewReader("{\n  \"index_name\": \"myIndex\"\n}")

	req, _ := http.NewRequest("PUT", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Put.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"index_name\": \"myIndex\"\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.put("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"index_name\": \"myIndex\"\n}")
  .asString();
```

```php
request('PUT', 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c', [
  'body' => '{
  "index_name": "myIndex"
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c");
var request = new RestRequest(Method.PUT);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"index_name\": \"myIndex\"\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = ["index_name": "myIndex"] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "PUT"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X PUT https://api.twelvelabs.io/v1.3/indexes/:index-id \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "index_name": "string"
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id"

payload = { "index_name": "string" }
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.put(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id';
const options = {
  method: 'PUT',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"index_name":"string"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id"

	payload := strings.NewReader("{\n  \"index_name\": \"string\"\n}")

	req, _ := http.NewRequest("PUT", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Put.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"index_name\": \"string\"\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.put("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"index_name\": \"string\"\n}")
  .asString();
```

```php
request('PUT', 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id', [
  'body' => '{
  "index_name": "string"
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id");
var request = new RestRequest(Method.PUT);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"index_name\": \"string\"\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = ["index_name": "string"] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "PUT"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Delete an index

```http
DELETE https://api.twelvelabs.io/v1.3/indexes/{index-id}
```

This method deletes the specified index and all the videos within it. This action cannot be undone.



## Path Parameters

- index-id (required): Unique identifier of the index to delete.


## Response Body


- 400: The request has failed.

## Examples

```shell
curl -X DELETE https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c"

headers = {"x-api-key": ""}

response = requests.delete(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c';
const options = {method: 'DELETE', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c"

	req, _ := http.NewRequest("DELETE", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Delete.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.delete("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .asString();
```

```php
request('DELETE', 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c");
var request = new RestRequest(Method.DELETE);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "DELETE"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X DELETE https://api.twelvelabs.io/v1.3/indexes/:index-id \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id"

headers = {"x-api-key": ""}

response = requests.delete(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id';
const options = {method: 'DELETE', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id"

	req, _ := http.NewRequest("DELETE", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Delete.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.delete("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id")
  .header("x-api-key", "")
  .asString();
```

```php
request('DELETE', 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id");
var request = new RestRequest(Method.DELETE);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "DELETE"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Upload videos

A video indexing task represents a request to upload and index a video, and you can use this endpoint to manage your tasks.


  When using this endpoint, you can set up webhooks to receive notifications. For details, see the [Webhooks](/v1.3/docs/advanced/webhooks/manage) section.



# The task object

A task represents a request to upload and index a video. The `task` object is composed of the following fields:

* `_id`: A string representing the unique identifier of the task. It is assigned by the API service when a new task is created.
* `created_at`: A string indicating the date and time, in the RFC 3339 format, that the task was created.
* `estimated_time`: A string indicating the estimated date and time, in the RFC 3339 format, that the video is ready to be searched.
* `index_id`: A string representing the index to which the video has been uploaded.
* `video_id`: A string representing the unique identifier of the video associated with this video indexing task.
* `system_metadata`: An object that contains the system-generated metadata about the video
* `status`: A string indicating the status of the video indexing task. It can take one of the following values:
  * **Validating**: Your video has finished uploading, and the API service is validating it by ensuring it meets the requirements listed on the [Create a video indexing task](/v1.3/api-reference/tasks/create) page.
  * **Pending**: The platform is spawning a new worker server to process your video.
  * **Queued**: A worker server has been assigned, and the platform is preparing to begin indexing.
  * **Indexing**: The platform transforms the video you uploaded into embeddings. An embedding is a compressed version of the video and contains all the information that TwelveLabs' deep-learning models need to perform downstream tasks.
  * **Ready**: The platform has finished indexing your video.
  * **Failed**: If an error occurs, the status is set to `Failed`.
* `hls`: The platform returns this object only for the videos that you uploaded with the `enable_video_stream` parameter set to `true`. This object has the following fields:
  * `video_url`: A string representing the URL of the video. You can use this URL to access the stream over the HLS protocol.
  * `thumbnail_urls`: An array containing the URL of the thumbnail.
  * `status`: A string representing the encoding status of the video file from its original format to a streamable format.
  * `updated_at`: A string indicating the date and time, in the RFC 3339 format, that the encoding status was last updated.
* `updated_at`: A string indicating the date and time, in the RFC 3339 format, that the task object was last updated. The API service updates this field every time the video indexing task transitions to a different state.


# Create a video indexing task

```http
POST https://api.twelvelabs.io/v1.3/tasks
Content-Type: multipart/form-data
```

This method creates a video indexing task that uploads and indexes a video.

Upload options:
- **Local file**: Use the `video_file` parameter.
- **Publicly accessible URL**: Use the `video_url` parameter.


  The videos you wish to upload must meet the following requirements:
  - **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
  - **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  - **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the [FFmpeg Formats Documentation](https://ffmpeg.org/ffmpeg-formats.html) page. For videos in other formats, contact us at support@twelvelabs.io.
  - **Duration**: For Marengo, it must be between 4 seconds and 2 hours (7,200s). For Pegasus, it must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration for Pegasus will be 2 hours (7,200 seconds).
  - **File size**: Must not exceed 2 GB.
    If you require different options, contact us at support@twelvelabs.io.
  
  If both Marengo and Pegasus are enabled for your index, the most restrictive prerequisites will apply.



- The platform supports video URLs that can play without additional user interaction or custom video players. Ensure your URL points to the raw video file, not a web page containing the video. Links to third-party hosting sites, cloud storage services, or videos requiring extra steps to play are not supported.
- This endpoint is rate-limited. For details, see the [Rate limits](/v1.3/docs/get-started/rate-limits) page.





## Response Body

- 200: A video indexing task has successfully been created.
- 400: The request has failed.

## Examples

```shell
curl -X POST https://api.twelvelabs.io/v1.3/tasks \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F index_id="6298d673f1090f1100476d4c" \
     -F video_file=@@/Users/john/Documents/01.mp4
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks"

files = { "video_file": "open('@/Users/john/Documents/01.mp4', 'rb')" }
payload = {
    "index_id": "6298d673f1090f1100476d4c",
    "video_url": ,
    "enable_video_stream": ,
    "user_metadata": 
}
headers = {"x-api-key": ""}

response = requests.post(url, data=payload, files=files, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks';
const form = new FormData();
form.append('index_id', '6298d673f1090f1100476d4c');
form.append('video_file', '@/Users/john/Documents/01.mp4');
form.append('video_url', '');
form.append('enable_video_stream', '');
form.append('user_metadata', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\n6298d673f1090f1100476d4c\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_file\"; filename=\"01.mp4\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"enable_video_stream\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\n6298d673f1090f1100476d4c\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_file\"; filename=\"01.mp4\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"enable_video_stream\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/tasks")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\n6298d673f1090f1100476d4c\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_file\"; filename=\"01.mp4\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"enable_video_stream\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/tasks', [
  'multipart' => [
    [
        'name' => 'index_id',
        'contents' => '6298d673f1090f1100476d4c'
    ],
    [
        'name' => 'video_file',
        'filename' => '@/Users/john/Documents/01.mp4',
        'contents' => null
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\n6298d673f1090f1100476d4c\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_file\"; filename=\"01.mp4\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"enable_video_stream\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "index_id",
    "value": "6298d673f1090f1100476d4c"
  ],
  [
    "name": "video_file",
    "fileName": "@/Users/john/Documents/01.mp4"
  ],
  [
    "name": "video_url",
    "value": 
  ],
  [
    "name": "enable_video_stream",
    "value": 
  ],
  [
    "name": "user_metadata",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/tasks \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F index_id="string"
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks"

payload = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"enable_video_stream\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"
headers = {
    "x-api-key": "",
    "Content-Type": "multipart/form-data; boundary=---011000010111000001101001"
}

response = requests.post(url, data=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks';
const form = new FormData();
form.append('index_id', 'string');
form.append('video_url', '');
form.append('enable_video_stream', '');
form.append('user_metadata', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"enable_video_stream\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"enable_video_stream\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/tasks")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"enable_video_stream\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/tasks', [
  'multipart' => [
    [
        'name' => 'index_id',
        'contents' => 'string'
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"enable_video_stream\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "index_id",
    "value": "string"
  ],
  [
    "name": "video_url",
    "value": 
  ],
  [
    "name": "enable_video_stream",
    "value": 
  ],
  [
    "name": "user_metadata",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# List video indexing tasks

```http
GET https://api.twelvelabs.io/v1.3/tasks
```

This method returns a list of the video indexing tasks in your account. The API returns your video indexing tasks sorted by creation date, with the newest at the top of the list.



## Query Parameters

- page (optional): A number that identifies the page to retrieve.

**Default**: `1`.

- page_limit (optional): The number of items to return on each page.

**Default**: `10`.
**Max**: `50`.

- sort_by (optional): The field to sort on. The following options are available:
- `updated_at`: Sorts by the time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"), when the item was updated.
- `created_at`: Sorts by the time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"), when the item was created.

**Default**: `created_at`.

- sort_option (optional): The sorting direction. The following options are available:
- `asc`
- `desc`

**Default**: `desc`.

- index_id (optional): Filter by the unique identifier of an index.

- status (optional): Filter by one or more video indexing task statuses. The following options are available:
- `ready`: The video has been successfully uploaded and indexed.
- `uploading`: The video is being uploaded.
- `validating`: The video is being validated against the prerequisites.
- `pending`: The video is pending.
- `queued`: The video is queued.
- `indexing`: The video is being indexed.
- `failed`: The video indexing task failed.

To filter by multiple statuses, specify the `status` parameter for each value:
```
status=ready&status=validating
```

- filename (optional): Filter by filename.

- duration (optional): Filter by duration. Expressed in seconds.

- width (optional): Filter by width.

- height (optional): Filter by height.

- created_at (optional): Filter video indexing tasks by the creation date and time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"). The platform returns the video indexing tasks that were created on the specified date at or after the given time.

- updated_at (optional): Filter video indexing tasks by the last update date and time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"). The platform returns the video indexing tasks that were updated on the specified date at or after the given time.


## Response Body

- 200: The video indexing tasks have successfully been retrieved.
- 400: The request has failed.

## Examples

```shell
curl https://api.twelvelabs.io/v1.3/tasks \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/tasks")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/tasks', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -G https://api.twelvelabs.io/v1.3/tasks \
     -H "x-api-key: " \
     -d page=0 \
     -d page_limit=0
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks"

querystring = {"page":"0","page_limit":"0"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks?page=0&page_limit=0';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks?page=0&page_limit=0"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks?page=0&page_limit=0")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/tasks?page=0&page_limit=0")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/tasks?page=0&page_limit=0', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks?page=0&page_limit=0");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks?page=0&page_limit=0")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Retrieve a video indexing task

```http
GET https://api.twelvelabs.io/v1.3/tasks/{task_id}
```

This method retrieves a video indexing task.



## Path Parameters

- task_id (required): The unique identifier of the video indexing task to retrieve.


## Response Body

- 200: The specified video indexing task has successfully been retrieved.
- 400: The request has failed.

## Examples

```shell
curl https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl https://api.twelvelabs.io/v1.3/tasks/:task_id \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/%3Atask_id"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/%3Atask_id';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/%3Atask_id"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/%3Atask_id")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/tasks/%3Atask_id")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/tasks/%3Atask_id', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/%3Atask_id");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/%3Atask_id")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Delete a video indexing task

```http
DELETE https://api.twelvelabs.io/v1.3/tasks/{task_id}
```

This action cannot be undone.
Note the following about deleting a video indexing task:
- You can only delete video indexing tasks for which the status is `ready` or `failed`.
- If the status of your video indexing task is `ready`, you must first delete the video vector associated with your video indexing task by calling the [`DELETE`](/v1.3/api-reference/videos/delete) method of the `/indexes/videos` endpoint.




## Path Parameters

- task_id (required): The unique identifier of the video indexing task you want to delete.


## Response Body


- 400: The request has failed.

## Examples

```shell
curl -X DELETE https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c"

headers = {"x-api-key": ""}

response = requests.delete(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c';
const options = {method: 'DELETE', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c"

	req, _ := http.NewRequest("DELETE", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Delete.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.delete("https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .asString();
```

```php
request('DELETE', 'https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c");
var request = new RestRequest(Method.DELETE);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "DELETE"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X DELETE https://api.twelvelabs.io/v1.3/tasks/:task_id \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/%3Atask_id"

headers = {"x-api-key": ""}

response = requests.delete(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/%3Atask_id';
const options = {method: 'DELETE', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/%3Atask_id"

	req, _ := http.NewRequest("DELETE", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/%3Atask_id")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Delete.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.delete("https://api.twelvelabs.io/v1.3/tasks/%3Atask_id")
  .header("x-api-key", "")
  .asString();
```

```php
request('DELETE', 'https://api.twelvelabs.io/v1.3/tasks/%3Atask_id', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/%3Atask_id");
var request = new RestRequest(Method.DELETE);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/%3Atask_id")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "DELETE"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Import videos

```http
POST https://api.twelvelabs.io/v1.3/tasks/transfers/import/{integration-id}
Content-Type: application/json
```

An import represents the process of uploading and indexing all videos from the specified integration.

This method initiates an asynchronous import and returns two lists:
- Videos that will be imported.
- Videos that will not be imported, typically because they do not meet the prerequisites of all enabled video understanding models for your index. Note that the most restrictive prerequisites among the enabled models will apply.

The actual uploading and indexing of videos occur asynchronously after you invoke this method. To monitor the status of each upload after invoking this method, use the [Retrieve import status](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/get-status) method.


  The videos you wish to upload must meet the following requirements:
  - **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
  - **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  - **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the [FFmpeg Formats Documentation](https://ffmpeg.org/ffmpeg-formats.html) page. For videos in other formats, contact us at support@twelvelabs.io.
  - **Duration**: For Marengo, it must be between 4 seconds and 2 hours (7,200s). For Pegasus, it must be between 4 seconds and 60 minutes (3600s). In a future release, the maximum duration for Pegasus will be 2 hours (7,200 seconds).
  - **File size**: Must not exceed 2 GB.
    If you require different options, contact us at support@twelvelabs.io.

  If both Marengo and Pegasus are enabled for your index, the most restrictive prerequisites will apply.



- Before importing videos, you must set up an integration. For details, see the [Set up an integration](/v1.3/docs/advanced/cloud-to-cloud-integrations#set-up-an-integration) section.
- By default, the platform checks for duplicate files using hashes within the target index and will not upload the same video to the same index twice. However, the same video can exist in multiple indexes. To bypass duplicate checking entirely and import duplicate videos into the same index, set the value of the `incremental_import` parameter to `false`.
- Only one import job can run at a time. To start a new import, wait for the current job to complete. Use the [`GET`](/v1.3/api-reference/tasks/cloud-to-cloud-integrations/get-status) method of the `/tasks/transfers/import/{integration-id}/logs` endpoint to retrieve a list of your import jobs, including their creation time, completion time, and processing status for each video file.





## Path Parameters

- integration-id (required): The unique identifier of the integration for which you want to import videos. You can retrieve it from the [Integrations](https://playground.twelvelabs.io/dashboard/integrations) page.

## Response Body

- 201: An import has successfully been initiated.

- 400: The request has failed.

## Examples

```shell
curl -X POST https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "index_id": "6298d673f1090f1100476d4c",
  "incremental_import": true,
  "retry_failed": false,
  "user_metadata": {
    "category": "recentlyAdded",
    "batchNumber": 5
  }
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c"

payload = {
    "index_id": "6298d673f1090f1100476d4c",
    "incremental_import": True,
    "retry_failed": False,
    "user_metadata": {
        "category": "recentlyAdded",
        "batchNumber": 5
    }
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"index_id":"6298d673f1090f1100476d4c","incremental_import":true,"retry_failed":false,"user_metadata":{"category":"recentlyAdded","batchNumber":5}}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c"

	payload := strings.NewReader("{\n  \"index_id\": \"6298d673f1090f1100476d4c\",\n  \"incremental_import\": true,\n  \"retry_failed\": false,\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5\n  }\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"index_id\": \"6298d673f1090f1100476d4c\",\n  \"incremental_import\": true,\n  \"retry_failed\": false,\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5\n  }\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"index_id\": \"6298d673f1090f1100476d4c\",\n  \"incremental_import\": true,\n  \"retry_failed\": false,\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5\n  }\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c', [
  'body' => '{
  "index_id": "6298d673f1090f1100476d4c",
  "incremental_import": true,
  "retry_failed": false,
  "user_metadata": {
    "category": "recentlyAdded",
    "batchNumber": 5
  }
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"index_id\": \"6298d673f1090f1100476d4c\",\n  \"incremental_import\": true,\n  \"retry_failed\": false,\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5\n  }\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "index_id": "6298d673f1090f1100476d4c",
  "incremental_import": true,
  "retry_failed": false,
  "user_metadata": [
    "category": "recentlyAdded",
    "batchNumber": 5
  ]
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/tasks/transfers/import/:integration-id \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "index_id": "string"
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id"

payload = { "index_id": "string" }
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"index_id":"string"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id"

	payload := strings.NewReader("{\n  \"index_id\": \"string\"\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"index_id\": \"string\"\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"index_id\": \"string\"\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id', [
  'body' => '{
  "index_id": "string"
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"index_id\": \"string\"\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = ["index_id": "string"] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Retrieve import status

```http
GET https://api.twelvelabs.io/v1.3/tasks/transfers/import/{integration-id}/status
```

This method retrieves the current status for each video from a specified integration and index. It returns an object containing lists of videos grouped by status. See the [Task object](/v1.3/api-reference/tasks/the-task-object) page for details on each status.



## Path Parameters

- integration-id (required): The unique identifier of the integration for which you want to retrieve the status of your imported videos. You can retrieve it from the [Integrations](https://playground.twelvelabs.io/dashboard/integrations) page.

## Query Parameters

- index_id (required): The unique identifier of the index for which you want to retrieve the status of your imported videos.


## Response Body

- 200: The status for each video from the specified integration and index has successfully been retrieved

- 400: The request has failed.

## Examples

```shell
curl -G https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/status \
     -H "x-api-key: " \
     -d index_id=6298d673f1090f1100476d4c
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/status"

querystring = {"index_id":"6298d673f1090f1100476d4c"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/status?index_id=6298d673f1090f1100476d4c';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/status?index_id=6298d673f1090f1100476d4c"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/status?index_id=6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/status?index_id=6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/status?index_id=6298d673f1090f1100476d4c', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/status?index_id=6298d673f1090f1100476d4c");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/status?index_id=6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -G https://api.twelvelabs.io/v1.3/tasks/transfers/import/:integration-id/status \
     -H "x-api-key: " \
     -d index_id=string
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/status"

querystring = {"index_id":"string"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/status?index_id=string';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/status?index_id=string"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/status?index_id=string")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/status?index_id=string")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/status?index_id=string', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/status?index_id=string");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/status?index_id=string")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Retrieve import logs

```http
GET https://api.twelvelabs.io/v1.3/tasks/transfers/import/{integration-id}/logs
```

This endpoint returns a chronological list of import operations for the specified integration. The list is sorted by creation date, with the oldest imports first. Each item in the list contains:
- The number of videos in each status
- Detailed error information for failed uploads, including filenames and error messages.

Use this endpoint to track import progress and troubleshoot potential issues across multiple operations.




## Path Parameters

- integration-id (required): The unique identifier of the integration for which you want to retrieve the import logs. You can retrieve it from the [Integrations](https://playground.twelvelabs.io/dashboard/integrations) page.

## Response Body

- 200: The import logs have successfully been retrieved.
- 400: The request has failed.

## Examples

```shell
curl https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/logs \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/logs"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/logs';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/logs"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/logs")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/logs")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/logs', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/logs");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/transfers/import/6298d673f1090f1100476d4c/logs")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl https://api.twelvelabs.io/v1.3/tasks/transfers/import/:integration-id/logs \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/logs"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/logs';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/logs"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/logs")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/logs")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/logs', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/logs");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/tasks/transfers/import/%3Aintegration-id/logs")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Manage videos

Use this endpoint to manage videos you've uploaded to the platform and retrieve information from them.


# The video object

The `video` object has the following fields:

* `_id`: A string representing the unique identifier of a video. The platform creates a new video object and assigns it a unique identifier when the video has been indexed. Note that video IDs are different from task IDs.
* `created_at`: A string indicating the date and time, in the RFC 3339 format, that the video indexing task was created.
* `indexed_at`: A string indicating the date and time, in the RFC 3339 format, that the video has finished indexing.
* `system_metadata`: An object that contains system-generated metadata about the video. It contains the following fields:
  * `duration`
  * `filename`
  * `fps`
  * `height`
  * `model_names`
  * `size`
  * `video_title`
  * `width`
* `user_metadata`: Any custom metadata you've specified by calling the [`PUT`](/v1.3/api-reference/videos/update)  method of the `/indexes/:index-id/videos/:video-id` endpoint.
* `hls`: The platform returns this object only for the videos that you uploaded with the `enable_video_stream` parameter set to `true`. This object has the following fields:
  * `video_url`: A string representing the URL of the video. You can use this URL to access the stream over the HLS protocol.
  * `thumbnail_urls`: An array containing the URL of the thumbnail.
  * `status`: A string representing the encoding status of the video file from its original format to a streamable format.
  * `updated_at`: A string indicating the date and time, in the RFC 3339 format, that the encoding status was last updated.
* `updated_at`: A string indicating the date and time, in the RFC 3339 format, that the video indexing task object was last updated. The platform updates this field every time the video indexing task transitions to a different state.


# List videos

```http
GET https://api.twelvelabs.io/v1.3/indexes/{index-id}/videos
```

This method returns a list of the videos in the specified index. By default, the API returns your videos sorted by creation date, with the newest at the top of the list.




## Path Parameters

- index-id (required): The unique identifier of the index for which the API will retrieve the videos.

## Query Parameters

- page (optional): A number that identifies the page to retrieve.

**Default**: `1`.

- page_limit (optional): The number of items to return on each page.

**Default**: `10`.
**Max**: `50`.

- sort_by (optional): The field to sort on. The following options are available:
- `updated_at`: Sorts by the time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"), when the item was updated.
- `created_at`: Sorts by the time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"), when the item was created.

**Default**: `created_at`.

- sort_option (optional): The sorting direction. The following options are available:
- `asc`
- `desc`

**Default**: `desc`.

- filename (optional): Filter by filename.

- duration (optional): Filter by duration. Expressed in seconds.

- fps (optional): Filter by frames per second.

- width (optional): Filter by width.

- height (optional): Filter by height.

- size (optional): Filter by size. Expressed in bytes.

- created_at (optional): Filter videos by the creation date and time of their associated indexing tasks, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"). The platform returns the videos whose indexing tasks were created on the specified date at or after the given time.

- updated_at (optional): This filter applies only to videos updated using the [`PUT`](/v1.3/api-reference/videos/update) method of the `/indexes/{index-id}/videos/{video-id}` endpoint. It filters videos by the last update date and time, in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ"). The platform returns the video indexing tasks that were last updated on the specified date at or after the given time.

- user_metadata (optional): To enable filtering by custom fields, you must first add user-defined metadata to your video by calling the [`PUT`](/v1.3/api-reference/videos/update) method of the `/indexes/:index-id/videos/:video-id` endpoint.

Examples:
- To filter on a string: `?category=recentlyAdded`
- To filter on an integer: `?batchNumber=5`
- To filter on a float: `?rating=9.3`
- To filter on a boolean: `?needsReview=true`


## Response Body

- 200: The video vectors in the specified index have successfully been retrieved.
- 400: The request has failed.

## Examples

```shell
curl -G https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos \
     -H "x-api-key: " \
     -d filename=01.mp4 \
     -d duration=10 \
     -d fps=25 \
     -d width=1920 \
     -d height=1080 \
     -d size=1048576 \
     --data-urlencode created_at=2024-08-16T16:53:59Z \
     --data-urlencode updated_at=2024-08-16T16:53:59Z
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos"

querystring = {"filename":"01.mp4","duration":"10","fps":"25","width":"1920","height":"1080","size":"1048576","created_at":"2024-08-16T16:53:59Z","updated_at":"2024-08-16T16:53:59Z","user_metadata":""}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos?filename=01.mp4&duration=10&fps=25&width=1920&height=1080&size=1048576&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A53%3A59Z&user_metadata=';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos?filename=01.mp4&duration=10&fps=25&width=1920&height=1080&size=1048576&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A53%3A59Z&user_metadata="

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos?filename=01.mp4&duration=10&fps=25&width=1920&height=1080&size=1048576&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A53%3A59Z&user_metadata=")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos?filename=01.mp4&duration=10&fps=25&width=1920&height=1080&size=1048576&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A53%3A59Z&user_metadata=")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos?filename=01.mp4&duration=10&fps=25&width=1920&height=1080&size=1048576&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A53%3A59Z&user_metadata=', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos?filename=01.mp4&duration=10&fps=25&width=1920&height=1080&size=1048576&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A53%3A59Z&user_metadata=");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos?filename=01.mp4&duration=10&fps=25&width=1920&height=1080&size=1048576&created_at=2024-08-16T16%3A53%3A59Z&updated_at=2024-08-16T16%3A53%3A59Z&user_metadata=")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -G https://api.twelvelabs.io/v1.3/indexes/:index-id/videos \
     -H "x-api-key: " \
     -d page=0 \
     -d page_limit=0
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos"

querystring = {"page":"0","page_limit":"0"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos?page=0&page_limit=0';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos?page=0&page_limit=0"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos?page=0&page_limit=0")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos?page=0&page_limit=0")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos?page=0&page_limit=0', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos?page=0&page_limit=0");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos?page=0&page_limit=0")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Retrieve video information

```http
GET https://api.twelvelabs.io/v1.3/indexes/{index-id}/videos/{video-id}
```

This method retrieves information about the specified video.




## Path Parameters

- index-id (required): The unique identifier of the index to which the video has been uploaded.

- video-id (required): The unique identifier of the video to retrieve.


## Query Parameters

- embedding_option (optional): Specifies which types of embeddings to retrieve. You can include one or more of the following values:
- `visual-text`:  Returns visual embeddings optimized for text search.
- `audio`: Returns audio embeddings.


To retrieve embeddings for a video, it must be indexed using the Marengo video understanding model version 2.7 or later. For details on enabling this model for an index, see the [Create an index](/reference/create-index) page.

The platform does not return embeddings if you don't provide this parameter.

The values you specify in `embedding_option` must be included in the `model_options` defined when the index was created. For example, if `model_options` is set to `visual,` you cannot set `embedding_option` to `audio` or  both `visual-text` and `audio`.

- transcription (optional): The parameter indicates whether to retrieve a transcription of the spoken words for the indexed video. Note that the official SDKs will support this feature in a future release.


## Response Body

- 200: The specified video information has successfully been retrieved.
- 400: The request has failed.
- 404: The specified resource does not exist.

## Examples

```shell
curl -G https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c \
     -H "x-api-key: " \
     -d transcription=true
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c"

querystring = {"transcription":"true"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c?transcription=true';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c?transcription=true"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c?transcription=true")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c?transcription=true")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c?transcription=true', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c?transcription=true");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c?transcription=true")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -G https://api.twelvelabs.io/v1.3/indexes/:index-id/videos/:video-id \
     -H "x-api-key: " \
     -d embedding_option=visual-text \
     -d transcription=true
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id"

querystring = {"embedding_option":"[\"visual-text\"]","transcription":"true"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -G https://api.twelvelabs.io/v1.3/indexes/:index-id/videos/:video-id \
     -H "x-api-key: " \
     -d embedding_option=visual-text \
     -d transcription=true
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id"

querystring = {"embedding_option":"[\"visual-text\"]","transcription":"true"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id?embedding_option=%5B%22visual-text%22%5D&transcription=true")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Update video information

```http
PUT https://api.twelvelabs.io/v1.3/indexes/{index-id}/videos/{video-id}
Content-Type: application/json
```

Use this method to update the metadata of a video such as file name.



## Path Parameters

- index-id (required): The unique identifier of the index to which the video has been uploaded.

- video-id (required): The unique identifier of the video to update.


## Response Body


- 400: The request has failed.

## Examples

```shell
curl -X PUT https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "user_metadata": {
    "category": "recentlyAdded",
    "batchNumber": 5,
    "rating": 9.3,
    "needsReview": true
  }
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c"

payload = { "user_metadata": {
        "category": "recentlyAdded",
        "batchNumber": 5,
        "rating": 9.3,
        "needsReview": True
    } }
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.put(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c';
const options = {
  method: 'PUT',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"user_metadata":{"category":"recentlyAdded","batchNumber":5,"rating":9.3,"needsReview":true}}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c"

	payload := strings.NewReader("{\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5,\n    \"rating\": 9.3,\n    \"needsReview\": true\n  }\n}")

	req, _ := http.NewRequest("PUT", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Put.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5,\n    \"rating\": 9.3,\n    \"needsReview\": true\n  }\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.put("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5,\n    \"rating\": 9.3,\n    \"needsReview\": true\n  }\n}")
  .asString();
```

```php
request('PUT', 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c', [
  'body' => '{
  "user_metadata": {
    "category": "recentlyAdded",
    "batchNumber": 5,
    "rating": 9.3,
    "needsReview": true
  }
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c");
var request = new RestRequest(Method.PUT);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5,\n    \"rating\": 9.3,\n    \"needsReview\": true\n  }\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = ["user_metadata": [
    "category": "recentlyAdded",
    "batchNumber": 5,
    "rating": 9.3,
    "needsReview": true
  ]] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "PUT"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X PUT https://api.twelvelabs.io/v1.3/indexes/:index-id/videos/:video-id \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id"

payload = {}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.put(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id';
const options = {
  method: 'PUT',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id"

	payload := strings.NewReader("{}")

	req, _ := http.NewRequest("PUT", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Put.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.put("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{}")
  .asString();
```

```php
request('PUT', 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id', [
  'body' => '{}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id");
var request = new RestRequest(Method.PUT);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "PUT"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Partial update video information

```http
PATCH https://api.twelvelabs.io/v1.3/indexes/{index-id}/videos/{video-id}
Content-Type: application/json
```

Use this method to update one or more fields of the metadata of a video such as file name. Also, you can delete a field by setting it to `null`.



## Path Parameters

- index-id (required): The unique identifier of the index to which the video has been uploaded.

- video-id (required): The unique identifier of the video to update.


## Response Body


- 400: The request has failed.

## Examples

```shell
curl -X PATCH https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "user_metadata": {
    "category": "recentlyAdded",
    "batchNumber": 5,
    "rating": 9.3,
    "needsReview": true
  }
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c"

payload = { "user_metadata": {
        "category": "recentlyAdded",
        "batchNumber": 5,
        "rating": 9.3,
        "needsReview": True
    } }
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.patch(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c';
const options = {
  method: 'PATCH',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"user_metadata":{"category":"recentlyAdded","batchNumber":5,"rating":9.3,"needsReview":true}}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c"

	payload := strings.NewReader("{\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5,\n    \"rating\": 9.3,\n    \"needsReview\": true\n  }\n}")

	req, _ := http.NewRequest("PATCH", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Patch.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5,\n    \"rating\": 9.3,\n    \"needsReview\": true\n  }\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.patch("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5,\n    \"rating\": 9.3,\n    \"needsReview\": true\n  }\n}")
  .asString();
```

```php
request('PATCH', 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c', [
  'body' => '{
  "user_metadata": {
    "category": "recentlyAdded",
    "batchNumber": 5,
    "rating": 9.3,
    "needsReview": true
  }
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c");
var request = new RestRequest(Method.PATCH);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"user_metadata\": {\n    \"category\": \"recentlyAdded\",\n    \"batchNumber\": 5,\n    \"rating\": 9.3,\n    \"needsReview\": true\n  }\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = ["user_metadata": [
    "category": "recentlyAdded",
    "batchNumber": 5,
    "rating": 9.3,
    "needsReview": true
  ]] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "PATCH"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X PATCH https://api.twelvelabs.io/v1.3/indexes/:index-id/videos/:video-id \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id"

payload = {}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.patch(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id';
const options = {
  method: 'PATCH',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id"

	payload := strings.NewReader("{}")

	req, _ := http.NewRequest("PATCH", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Patch.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.patch("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{}")
  .asString();
```

```php
request('PATCH', 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id', [
  'body' => '{}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id");
var request = new RestRequest(Method.PATCH);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "PATCH"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Delete video information

```http
DELETE https://api.twelvelabs.io/v1.3/indexes/{index-id}/videos/{video-id}
```

This method deletes all the information about the specified video. This action cannot be undone.




## Path Parameters

- index-id (required): The unique identifier of the index to which the video has been uploaded.

- video-id (required): The unique identifier of the video to delete.


## Response Body


- 400: The request has failed.

## Examples

```shell
curl -X DELETE https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c"

headers = {"x-api-key": ""}

response = requests.delete(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c';
const options = {method: 'DELETE', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c"

	req, _ := http.NewRequest("DELETE", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Delete.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.delete("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c")
  .header("x-api-key", "")
  .asString();
```

```php
request('DELETE', 'https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c");
var request = new RestRequest(Method.DELETE);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/6298d673f1090f1100476d4c/videos/6298d673f1090f1100476d4c")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "DELETE"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X DELETE https://api.twelvelabs.io/v1.3/indexes/:index-id/videos/:video-id \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id"

headers = {"x-api-key": ""}

response = requests.delete(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id';
const options = {method: 'DELETE', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id"

	req, _ := http.NewRequest("DELETE", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Delete.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.delete("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id")
  .header("x-api-key", "")
  .asString();
```

```php
request('DELETE', 'https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id");
var request = new RestRequest(Method.DELETE);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/indexes/%3Aindex-id/videos/%3Avideo-id")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "DELETE"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Any-to-video search

Use this endpoint to perform any-to-video searches. Currently, the platform supports text and image queries.


  * This endpoint supports pagination and filtering.
  * When you use pagination, you will not be charged for retrieving subsequent pages of results.


**Related guides**:

* [Search](/v1.3/docs/guides/search).

[**Related quickstart notebook**](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Search.ipynb)


# Make any-to-video search requests

```http
POST https://api.twelvelabs.io/v1.3/search
Content-Type: multipart/form-data
```

Use this endpoint to search for relevant matches in an index using text or various media queries.

**Text queries**:
- Use the `query_text` parameter to specify your query.

**Media queries**:
- Set the `query_media_type` parameter to the corresponding media type (example: `image`).
- Specify either one of the following parameters:
  - `query_media_url`: Publicly accessible URL of your media file.
  - `query_media_file`: Local media file.
  If both `query_media_url` and `query_media_file` are specified in the same request, `query_media_url` takes precedence.

Your images must meet the following requirements:
  - **Format**: JPEG and PNG.
  - **Dimension**: Must be at least 64 x 64 pixels.
  - **Size**: Must not exceed 5MB.



This endpoint is rate-limited. For details, see the [Rate limits](/v1.3/docs/get-started/rate-limits) page.





## Response Body

- 200: Successfully performed a search request.
- 400: The request has failed.
- 429: If the rate limit is reached, the platform returns an `HTTP 429 - Too many requests` error response. The response body is empty.


## Examples

```shell
curl -X POST https://api.twelvelabs.io/v1.3/search \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F query_media_file=@ \
     -F query_text="A man walking a dog" \
     -F index_id="6298d673f1090f1100476d4c" \
     -F search_options="visual" \
     -F adjust_confidence_level='0.5' \
     -F group_by="clip" \
     -F sort_option="score" \
     -F operator="or" \
     -F page_limit='10' \
     -F filter="{\"id\":[\"66284191ea717fa66a274832\"]}"
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/search"

files = { "query_media_file": "open('', 'rb')" }
payload = {
    "query_media_type": ,
    "query_media_url": ,
    "query_text": "A man walking a dog",
    "index_id": "6298d673f1090f1100476d4c",
    "adjust_confidence_level": "0.5",
    "group_by": "clip",
    "threshold": ,
    "sort_option": "score",
    "operator": "or",
    "page_limit": "10",
    "filter": "{\"id\":[\"66284191ea717fa66a274832\"]}",
    "include_user_metadata": 
}
headers = {"x-api-key": ""}

response = requests.post(url, data=payload, files=files, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/search';
const form = new FormData();
form.append('query_media_type', '');
form.append('query_media_url', '');
form.append('query_media_file', '');
form.append('query_text', 'A man walking a dog');
form.append('index_id', '6298d673f1090f1100476d4c');
form.append('adjust_confidence_level', '0.5');
form.append('group_by', 'clip');
form.append('threshold', '');
form.append('sort_option', 'score');
form.append('operator', 'or');
form.append('page_limit', '10');
form.append('filter', '{"id":["66284191ea717fa66a274832"]}');
form.append('include_user_metadata', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/search"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\nA man walking a dog\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\n6298d673f1090f1100476d4c\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n0.5\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\nclip\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\nscore\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\nor\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n10\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n{\"id\":[\"66284191ea717fa66a274832\"]}\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/search")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\nA man walking a dog\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\n6298d673f1090f1100476d4c\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n0.5\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\nclip\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\nscore\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\nor\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n10\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n{\"id\":[\"66284191ea717fa66a274832\"]}\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/search")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\nA man walking a dog\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\n6298d673f1090f1100476d4c\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n0.5\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\nclip\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\nscore\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\nor\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n10\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n{\"id\":[\"66284191ea717fa66a274832\"]}\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/search', [
  'multipart' => [
    [
        'name' => 'query_media_file',
        'filename' => '',
        'contents' => null
    ],
    [
        'name' => 'query_text',
        'contents' => 'A man walking a dog'
    ],
    [
        'name' => 'index_id',
        'contents' => '6298d673f1090f1100476d4c'
    ],
    [
        'name' => 'adjust_confidence_level',
        'contents' => '0.5'
    ],
    [
        'name' => 'group_by',
        'contents' => 'clip'
    ],
    [
        'name' => 'sort_option',
        'contents' => 'score'
    ],
    [
        'name' => 'operator',
        'contents' => 'or'
    ],
    [
        'name' => 'page_limit',
        'contents' => '10'
    ],
    [
        'name' => 'filter',
        'contents' => '{"id":["66284191ea717fa66a274832"]}'
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/search");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\nA man walking a dog\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\n6298d673f1090f1100476d4c\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n0.5\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\nclip\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\nscore\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\nor\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n10\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n{\"id\":[\"66284191ea717fa66a274832\"]}\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "query_media_type",
    "value": 
  ],
  [
    "name": "query_media_url",
    "value": 
  ],
  [
    "name": "query_media_file",
    "fileName": ""
  ],
  [
    "name": "query_text",
    "value": "A man walking a dog"
  ],
  [
    "name": "index_id",
    "value": "6298d673f1090f1100476d4c"
  ],
  [
    "name": "adjust_confidence_level",
    "value": "0.5"
  ],
  [
    "name": "group_by",
    "value": "clip"
  ],
  [
    "name": "threshold",
    "value": 
  ],
  [
    "name": "sort_option",
    "value": "score"
  ],
  [
    "name": "operator",
    "value": "or"
  ],
  [
    "name": "page_limit",
    "value": "10"
  ],
  [
    "name": "filter",
    "value": "{\"id\":[\"66284191ea717fa66a274832\"]}"
  ],
  [
    "name": "include_user_metadata",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/search")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/search \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F index_id="string" \
     -F search_options='[
  "visual"
]'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/search"

payload = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"
headers = {
    "x-api-key": "",
    "Content-Type": "multipart/form-data; boundary=---011000010111000001101001"
}

response = requests.post(url, data=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/search';
const form = new FormData();
form.append('query_media_type', '');
form.append('query_media_url', '');
form.append('query_text', '');
form.append('index_id', 'string');
form.append('search_options', '[
  "visual"
]');
form.append('adjust_confidence_level', '');
form.append('group_by', '');
form.append('threshold', '');
form.append('sort_option', '');
form.append('operator', '');
form.append('page_limit', '');
form.append('filter', '');
form.append('include_user_metadata', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/search"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/search")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/search")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/search', [
  'multipart' => [
    [
        'name' => 'index_id',
        'contents' => 'string'
    ],
    [
        'name' => 'search_options',
        'contents' => '[
  "visual"
]'
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/search");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "query_media_type",
    "value": 
  ],
  [
    "name": "query_media_url",
    "value": 
  ],
  [
    "name": "query_text",
    "value": 
  ],
  [
    "name": "index_id",
    "value": "string"
  ],
  [
    "name": "search_options",
    "value": "[
  \"visual\"
]"
  ],
  [
    "name": "adjust_confidence_level",
    "value": 
  ],
  [
    "name": "group_by",
    "value": 
  ],
  [
    "name": "threshold",
    "value": 
  ],
  [
    "name": "sort_option",
    "value": 
  ],
  [
    "name": "operator",
    "value": 
  ],
  [
    "name": "page_limit",
    "value": 
  ],
  [
    "name": "filter",
    "value": 
  ],
  [
    "name": "include_user_metadata",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/search")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/search \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F index_id="string" \
     -F search_options='[
  "visual"
]'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/search"

payload = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"
headers = {
    "x-api-key": "",
    "Content-Type": "multipart/form-data; boundary=---011000010111000001101001"
}

response = requests.post(url, data=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/search';
const form = new FormData();
form.append('query_media_type', '');
form.append('query_media_url', '');
form.append('query_text', '');
form.append('index_id', 'string');
form.append('search_options', '[
  "visual"
]');
form.append('adjust_confidence_level', '');
form.append('group_by', '');
form.append('threshold', '');
form.append('sort_option', '');
form.append('operator', '');
form.append('page_limit', '');
form.append('filter', '');
form.append('include_user_metadata', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/search"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/search")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/search")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/search', [
  'multipart' => [
    [
        'name' => 'index_id',
        'contents' => 'string'
    ],
    [
        'name' => 'search_options',
        'contents' => '[
  "visual"
]'
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/search");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_type\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_media_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"query_text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"index_id\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"search_options\"\r\n\r\n[\n  \"visual\"\n]\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"adjust_confidence_level\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"group_by\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"threshold\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"sort_option\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"operator\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"page_limit\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"filter\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"include_user_metadata\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "query_media_type",
    "value": 
  ],
  [
    "name": "query_media_url",
    "value": 
  ],
  [
    "name": "query_text",
    "value": 
  ],
  [
    "name": "index_id",
    "value": "string"
  ],
  [
    "name": "search_options",
    "value": "[
  \"visual\"
]"
  ],
  [
    "name": "adjust_confidence_level",
    "value": 
  ],
  [
    "name": "group_by",
    "value": 
  ],
  [
    "name": "threshold",
    "value": 
  ],
  [
    "name": "sort_option",
    "value": 
  ],
  [
    "name": "operator",
    "value": 
  ],
  [
    "name": "page_limit",
    "value": 
  ],
  [
    "name": "filter",
    "value": 
  ],
  [
    "name": "include_user_metadata",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/search")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Retrieve a specific page of search results

```http
GET https://api.twelvelabs.io/v1.3/search/{page-token}
```

Use this endpoint to retrieve a specific page of search results.


When you use pagination, you will not be charged for retrieving subsequent pages of results.





## Path Parameters

- page-token (required): A token that identifies the page to retrieve.


## Query Parameters

- include_user_metadata (optional): Specifies whether to include user-defined metadata in the search results.


## Response Body

- 200: Successfully retrieved the specified page of search results.
- 400: The request has failed.

## Examples

```shell
curl https://api.twelvelabs.io/v1.3/search/1234567890 \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/search/1234567890"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/search/1234567890';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/search/1234567890"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/search/1234567890")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/search/1234567890")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/search/1234567890', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/search/1234567890");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/search/1234567890")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -G https://api.twelvelabs.io/v1.3/search/:page-token \
     -H "x-api-key: " \
     -d include_user_metadata=true
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/search/%3Apage-token"

querystring = {"include_user_metadata":"true"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/search/%3Apage-token?include_user_metadata=true';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/search/%3Apage-token?include_user_metadata=true"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/search/%3Apage-token?include_user_metadata=true")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/search/%3Apage-token?include_user_metadata=true")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/search/%3Apage-token?include_user_metadata=true', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/search/%3Apage-token?include_user_metadata=true");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/search/%3Apage-token?include_user_metadata=true")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Create video embeddings

To create video embeddings, you must first upload your videos, and the platform must finish processing them. Uploading and processing videos require some time. Consequently, creating embeddings is an asynchronous process comprised of three steps:

1. [Create a video embedding task](/v1.3/api-reference/video-embeddings/create-video-embedding-task)  that uploads and processes a video.
2. [Monitor the status](/v1.3/api-reference/video-embeddings/retrieve-video-embedding-task-status) of your video embedding task.
3. [Retrieve the embeddings](/v1.3/api-reference/video-embeddings/retrieve-video-embeddings).

**Related guide**: [Create video embeddings](/v1.3/docs/guides/create-embeddings/video).

[**Related quickstart notebook**](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Embeddings.ipynb)


# The video embedding object

The video embedding object has the following fields:

* `segments`: An array of objects containing the embeddings for each video segment and the associated information. Each object contains the following fields:
  * `start_offset_time`: The start time of the video segment for this embedding. If the embedding scope is video, this field equals 0.
  * `end_offset_time`: The end time of the video segment for this embedding. If the embedding scope is video, this field equals the duration of the video.
  * `embedding_scope`: Indicates the scope of the embeddings. It can take the following values:
    * `video`: The platform has generated an embedding for the entire video.
    * `clip`: The platform generated embeddings for a specific segment.
  * `float`: An array of floating point numbers representing an embedding.  This array has 1024 dimensions, and you can use it with cosine similarity for various downstream tasks.
* `metadata`: An object containing metadata about the video. This object contains the following fields:
  * `input_filename`: The name of the video file. The platform returns this field when you upload a video from your local file system.
  * `input_url`: The URL of the video. The platform returns this field when you upload a video from a publicly accessible URL.
  * `video_clip_length`: The duration for each clip in seconds, as specified by the `video_clip_length` parameter of the [`POST`](/v1.3/api-reference/video-embeddings/create-video-embedding-task) method of the `/embed/task` endpoint. Note that the platform automatically truncates video segments shorter than 2 seconds. For a 31-second video divided into 6-second segments, the final 1-second segment will be truncated. This truncation only applies to the last segment if it does not meet the minimum length requirement of 2 seconds.
  * `video_embedding_scope`:   The scope of the video embedding. It can take one of the following values: `['clip']` or `['clip', 'video]`.
  * `duration`:  The total duration of the video in seconds.


# Create a video embedding task

```http
POST https://api.twelvelabs.io/v1.3/embed/tasks
Content-Type: multipart/form-data
```

This method creates a new video embedding task that uploads a video to the platform and creates one or multiple video embeddings.

Upload options:
- **Local file**: Use the `video_file` parameter
- **Publicly accessible URL**: Use the `video_url` parameter.

Specify at least one option. If both are provided, `video_url` takes precedence.


  The videos you wish to upload must meet the following requirements:
  - **Video resolution**: Must be at least 360x360 and must not exceed 3840x2160.
  - **Aspect ratio**: Must be one of 1:1, 4:3, 4:5, 5:4, 16:9, 9:16, or 17:9.
  - **Video and audio formats**: Your video files must be encoded in the video and audio formats listed on the [FFmpeg Formats Documentation](https://ffmpeg.org/ffmpeg-formats.html) page. For videos in other formats, contact us at support@twelvelabs.io.
  - **Duration**: Must be between 4 seconds and 2 hours (7,200s).
  - **File size**: Must not exceed 2 GB.
    If you require different options, contact us at support@twelvelabs.io.



- The "Marengo-retrieval-2.7" video understanding model generates embeddings for all modalities in the same latent space. This shared space enables any-to-any searches across different types of content.
- Video embeddings are stored for seven days.
- The platform supports uploading video files that can play without additional user interaction or custom video players. Ensure your URL points to the raw video file, not a web page containing the video. Links to third-party hosting sites, cloud storage services, or videos requiring extra steps to play are not supported.





## Response Body

- 200: A video embedding task has successfully been created.

- 400: The request has failed.

## Examples

```shell Video embedding using clip scope
curl -X POST https://api.twelvelabs.io/v1.3/embed/tasks \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F model_name="Marengo-retrieval-2.7" \
     -F video_file=@/Users/john/Documents/video.mp4 \
     -F video_embedding_scope='[
  "clip"
]'
```

```python Video embedding using clip scope
import requests

url = "https://api.twelvelabs.io/v1.3/embed/tasks"

files = { "video_file": "open('/Users/john/Documents/video.mp4', 'rb')" }
payload = {
    "model_name": "Marengo-retrieval-2.7",
    "video_url": ,
    "video_start_offset_sec": ,
    "video_end_offset_sec": ,
    "video_clip_length": ,
    "video_embedding_scope": "[
  \"clip\"
]"
}
headers = {"x-api-key": ""}

response = requests.post(url, data=payload, files=files, headers=headers)

print(response.json())
```

```javascript Video embedding using clip scope
const url = 'https://api.twelvelabs.io/v1.3/embed/tasks';
const form = new FormData();
form.append('model_name', 'Marengo-retrieval-2.7');
form.append('video_file', '/Users/john/Documents/video.mp4');
form.append('video_url', '');
form.append('video_start_offset_sec', '');
form.append('video_end_offset_sec', '');
form.append('video_clip_length', '');
form.append('video_embedding_scope', '[
  "clip"
]');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Video embedding using clip scope
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed/tasks"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_file\"; filename=\"video.mp4\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_end_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_clip_length\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_embedding_scope\"\r\n\r\n[\n  \"clip\"\n]\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Video embedding using clip scope
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed/tasks")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_file\"; filename=\"video.mp4\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_end_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_clip_length\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_embedding_scope\"\r\n\r\n[\n  \"clip\"\n]\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java Video embedding using clip scope
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed/tasks")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_file\"; filename=\"video.mp4\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_end_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_clip_length\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_embedding_scope\"\r\n\r\n[\n  \"clip\"\n]\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php Video embedding using clip scope
request('POST', 'https://api.twelvelabs.io/v1.3/embed/tasks', [
  'multipart' => [
    [
        'name' => 'model_name',
        'contents' => 'Marengo-retrieval-2.7'
    ],
    [
        'name' => 'video_file',
        'filename' => '/Users/john/Documents/video.mp4',
        'contents' => null
    ],
    [
        'name' => 'video_embedding_scope',
        'contents' => '[
  "clip"
]'
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Video embedding using clip scope
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed/tasks");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_file\"; filename=\"video.mp4\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_end_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_clip_length\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_embedding_scope\"\r\n\r\n[\n  \"clip\"\n]\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Video embedding using clip scope
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "model_name",
    "value": "Marengo-retrieval-2.7"
  ],
  [
    "name": "video_file",
    "fileName": "/Users/john/Documents/video.mp4"
  ],
  [
    "name": "video_url",
    "value": 
  ],
  [
    "name": "video_start_offset_sec",
    "value": 
  ],
  [
    "name": "video_end_offset_sec",
    "value": 
  ],
  [
    "name": "video_clip_length",
    "value": 
  ],
  [
    "name": "video_embedding_scope",
    "value": "[
  \"clip\"
]"
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed/tasks")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell embed_tasks_create_example
curl -X POST https://api.twelvelabs.io/v1.3/embed/tasks \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data"
```

```python embed_tasks_create_example
import requests

url = "https://api.twelvelabs.io/v1.3/embed/tasks"

payload = "-----011000010111000001101001--\r\n"
headers = {
    "x-api-key": "",
    "Content-Type": "multipart/form-data; boundary=---011000010111000001101001"
}

response = requests.post(url, data=payload, headers=headers)

print(response.json())
```

```javascript embed_tasks_create_example
const url = 'https://api.twelvelabs.io/v1.3/embed/tasks';
const form = new FormData();

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go embed_tasks_create_example
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed/tasks"

	payload := strings.NewReader("-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby embed_tasks_create_example
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed/tasks")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java embed_tasks_create_example
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed/tasks")
  .header("x-api-key", "")
  .body("-----011000010111000001101001--\r\n")
  .asString();
```

```php embed_tasks_create_example
request('POST', 'https://api.twelvelabs.io/v1.3/embed/tasks', [
  'headers' => [
    'Content-Type' => 'multipart/form-data; boundary=---011000010111000001101001',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp embed_tasks_create_example
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed/tasks");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "multipart/form-data; boundary=---011000010111000001101001");
request.AddParameter("undefined", "-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift embed_tasks_create_example
import Foundation

let headers = ["x-api-key": ""]
let parameters = []

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed/tasks")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/embed/tasks \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F model_name="string"
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/embed/tasks"

payload = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_end_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_clip_length\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_embedding_scope\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"
headers = {
    "x-api-key": "",
    "Content-Type": "multipart/form-data; boundary=---011000010111000001101001"
}

response = requests.post(url, data=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/embed/tasks';
const form = new FormData();
form.append('model_name', 'string');
form.append('video_url', '');
form.append('video_start_offset_sec', '');
form.append('video_end_offset_sec', '');
form.append('video_clip_length', '');
form.append('video_embedding_scope', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed/tasks"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_end_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_clip_length\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_embedding_scope\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed/tasks")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_end_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_clip_length\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_embedding_scope\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed/tasks")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_end_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_clip_length\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_embedding_scope\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/embed/tasks', [
  'multipart' => [
    [
        'name' => 'model_name',
        'contents' => 'string'
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed/tasks");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_end_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_clip_length\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"video_embedding_scope\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "model_name",
    "value": "string"
  ],
  [
    "name": "video_url",
    "value": 
  ],
  [
    "name": "video_start_offset_sec",
    "value": 
  ],
  [
    "name": "video_end_offset_sec",
    "value": 
  ],
  [
    "name": "video_clip_length",
    "value": 
  ],
  [
    "name": "video_embedding_scope",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed/tasks")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# List video embedding tasks

```http
GET https://api.twelvelabs.io/v1.3/embed/tasks
```

This method returns a list of the video embedding tasks in your account. The platform returns your video embedding tasks sorted by creation date, with the newest at the top of the list.


- Video embeddings are stored for seven days
- When you invoke this method without specifying the `started_at` and `ended_at` parameters, the platform returns all the video embedding tasks created within the last seven days.





## Query Parameters

- started_at (optional): Retrieve the video embedding tasks that were created after the given date and time, expressed in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ").

- ended_at (optional): Retrieve the video embedding tasks that were created before the given date and time, expressed in the RFC 3339 format ("YYYY-MM-DDTHH:mm:ssZ").

- status (optional): Filter video embedding tasks by their current status. Possible values are `processing`, `ready`, or `failed`.
- page (optional): A number that identifies the page to retrieve.

**Default**: `1`.

- page_limit (optional): The number of items to return on each page.

**Default**: `10`.
**Max**: `50`.


## Response Body

- 200: A list of video embedding tasks has successfully been retrieved.

- 400: The request has failed.

## Examples

```shell
curl https://api.twelvelabs.io/v1.3/embed/tasks \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/embed/tasks"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/embed/tasks';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed/tasks"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed/tasks")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/embed/tasks")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/embed/tasks', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed/tasks");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed/tasks")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -G https://api.twelvelabs.io/v1.3/embed/tasks \
     -H "x-api-key: " \
     -d started_at=string \
     -d ended_at=string
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/embed/tasks"

querystring = {"started_at":"string","ended_at":"string"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/embed/tasks?started_at=string&ended_at=string';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed/tasks?started_at=string&ended_at=string"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed/tasks?started_at=string&ended_at=string")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/embed/tasks?started_at=string&ended_at=string")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/embed/tasks?started_at=string&ended_at=string', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed/tasks?started_at=string&ended_at=string");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed/tasks?started_at=string&ended_at=string")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Retrieve the status of a video embedding task

```http
GET https://api.twelvelabs.io/v1.3/embed/tasks/{task_id}/status
```

This method retrieves the status of a video embedding task. Check the task status of a video embedding task to determine when you can retrieve the embedding.

A task can have one of the following statuses:
- `processing`: The platform is creating the embeddings.
- `ready`:  Processing is complete. Retrieve the embeddings by invoking the [`GET`](/v1.3/api-reference/video-embeddings/retrieve-video-embeddings) method of the `/embed/tasks/{task_id} endpoint`.
- `failed`: The task could not be completed, and the embeddings haven't been created.




## Path Parameters

- task_id (required): The unique identifier of your video embedding task.


## Response Body

- 200: The status of your video embedding task has been retrieved.

- 400: The request has failed.

## Examples

```shell
curl https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6/status \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6/status"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6/status';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6/status"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6/status")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6/status")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6/status', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6/status");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6/status")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl https://api.twelvelabs.io/v1.3/embed/tasks/:task_id/status \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id/status"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id/status';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id/status"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id/status")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id/status")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id/status', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id/status");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id/status")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Retrieve video embeddings

```http
GET https://api.twelvelabs.io/v1.3/embed/tasks/{task_id}
```

This method retrieves embeddings for a specific video embedding task. Ensure the task status is `ready` before invoking this method. Refer to the [Retrieve the status of a video embedding tasks](/v1.3/api-reference/video-embeddings/retrieve-video-embedding-task-status) page for instructions on checking the task status.




## Path Parameters

- task_id (required): The unique identifier of your video embedding task.


## Query Parameters

- embedding_option (optional): Specifies which types of embeddings to retrieve. You can include one or more of the following values:
  - `visual-text`:  Returns visual embeddings optimized for text search.
  - `audio`: Returns audio embeddings.

The platform returns all available embeddings if you don't provide this parameter.


## Response Body

- 200: Video embeddings have successfully been retrieved.

- 400: The request has failed.

## Examples

```shell
curl https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6 \
     -H "x-api-key: "
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6"

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed/tasks/663da73b31cdd0c1f638a8e6")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -G https://api.twelvelabs.io/v1.3/embed/tasks/:task_id \
     -H "x-api-key: " \
     -d embedding_option=visual-text
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id"

querystring = {"embedding_option":"[\"visual-text\"]"}

headers = {"x-api-key": ""}

response = requests.get(url, headers=headers, params=querystring)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id?embedding_option=%5B%22visual-text%22%5D';
const options = {method: 'GET', headers: {'x-api-key': ''}};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id?embedding_option=%5B%22visual-text%22%5D"

	req, _ := http.NewRequest("GET", url, nil)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id?embedding_option=%5B%22visual-text%22%5D")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Get.new(url)
request["x-api-key"] = ''

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.get("https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id?embedding_option=%5B%22visual-text%22%5D")
  .header("x-api-key", "")
  .asString();
```

```php
request('GET', 'https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id?embedding_option=%5B%22visual-text%22%5D', [
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id?embedding_option=%5B%22visual-text%22%5D");
var request = new RestRequest(Method.GET);
request.AddHeader("x-api-key", "");
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed/tasks/%3Atask_id?embedding_option=%5B%22visual-text%22%5D")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "GET"
request.allHTTPHeaderFields = headers

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Create text, image, and audio embeddings

Use this endpoint to create text, image, and audio embeddings.

**Related guides**:

* [Create text embeddings](/v1.3/docs/guides/create-embeddings/text)
* [Crete image embeddings](/v1.3/docs/guides/create-embeddings/image)
* [Create audio embeddings](/v1.3/docs/guides/create-embeddings/audio)


# The embedding object

The embedding object has the following fields:

* `model_name`: A string that represents the name of the [video understanding model](/v1.3/docs/concepts/models) used by the platform to create the embeddings.

* One or more of the following embedding fields, depending on the parameters specified in the request:
  * `text_embedding`
  * `audio_embedding`
  * `image_embedding`\
    Each of these fields is an object containing the following fields, among other information:
    * `segments`: An array of objects that contains the embeddings for each segment and associated information. Each of these objects contains, among other information, an array of floating point numbers named `float` representing an embedding. This array has 1024 dimensions, and you can use it with cosine similarity for various downstream tasks.

The “Marengo-retrieval-2.7” video understanding model generates embeddings for all modalities in the same latent space. This shared space enables any-to-any searches across different types of content.


# Create embeddings for text, image, and audio

```http
POST https://api.twelvelabs.io/v1.3/embed
Content-Type: multipart/form-data
```

This method creates embeddings for text, image, and audio content.

Before you create an embedding, ensure that your image or audio files meet the following prerequisites:
- [Image embeddings](/v1.3/docs/guides/create-embeddings/image#prerequisites)
- [Audio embeddings](/v1.3/docs/guides/create-embeddings/audio#prerequisites)

Parameters for embeddings:
- **Common parameters**:
  - `model_name`: The video understanding model you want to use. Example: "Marengo-retrieval-2.7".
- **Text embeddings**:
  - `text`: Text for which to create an embedding.
- **Image embeddings**:
  Provide one of the following:
  - `image_url`: Publicly accessible URL of your image file.
  - `image_file`:  Local image file.
- **Audio embeddings**:
  Provide one of the following:
  - `audio_url`: Publicly accessible URL of your audio file.
  - `audio_file`: Local audio file.






## Response Body

- 200: A text embedding has successfully been created.

- 400: The request has failed.

## Examples

```shell Text embedding
curl -X POST https://api.twelvelabs.io/v1.3/embed \
     -H "x-api-key: 
- The "Marengo-retrieval-2.7" video understanding model generates embeddings for all modalities in the same latent space. This shared space enables any-to-any searches across different types of content.
- You can create multiple types of embeddings in a single API call.
- Audio embeddings combine generic sound and human speech in a single embedding. For videos with transcriptions, you can retrieve transcriptions and then [create text embeddings](/v1.3/api-reference/text-image-audio-embeddings/create-text-image-audio-embeddings) from these transcriptions.
" \
     -H "Content-Type: multipart/form-data" \
     -F model_name="Marengo-retrieval-2.7" \
     -F text="Man with a dog crossing the street" \
     -F image_file=@ \
     -F audio_file=@
```

```python Text embedding
import requests

url = "https://api.twelvelabs.io/v1.3/embed"

files = {
    "image_file": "open('', 'rb')",
    "audio_file": "open('', 'rb')"
}
payload = {
    "model_name": "Marengo-retrieval-2.7",
    "text": "Man with a dog crossing the street",
    "text_truncate": ,
    "image_url": ,
    "audio_url": ,
    "audio_start_offset_sec": 
}
headers = {"x-api-key": ""}

response = requests.post(url, data=payload, files=files, headers=headers)

print(response.json())
```

```javascript Text embedding
const url = 'https://api.twelvelabs.io/v1.3/embed';
const form = new FormData();
form.append('model_name', 'Marengo-retrieval-2.7');
form.append('text', 'Man with a dog crossing the street');
form.append('text_truncate', '');
form.append('image_url', '');
form.append('image_file', '');
form.append('audio_url', '');
form.append('audio_file', '');
form.append('audio_start_offset_sec', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Text embedding
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\nMan with a dog crossing the street\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Text embedding
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\nMan with a dog crossing the street\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java Text embedding
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\nMan with a dog crossing the street\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php Text embedding
request('POST', 'https://api.twelvelabs.io/v1.3/embed', [
  'multipart' => [
    [
        'name' => 'model_name',
        'contents' => 'Marengo-retrieval-2.7'
    ],
    [
        'name' => 'text',
        'contents' => 'Man with a dog crossing the street'
    ],
    [
        'name' => 'image_file',
        'filename' => '',
        'contents' => null
    ],
    [
        'name' => 'audio_file',
        'filename' => '',
        'contents' => null
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Text embedding
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\nMan with a dog crossing the street\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Text embedding
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "model_name",
    "value": "Marengo-retrieval-2.7"
  ],
  [
    "name": "text",
    "value": "Man with a dog crossing the street"
  ],
  [
    "name": "text_truncate",
    "value": 
  ],
  [
    "name": "image_url",
    "value": 
  ],
  [
    "name": "image_file",
    "fileName": ""
  ],
  [
    "name": "audio_url",
    "value": 
  ],
  [
    "name": "audio_file",
    "fileName": ""
  ],
  [
    "name": "audio_start_offset_sec",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Image embedding using URL
curl -X POST https://api.twelvelabs.io/v1.3/embed \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F model_name="Marengo-retrieval-2.7" \
     -F image_url="https://example.com/image.jpg" \
     -F image_file=@ \
     -F audio_file=@
```

```python Image embedding using URL
import requests

url = "https://api.twelvelabs.io/v1.3/embed"

files = {
    "image_file": "open('', 'rb')",
    "audio_file": "open('', 'rb')"
}
payload = {
    "model_name": "Marengo-retrieval-2.7",
    "text": ,
    "text_truncate": ,
    "image_url": "https://example.com/image.jpg",
    "audio_url": ,
    "audio_start_offset_sec": 
}
headers = {"x-api-key": ""}

response = requests.post(url, data=payload, files=files, headers=headers)

print(response.json())
```

```javascript Image embedding using URL
const url = 'https://api.twelvelabs.io/v1.3/embed';
const form = new FormData();
form.append('model_name', 'Marengo-retrieval-2.7');
form.append('text', '');
form.append('text_truncate', '');
form.append('image_url', 'https://example.com/image.jpg');
form.append('image_file', '');
form.append('audio_url', '');
form.append('audio_file', '');
form.append('audio_start_offset_sec', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Image embedding using URL
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\nhttps://example.com/image.jpg\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Image embedding using URL
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\nhttps://example.com/image.jpg\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java Image embedding using URL
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\nhttps://example.com/image.jpg\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php Image embedding using URL
request('POST', 'https://api.twelvelabs.io/v1.3/embed', [
  'multipart' => [
    [
        'name' => 'model_name',
        'contents' => 'Marengo-retrieval-2.7'
    ],
    [
        'name' => 'image_url',
        'contents' => 'https://example.com/image.jpg'
    ],
    [
        'name' => 'image_file',
        'filename' => '',
        'contents' => null
    ],
    [
        'name' => 'audio_file',
        'filename' => '',
        'contents' => null
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Image embedding using URL
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\nhttps://example.com/image.jpg\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Image embedding using URL
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "model_name",
    "value": "Marengo-retrieval-2.7"
  ],
  [
    "name": "text",
    "value": 
  ],
  [
    "name": "text_truncate",
    "value": 
  ],
  [
    "name": "image_url",
    "value": "https://example.com/image.jpg"
  ],
  [
    "name": "image_file",
    "fileName": ""
  ],
  [
    "name": "audio_url",
    "value": 
  ],
  [
    "name": "audio_file",
    "fileName": ""
  ],
  [
    "name": "audio_start_offset_sec",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Image embedding using local file
curl -X POST https://api.twelvelabs.io/v1.3/embed \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F model_name="Marengo-retrieval-2.7" \
     -F image_file=@/Users/john/Documents/image.jpg \
     -F audio_file=@
```

```python Image embedding using local file
import requests

url = "https://api.twelvelabs.io/v1.3/embed"

files = {
    "image_file": "open('/Users/john/Documents/image.jpg', 'rb')",
    "audio_file": "open('', 'rb')"
}
payload = {
    "model_name": "Marengo-retrieval-2.7",
    "text": ,
    "text_truncate": ,
    "image_url": ,
    "audio_url": ,
    "audio_start_offset_sec": 
}
headers = {"x-api-key": ""}

response = requests.post(url, data=payload, files=files, headers=headers)

print(response.json())
```

```javascript Image embedding using local file
const url = 'https://api.twelvelabs.io/v1.3/embed';
const form = new FormData();
form.append('model_name', 'Marengo-retrieval-2.7');
form.append('text', '');
form.append('text_truncate', '');
form.append('image_url', '');
form.append('image_file', '/Users/john/Documents/image.jpg');
form.append('audio_url', '');
form.append('audio_file', '');
form.append('audio_start_offset_sec', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Image embedding using local file
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"image.jpg\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Image embedding using local file
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"image.jpg\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java Image embedding using local file
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"image.jpg\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php Image embedding using local file
request('POST', 'https://api.twelvelabs.io/v1.3/embed', [
  'multipart' => [
    [
        'name' => 'model_name',
        'contents' => 'Marengo-retrieval-2.7'
    ],
    [
        'name' => 'image_file',
        'filename' => '/Users/john/Documents/image.jpg',
        'contents' => null
    ],
    [
        'name' => 'audio_file',
        'filename' => '',
        'contents' => null
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Image embedding using local file
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"image.jpg\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Image embedding using local file
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "model_name",
    "value": "Marengo-retrieval-2.7"
  ],
  [
    "name": "text",
    "value": 
  ],
  [
    "name": "text_truncate",
    "value": 
  ],
  [
    "name": "image_url",
    "value": 
  ],
  [
    "name": "image_file",
    "fileName": "/Users/john/Documents/image.jpg"
  ],
  [
    "name": "audio_url",
    "value": 
  ],
  [
    "name": "audio_file",
    "fileName": ""
  ],
  [
    "name": "audio_start_offset_sec",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Audio embedding using URL
curl -X POST https://api.twelvelabs.io/v1.3/embed \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F model_name="Marengo-retrieval-2.7" \
     -F image_file=@ \
     -F audio_url="https://example.com/audio.mp3" \
     -F audio_file=@
```

```python Audio embedding using URL
import requests

url = "https://api.twelvelabs.io/v1.3/embed"

files = {
    "image_file": "open('', 'rb')",
    "audio_file": "open('', 'rb')"
}
payload = {
    "model_name": "Marengo-retrieval-2.7",
    "text": ,
    "text_truncate": ,
    "image_url": ,
    "audio_url": "https://example.com/audio.mp3",
    "audio_start_offset_sec": 
}
headers = {"x-api-key": ""}

response = requests.post(url, data=payload, files=files, headers=headers)

print(response.json())
```

```javascript Audio embedding using URL
const url = 'https://api.twelvelabs.io/v1.3/embed';
const form = new FormData();
form.append('model_name', 'Marengo-retrieval-2.7');
form.append('text', '');
form.append('text_truncate', '');
form.append('image_url', '');
form.append('image_file', '');
form.append('audio_url', 'https://example.com/audio.mp3');
form.append('audio_file', '');
form.append('audio_start_offset_sec', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Audio embedding using URL
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\nhttps://example.com/audio.mp3\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Audio embedding using URL
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\nhttps://example.com/audio.mp3\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java Audio embedding using URL
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\nhttps://example.com/audio.mp3\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php Audio embedding using URL
request('POST', 'https://api.twelvelabs.io/v1.3/embed', [
  'multipart' => [
    [
        'name' => 'model_name',
        'contents' => 'Marengo-retrieval-2.7'
    ],
    [
        'name' => 'image_file',
        'filename' => '',
        'contents' => null
    ],
    [
        'name' => 'audio_url',
        'contents' => 'https://example.com/audio.mp3'
    ],
    [
        'name' => 'audio_file',
        'filename' => '',
        'contents' => null
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Audio embedding using URL
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\nhttps://example.com/audio.mp3\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Audio embedding using URL
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "model_name",
    "value": "Marengo-retrieval-2.7"
  ],
  [
    "name": "text",
    "value": 
  ],
  [
    "name": "text_truncate",
    "value": 
  ],
  [
    "name": "image_url",
    "value": 
  ],
  [
    "name": "image_file",
    "fileName": ""
  ],
  [
    "name": "audio_url",
    "value": "https://example.com/audio.mp3"
  ],
  [
    "name": "audio_file",
    "fileName": ""
  ],
  [
    "name": "audio_start_offset_sec",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Audio embedding using local file
curl -X POST https://api.twelvelabs.io/v1.3/embed \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F model_name="Marengo-retrieval-2.7" \
     -F image_file=@ \
     -F audio_file=@/Users/john/Documents/audio.mp3
```

```python Audio embedding using local file
import requests

url = "https://api.twelvelabs.io/v1.3/embed"

files = {
    "image_file": "open('', 'rb')",
    "audio_file": "open('/Users/john/Documents/audio.mp3', 'rb')"
}
payload = {
    "model_name": "Marengo-retrieval-2.7",
    "text": ,
    "text_truncate": ,
    "image_url": ,
    "audio_url": ,
    "audio_start_offset_sec": 
}
headers = {"x-api-key": ""}

response = requests.post(url, data=payload, files=files, headers=headers)

print(response.json())
```

```javascript Audio embedding using local file
const url = 'https://api.twelvelabs.io/v1.3/embed';
const form = new FormData();
form.append('model_name', 'Marengo-retrieval-2.7');
form.append('text', '');
form.append('text_truncate', '');
form.append('image_url', '');
form.append('image_file', '');
form.append('audio_url', '');
form.append('audio_file', '/Users/john/Documents/audio.mp3');
form.append('audio_start_offset_sec', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Audio embedding using local file
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"audio.mp3\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Audio embedding using local file
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"audio.mp3\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java Audio embedding using local file
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"audio.mp3\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php Audio embedding using local file
request('POST', 'https://api.twelvelabs.io/v1.3/embed', [
  'multipart' => [
    [
        'name' => 'model_name',
        'contents' => 'Marengo-retrieval-2.7'
    ],
    [
        'name' => 'image_file',
        'filename' => '',
        'contents' => null
    ],
    [
        'name' => 'audio_file',
        'filename' => '/Users/john/Documents/audio.mp3',
        'contents' => null
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Audio embedding using local file
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"audio.mp3\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Audio embedding using local file
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "model_name",
    "value": "Marengo-retrieval-2.7"
  ],
  [
    "name": "text",
    "value": 
  ],
  [
    "name": "text_truncate",
    "value": 
  ],
  [
    "name": "image_url",
    "value": 
  ],
  [
    "name": "image_file",
    "fileName": ""
  ],
  [
    "name": "audio_url",
    "value": 
  ],
  [
    "name": "audio_file",
    "fileName": "/Users/john/Documents/audio.mp3"
  ],
  [
    "name": "audio_start_offset_sec",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Text embedding with truncation
curl -X POST https://api.twelvelabs.io/v1.3/embed \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F model_name="Marengo-retrieval-2.7" \
     -F text="This is a very long text that might exceed the token limit and need truncation" \
     -F text_truncate="start" \
     -F image_file=@ \
     -F audio_file=@
```

```python Text embedding with truncation
import requests

url = "https://api.twelvelabs.io/v1.3/embed"

files = {
    "image_file": "open('', 'rb')",
    "audio_file": "open('', 'rb')"
}
payload = {
    "model_name": "Marengo-retrieval-2.7",
    "text": "This is a very long text that might exceed the token limit and need truncation",
    "text_truncate": "start",
    "image_url": ,
    "audio_url": ,
    "audio_start_offset_sec": 
}
headers = {"x-api-key": ""}

response = requests.post(url, data=payload, files=files, headers=headers)

print(response.json())
```

```javascript Text embedding with truncation
const url = 'https://api.twelvelabs.io/v1.3/embed';
const form = new FormData();
form.append('model_name', 'Marengo-retrieval-2.7');
form.append('text', 'This is a very long text that might exceed the token limit and need truncation');
form.append('text_truncate', 'start');
form.append('image_url', '');
form.append('image_file', '');
form.append('audio_url', '');
form.append('audio_file', '');
form.append('audio_start_offset_sec', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Text embedding with truncation
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\nThis is a very long text that might exceed the token limit and need truncation\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\nstart\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Text embedding with truncation
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\nThis is a very long text that might exceed the token limit and need truncation\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\nstart\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java Text embedding with truncation
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\nThis is a very long text that might exceed the token limit and need truncation\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\nstart\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php Text embedding with truncation
request('POST', 'https://api.twelvelabs.io/v1.3/embed', [
  'multipart' => [
    [
        'name' => 'model_name',
        'contents' => 'Marengo-retrieval-2.7'
    ],
    [
        'name' => 'text',
        'contents' => 'This is a very long text that might exceed the token limit and need truncation'
    ],
    [
        'name' => 'text_truncate',
        'contents' => 'start'
    ],
    [
        'name' => 'image_file',
        'filename' => '',
        'contents' => null
    ],
    [
        'name' => 'audio_file',
        'filename' => '',
        'contents' => null
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Text embedding with truncation
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nMarengo-retrieval-2.7\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\nThis is a very long text that might exceed the token limit and need truncation\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\nstart\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_file\"; filename=\"\"\r\nContent-Type: application/octet-stream\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Text embedding with truncation
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "model_name",
    "value": "Marengo-retrieval-2.7"
  ],
  [
    "name": "text",
    "value": "This is a very long text that might exceed the token limit and need truncation"
  ],
  [
    "name": "text_truncate",
    "value": "start"
  ],
  [
    "name": "image_url",
    "value": 
  ],
  [
    "name": "image_file",
    "fileName": ""
  ],
  [
    "name": "audio_url",
    "value": 
  ],
  [
    "name": "audio_file",
    "fileName": ""
  ],
  [
    "name": "audio_start_offset_sec",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/embed \
     -H "x-api-key: " \
     -H "Content-Type: multipart/form-data" \
     -F model_name="string"
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/embed"

payload = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"
headers = {
    "x-api-key": "",
    "Content-Type": "multipart/form-data; boundary=---011000010111000001101001"
}

response = requests.post(url, data=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/embed';
const form = new FormData();
form.append('model_name', 'string');
form.append('text', '');
form.append('text_truncate', '');
form.append('image_url', '');
form.append('audio_url', '');
form.append('audio_start_offset_sec', '');

const options = {method: 'POST', headers: {'x-api-key': ''}};

options.body = form;

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/embed"

	payload := strings.NewReader("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/embed")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request.body = "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/embed")
  .header("x-api-key", "")
  .body("-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/embed', [
  'multipart' => [
    [
        'name' => 'model_name',
        'contents' => 'string'
    ]
  ]
  'headers' => [
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/embed");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddParameter("undefined", "-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"model_name\"\r\n\r\nstring\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"text_truncate\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"image_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_url\"\r\n\r\n\r\n-----011000010111000001101001\r\nContent-Disposition: form-data; name=\"audio_start_offset_sec\"\r\n\r\n\r\n-----011000010111000001101001--\r\n", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = ["x-api-key": ""]
let parameters = [
  [
    "name": "model_name",
    "value": "string"
  ],
  [
    "name": "text",
    "value": 
  ],
  [
    "name": "text_truncate",
    "value": 
  ],
  [
    "name": "image_url",
    "value": 
  ],
  [
    "name": "audio_url",
    "value": 
  ],
  [
    "name": "audio_start_offset_sec",
    "value": 
  ]
]

let boundary = "---011000010111000001101001"

var body = ""
var error: NSError? = nil
for param in parameters {
  let paramName = param["name"]!
  body += "--\(boundary)\r\n"
  body += "Content-Disposition:form-data; name=\"\(paramName)\""
  if let filename = param["fileName"] {
    let contentType = param["content-type"]!
    let fileContent = String(contentsOfFile: filename, encoding: String.Encoding.utf8)
    if (error != nil) {
      print(error as Any)
    }
    body += "; filename=\"\(filename)\"\r\n"
    body += "Content-Type: \(contentType)\r\n\r\n"
    body += fileContent
  } else if let paramValue = param["value"] {
    body += "\r\n\r\n\(paramValue)"
  }
}

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/embed")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Analyze videos

The Analyze API suite analyzes videos and generates texts on their content. The Analyze API suite offers three distinct endpoints tailored to meet various requirements. Each endpoint has been designed with specific levels of flexibility and customization to accommodate different needs.

* [`/gist`](/v1.3/api-reference/analyze-videos/gist): For topics, titles, and hashtags using predefined formats.
* [`/summarize`](/v1.3/api-reference/analyze-videos/summarize):  For summaries, chapters, and highlights, allowing customization with a prompt
* [`/analyze`](/v1.3/api-reference/analyze-videos/analyze): For open-ended text, requiring clear instructions in the form of a prompt to guide the output.

**Related guide**: [Analyze videos](/v1.3/docs/guides/analyze-videos).

[**Related quickstart notebook**](https://colab.research.google.com/github/twelvelabs-io/twelvelabs-developer-experience/blob/main/quickstarts/TwelveLabs_Quickstart_Analyze.ipynb)


# Titles, topics, or hashtags

```http
POST https://api.twelvelabs.io/v1.3/gist
Content-Type: application/json
```

This endpoint analyzes videos and generates titles, topics, and hashtags.






## Response Body

- 200: The gist of the specified video has successfully been generated.

- 400: The request has failed.
- 429: If the rate limit is reached, the platform returns an `HTTP 429 - Too many requests` error response. The response body is empty.


## Examples

```shell
curl -X POST https://api.twelvelabs.io/v1.3/gist \
     -H "x-api-key: 
This endpoint is rate-limited. For details, see the [Rate limits](/v1.3/docs/get-started/rate-limits) page.
" \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "types": [
    "title",
    "topic"
  ]
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/gist"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "types": ["title", "topic"]
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/gist';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","types":["title","topic"]}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/gist"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"types\": [\n    \"title\",\n    \"topic\"\n  ]\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/gist")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"types\": [\n    \"title\",\n    \"topic\"\n  ]\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/gist")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"types\": [\n    \"title\",\n    \"topic\"\n  ]\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/gist', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "types": [
    "title",
    "topic"
  ]
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/gist");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"types\": [\n    \"title\",\n    \"topic\"\n  ]\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "types": ["title", "topic"]
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/gist")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/gist \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "string",
  "types": [
    "title"
  ]
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/gist"

payload = {
    "video_id": "string",
    "types": ["title"]
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/gist';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"string","types":["title"]}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/gist"

	payload := strings.NewReader("{\n  \"video_id\": \"string\",\n  \"types\": [\n    \"title\"\n  ]\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/gist")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"string\",\n  \"types\": [\n    \"title\"\n  ]\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/gist")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"string\",\n  \"types\": [\n    \"title\"\n  ]\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/gist', [
  'body' => '{
  "video_id": "string",
  "types": [
    "title"
  ]
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/gist");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"string\",\n  \"types\": [\n    \"title\"\n  ]\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "string",
  "types": ["title"]
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/gist")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/gist \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "string",
  "types": [
    "title"
  ]
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/gist"

payload = {
    "video_id": "string",
    "types": ["title"]
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/gist';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"string","types":["title"]}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/gist"

	payload := strings.NewReader("{\n  \"video_id\": \"string\",\n  \"types\": [\n    \"title\"\n  ]\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/gist")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"string\",\n  \"types\": [\n    \"title\"\n  ]\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/gist")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"string\",\n  \"types\": [\n    \"title\"\n  ]\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/gist', [
  'body' => '{
  "video_id": "string",
  "types": [
    "title"
  ]
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/gist");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"string\",\n  \"types\": [\n    \"title\"\n  ]\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "string",
  "types": ["title"]
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/gist")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Summaries, chapters, or highlights

```http
POST https://api.twelvelabs.io/v1.3/summarize
Content-Type: application/json
```

This endpoint analyzes videos and generates summaries, chapters, or highlights. Optionally, you can provide a prompt to customize the output.






## Response Body

- 200: The specified video has successfully been summarized.

- 400: The request has failed.
- 429: If the rate limit is reached, the platform returns an `HTTP 429 - Too many requests` error response. The response body is empty.


## Examples

```shell
curl -X POST https://api.twelvelabs.io/v1.3/summarize \
     -H "x-api-key: 
This endpoint is rate-limited. For details, see the [Rate limits](/v1.3/docs/get-started/rate-limits) page.
" \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "type": "summary",
  "prompt": "Generate a summary of this video for a social media post, up to two sentences.",
  "temperature": 0.2
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/summarize"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "type": "summary",
    "prompt": "Generate a summary of this video for a social media post, up to two sentences.",
    "temperature": 0.2
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/summarize';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","type":"summary","prompt":"Generate a summary of this video for a social media post, up to two sentences.","temperature":0.2}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/summarize"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"type\": \"summary\",\n  \"prompt\": \"Generate a summary of this video for a social media post, up to two sentences.\",\n  \"temperature\": 0.2\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/summarize")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"type\": \"summary\",\n  \"prompt\": \"Generate a summary of this video for a social media post, up to two sentences.\",\n  \"temperature\": 0.2\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/summarize")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"type\": \"summary\",\n  \"prompt\": \"Generate a summary of this video for a social media post, up to two sentences.\",\n  \"temperature\": 0.2\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/summarize', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "type": "summary",
  "prompt": "Generate a summary of this video for a social media post, up to two sentences.",
  "temperature": 0.2
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/summarize");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"type\": \"summary\",\n  \"prompt\": \"Generate a summary of this video for a social media post, up to two sentences.\",\n  \"temperature\": 0.2\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "type": "summary",
  "prompt": "Generate a summary of this video for a social media post, up to two sentences.",
  "temperature": 0.2
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/summarize")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/summarize \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "string",
  "type": "string"
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/summarize"

payload = {
    "video_id": "string",
    "type": "string"
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/summarize';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"string","type":"string"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/summarize"

	payload := strings.NewReader("{\n  \"video_id\": \"string\",\n  \"type\": \"string\"\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/summarize")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"string\",\n  \"type\": \"string\"\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/summarize")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"string\",\n  \"type\": \"string\"\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/summarize', [
  'body' => '{
  "video_id": "string",
  "type": "string"
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/summarize");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"string\",\n  \"type\": \"string\"\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "string",
  "type": "string"
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/summarize")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/summarize \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "string",
  "type": "string"
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/summarize"

payload = {
    "video_id": "string",
    "type": "string"
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/summarize';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"string","type":"string"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/summarize"

	payload := strings.NewReader("{\n  \"video_id\": \"string\",\n  \"type\": \"string\"\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/summarize")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"string\",\n  \"type\": \"string\"\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/summarize")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"string\",\n  \"type\": \"string\"\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/summarize', [
  'body' => '{
  "video_id": "string",
  "type": "string"
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/summarize");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"string\",\n  \"type\": \"string\"\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "string",
  "type": "string"
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/summarize")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Open-ended texts

```http
POST https://api.twelvelabs.io/v1.3/generate
Content-Type: application/json
```



This endpoint generates open-ended texts based on your videos, including but not limited to tables of content, action items, memos, and detailed analyses.






## Response Body

- 200: The specified video has successfully been processed.
- 400: The request has failed.
- 429: If the rate limit is reached, the platform returns an `HTTP 429 - Too many requests` error response. The response body is empty.


## Examples

```shell Non-streamed response
curl -X POST https://api.twelvelabs.io/v1.3/generate \
     -H "x-api-key: This endpoint will be deprecated on **July 30, 2025**. Transition to the [`/analyze`](/v1.3/api-reference/analyze-videos/analyze) endpoint, which provides identical functionality. Ensure you've updated your API calls before the deprecation date to ensure uninterrupted service.
- This endpoint is rate-limited. For details, see the [Rate limits](/v1.3/docs/get-started/rate-limits) page.
- This endpoint supports streaming responses. For details on integrating this feature into your application, refer to the [Streaming response](/v1.3/docs/guides/generate-text-from-video/open-ended-text#streaming-responses) guide.
" \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}'
```

```python Non-streamed response
import requests

url = "https://api.twelvelabs.io/v1.3/generate"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
    "temperature": 0.2,
    "stream": True
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript Non-streamed response
const url = 'https://api.twelvelabs.io/v1.3/generate';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","prompt":"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.","temperature":0.2,"stream":true}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Non-streamed response
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/generate"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Non-streamed response
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/generate")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}"

response = http.request(request)
puts response.read_body
```

```java Non-streamed response
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/generate")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")
  .asString();
```

```php Non-streamed response
request('POST', 'https://api.twelvelabs.io/v1.3/generate', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Non-streamed response
var client = new RestClient("https://api.twelvelabs.io/v1.3/generate");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Non-streamed response
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/generate")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Stream start
curl -X POST https://api.twelvelabs.io/v1.3/generate \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}'
```

```python Stream start
import requests

url = "https://api.twelvelabs.io/v1.3/generate"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
    "temperature": 0.2,
    "stream": True
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript Stream start
const url = 'https://api.twelvelabs.io/v1.3/generate';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","prompt":"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.","temperature":0.2,"stream":true}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Stream start
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/generate"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Stream start
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/generate")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}"

response = http.request(request)
puts response.read_body
```

```java Stream start
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/generate")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")
  .asString();
```

```php Stream start
request('POST', 'https://api.twelvelabs.io/v1.3/generate', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Stream start
var client = new RestClient("https://api.twelvelabs.io/v1.3/generate");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Stream start
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/generate")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Text generation
curl -X POST https://api.twelvelabs.io/v1.3/generate \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}'
```

```python Text generation
import requests

url = "https://api.twelvelabs.io/v1.3/generate"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
    "temperature": 0.2,
    "stream": True
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript Text generation
const url = 'https://api.twelvelabs.io/v1.3/generate';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","prompt":"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.","temperature":0.2,"stream":true}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Text generation
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/generate"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Text generation
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/generate")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}"

response = http.request(request)
puts response.read_body
```

```java Text generation
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/generate")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")
  .asString();
```

```php Text generation
request('POST', 'https://api.twelvelabs.io/v1.3/generate', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Text generation
var client = new RestClient("https://api.twelvelabs.io/v1.3/generate");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Text generation
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/generate")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Stream end
curl -X POST https://api.twelvelabs.io/v1.3/generate \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}'
```

```python Stream end
import requests

url = "https://api.twelvelabs.io/v1.3/generate"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
    "temperature": 0.2,
    "stream": True
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript Stream end
const url = 'https://api.twelvelabs.io/v1.3/generate';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","prompt":"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.","temperature":0.2,"stream":true}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Stream end
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/generate"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Stream end
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/generate")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}"

response = http.request(request)
puts response.read_body
```

```java Stream end
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/generate")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")
  .asString();
```

```php Stream end
request('POST', 'https://api.twelvelabs.io/v1.3/generate', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Stream end
var client = new RestClient("https://api.twelvelabs.io/v1.3/generate");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Stream end
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/generate")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/generate \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "string",
  "prompt": "string"
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/generate"

payload = {
    "video_id": "string",
    "prompt": "string"
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/generate';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"string","prompt":"string"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/generate"

	payload := strings.NewReader("{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/generate")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/generate")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/generate', [
  'body' => '{
  "video_id": "string",
  "prompt": "string"
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/generate");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "string",
  "prompt": "string"
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/generate")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/generate \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "string",
  "prompt": "string"
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/generate"

payload = {
    "video_id": "string",
    "prompt": "string"
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/generate';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"string","prompt":"string"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/generate"

	payload := strings.NewReader("{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/generate")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/generate")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/generate', [
  'body' => '{
  "video_id": "string",
  "prompt": "string"
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/generate");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "string",
  "prompt": "string"
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/generate")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Open-ended analysis

```http
POST https://api.twelvelabs.io/v1.3/analyze
Content-Type: application/json
```

This endpoint analyzes your videos and creates fully customizable text based on your prompts, including but not limited to tables of content, action items, memos, and detailed analyses.






## Response Body

- 200: The specified video has successfully been processed.
- 400: The request has failed.
- 429: If the rate limit is reached, the platform returns an `HTTP 429 - Too many requests` error response. The response body is empty.


## Examples

```shell Non-streamed response
curl -X POST https://api.twelvelabs.io/v1.3/analyze \
     -H "x-api-key: 
- This endpoint is rate-limited. For details, see the [Rate limits](/v1.3/docs/get-started/rate-limits) page.
- This endpoint supports streaming responses. For details on integrating this feature into your application, refer to the [Streaming response](/v1.3/docs/guides/generate-text-from-video/open-ended-text#streaming-responses) guide.
" \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}'
```

```python Non-streamed response
import requests

url = "https://api.twelvelabs.io/v1.3/analyze"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
    "temperature": 0.2,
    "stream": True
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript Non-streamed response
const url = 'https://api.twelvelabs.io/v1.3/analyze';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","prompt":"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.","temperature":0.2,"stream":true}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Non-streamed response
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/analyze"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Non-streamed response
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/analyze")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}"

response = http.request(request)
puts response.read_body
```

```java Non-streamed response
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/analyze")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")
  .asString();
```

```php Non-streamed response
request('POST', 'https://api.twelvelabs.io/v1.3/analyze', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Non-streamed response
var client = new RestClient("https://api.twelvelabs.io/v1.3/analyze");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Non-streamed response
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/analyze")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Stream start
curl -X POST https://api.twelvelabs.io/v1.3/analyze \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}'
```

```python Stream start
import requests

url = "https://api.twelvelabs.io/v1.3/analyze"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
    "temperature": 0.2,
    "stream": True
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript Stream start
const url = 'https://api.twelvelabs.io/v1.3/analyze';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","prompt":"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.","temperature":0.2,"stream":true}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Stream start
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/analyze"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Stream start
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/analyze")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}"

response = http.request(request)
puts response.read_body
```

```java Stream start
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/analyze")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")
  .asString();
```

```php Stream start
request('POST', 'https://api.twelvelabs.io/v1.3/analyze', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Stream start
var client = new RestClient("https://api.twelvelabs.io/v1.3/analyze");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Stream start
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/analyze")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Text generation
curl -X POST https://api.twelvelabs.io/v1.3/analyze \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}'
```

```python Text generation
import requests

url = "https://api.twelvelabs.io/v1.3/analyze"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
    "temperature": 0.2,
    "stream": True
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript Text generation
const url = 'https://api.twelvelabs.io/v1.3/analyze';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","prompt":"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.","temperature":0.2,"stream":true}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Text generation
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/analyze"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Text generation
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/analyze")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}"

response = http.request(request)
puts response.read_body
```

```java Text generation
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/analyze")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")
  .asString();
```

```php Text generation
request('POST', 'https://api.twelvelabs.io/v1.3/analyze', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Text generation
var client = new RestClient("https://api.twelvelabs.io/v1.3/analyze");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Text generation
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/analyze")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell Stream end
curl -X POST https://api.twelvelabs.io/v1.3/analyze \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}'
```

```python Stream end
import requests

url = "https://api.twelvelabs.io/v1.3/analyze"

payload = {
    "video_id": "6298d673f1090f1100476d4c",
    "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
    "temperature": 0.2,
    "stream": True
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript Stream end
const url = 'https://api.twelvelabs.io/v1.3/analyze';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"6298d673f1090f1100476d4c","prompt":"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.","temperature":0.2,"stream":true}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go Stream end
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/analyze"

	payload := strings.NewReader("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby Stream end
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/analyze")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}"

response = http.request(request)
puts response.read_body
```

```java Stream end
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/analyze")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}")
  .asString();
```

```php Stream end
request('POST', 'https://api.twelvelabs.io/v1.3/analyze', [
  'body' => '{
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp Stream end
var client = new RestClient("https://api.twelvelabs.io/v1.3/analyze");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"6298d673f1090f1100476d4c\",\n  \"prompt\": \"I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.\",\n  \"temperature\": 0.2,\n  \"stream\": true\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift Stream end
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "6298d673f1090f1100476d4c",
  "prompt": "I want to generate a description for my video with the following format - Title of the video, followed by a summary in 2-3 sentences, highlighting the main topic, key events, and concluding remarks.",
  "temperature": 0.2,
  "stream": true
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/analyze")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/analyze \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "string",
  "prompt": "string"
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/analyze"

payload = {
    "video_id": "string",
    "prompt": "string"
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/analyze';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"string","prompt":"string"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/analyze"

	payload := strings.NewReader("{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/analyze")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/analyze")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/analyze', [
  'body' => '{
  "video_id": "string",
  "prompt": "string"
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/analyze");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "string",
  "prompt": "string"
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/analyze")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

```shell
curl -X POST https://api.twelvelabs.io/v1.3/analyze \
     -H "x-api-key: " \
     -H "Content-Type: application/json" \
     -d '{
  "video_id": "string",
  "prompt": "string"
}'
```

```python
import requests

url = "https://api.twelvelabs.io/v1.3/analyze"

payload = {
    "video_id": "string",
    "prompt": "string"
}
headers = {
    "x-api-key": "",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```

```javascript
const url = 'https://api.twelvelabs.io/v1.3/analyze';
const options = {
  method: 'POST',
  headers: {'x-api-key': '', 'Content-Type': 'application/json'},
  body: '{"video_id":"string","prompt":"string"}'
};

try {
  const response = await fetch(url, options);
  const data = await response.json();
  console.log(data);
} catch (error) {
  console.error(error);
}
```

```go
package main

import (
	"fmt"
	"strings"
	"net/http"
	"io"
)

func main() {

	url := "https://api.twelvelabs.io/v1.3/analyze"

	payload := strings.NewReader("{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}")

	req, _ := http.NewRequest("POST", url, payload)

	req.Header.Add("x-api-key", "")
	req.Header.Add("Content-Type", "application/json")

	res, _ := http.DefaultClient.Do(req)

	defer res.Body.Close()
	body, _ := io.ReadAll(res.Body)

	fmt.Println(res)
	fmt.Println(string(body))

}
```

```ruby
require 'uri'
require 'net/http'

url = URI("https://api.twelvelabs.io/v1.3/analyze")

http = Net::HTTP.new(url.host, url.port)
http.use_ssl = true

request = Net::HTTP::Post.new(url)
request["x-api-key"] = ''
request["Content-Type"] = 'application/json'
request.body = "{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}"

response = http.request(request)
puts response.read_body
```

```java
HttpResponse response = Unirest.post("https://api.twelvelabs.io/v1.3/analyze")
  .header("x-api-key", "")
  .header("Content-Type", "application/json")
  .body("{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}")
  .asString();
```

```php
request('POST', 'https://api.twelvelabs.io/v1.3/analyze', [
  'body' => '{
  "video_id": "string",
  "prompt": "string"
}',
  'headers' => [
    'Content-Type' => 'application/json',
    'x-api-key' => '',
  ],
]);

echo $response->getBody();
```

```csharp
var client = new RestClient("https://api.twelvelabs.io/v1.3/analyze");
var request = new RestRequest(Method.POST);
request.AddHeader("x-api-key", "");
request.AddHeader("Content-Type", "application/json");
request.AddParameter("application/json", "{\n  \"video_id\": \"string\",\n  \"prompt\": \"string\"\n}", ParameterType.RequestBody);
IRestResponse response = client.Execute(request);
```

```swift
import Foundation

let headers = [
  "x-api-key": "",
  "Content-Type": "application/json"
]
let parameters = [
  "video_id": "string",
  "prompt": "string"
] as [String : Any]

let postData = JSONSerialization.data(withJSONObject: parameters, options: [])

let request = NSMutableURLRequest(url: NSURL(string: "https://api.twelvelabs.io/v1.3/analyze")! as URL,
                                        cachePolicy: .useProtocolCachePolicy,
                                    timeoutInterval: 10.0)
request.httpMethod = "POST"
request.allHTTPHeaderFields = headers
request.httpBody = postData as Data

let session = URLSession.shared
let dataTask = session.dataTask(with: request as URLRequest, completionHandler: { (data, response, error) -> Void in
  if (error != nil) {
    print(error as Any)
  } else {
    let httpResponse = response as? HTTPURLResponse
    print(httpResponse)
  }
})

dataTask.resume()
```

# Error codes

This page lists the most common error messages you may encounter while using the platform.

# General

* `parameter_invalid`
  * The `{parameter}` parameter is invalid.
  * The following parameters are invalid: `{parameters}`.
  * The request contains some invalid parameters.
* `parameter_not_provided`
  * The `{parameter}` parameter is required but was not provided.
  * The following required parameters were not provided: `{parameters}`.
  * Some required parameters are not provided.
* `parameter_unknown`
  * The `{parameter}` parameter is unknown.
  * The following parameters are unknown: `{parameters}`.
  * The request contains some unknown parameters.
* `resource_not_exist`
  * Resource with id `{resource_id}` does not exist in `{collection_name}`.
* `api_key_invalid`
  * API Key is either invalid or expired. Please check your API key or generate a new one from the dashboard and try again.
* `tags_not_allowed`
  * Tag `{tag}` is not allowed to use. Please remove it from the request.
  * The following tags are not allowed to be used: `{tags}`. Please remove these from the request.
* `api_upgrade_required`
  * This endpoint is supported starting with version `{version}`. Your version is `{current_version}`.

# The `/indexes` endpoint

* `index_option_cannot_be_changed`
  * Index option cannot be changed. Please remove index\_options parameter and try again. If you want to change index option, please create new index.
* `index_engine_cannot_be_changed`
  * Index engine cannot be changed. Please remove engine\_id parameter and try again. If you want to change engine, please create new index.
* `index_name_already_exists`
  * Index name `{index_name}` already exists. Please use another unique name and try again.

# The `/tasks` endpoint

* `video_resolution_too_low`
  * The resolution of the video is too low. Please upload a video with resolution between 360x360 and 3840x2160. Current resolution is `{current_resolution}`.
* `video_resolution_too_high`
  * The resolution of the video is too high. Please upload a video with resolution between 360x360 and 3840x2160. Current resolution is `{current_resolution}`.
* `video_resolution_invalid_aspect_ratio`
  * The aspect ratio of the video is invalid. Please upload a video with aspect ratio between 1:1 and 2.4:1. Current resolution is `{current_resolution}`.
* `video_duration_too_short`
  * Video is too short. Please use video with duration between 10 seconds and 2 hours(7200 seconds). Current duration is `{current_duration}` seconds.
* `video_duration_too_long`
  * Video is too long. Please use video with duration between 10 seconds and 2 hours(7200 seconds). Current duration is `{current_duration}` seconds.
* `video_file_broken`
  * Cannot read video file. Please check the video file is valid and try again.
* `task_cannot_be_deleted`
  * (Returns raw error message)
* `usage_limit_exceeded`
  * Not enough free credit. Please register a payment method or contact [sales@twelvelabs.io](mailto:sales@twelvelabs.io).
* `video_filesize_too_large`
  * The video is too large. Please use a video with a size less than `{maximum_size}`. The current size is `{current_file_size}`.

# The `/search` endpoint

* `search_option_not_supported`
  * Search option `{search_option}` is not supported for index `{index_id}`. Please use one of the following search options: `{supported_search_option}`.
* `search_option_combination_not_supported`
  * Search option `{search_option}` is not supported with `{other_combination}`.
* `search_filter_invalid`
  * Filter used in search is invalid. Please use the valid filter syntax by following filtering documentation.
* `search_page_token_expired`
  * The token that identifies the page to be retrieved is expired or invalid. You must make a new search request. Token: `{next_page_token}`.
* `index_not_supported_for_search`:
  * You can only perform search requests on indexes with an engine from the Marengo family enabled.

# The `/generate` endpoint

* `token_limit_exceeded`
  * Your request could not be processed due to exceeding maximum token limit. Please try with another request or another video with shorter duration.
* `index_not_supported_for_generate`
  * You can only summarize videos uploaded to an index with an engine from the Pegasus family enabled.

# The `/summarize` endpoint

* `token_limit_exceeded`
  * Your request could not be processed due to exceeding maximum token limit. Please try with another request or another video with shorter duration.

# The `/embed` endpoint

* `parameter_invalid`
  * The `text` parameter is invalid. The text token length should be less than or equal to 77.
  * The `text_truncate` parameter is invalid. You should use one of the following values: `none`, `start`, `end`.

# The `/embed/tasks` endpoint

* `parameter_invalid`
  * The `video_clip_length` parameter is invalid. `video_clip_length` should be within 2-10 seconds long
  * The `video_end_offset_sec` parameter is invalid. `video_end_offset_sec` should be greater than `video_start_offset_sec`

# The `/embed/tasks/{task-id}/status` endpoint

* `parameter_invalid`
  * The `task_id` parameter is invalid. `task_id` value is invalid